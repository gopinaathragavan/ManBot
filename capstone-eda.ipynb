{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuation from Scraping notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Walkthrough continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 GLOBAL IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:55:47.020248Z",
     "start_time": "2018-05-23T18:55:45.902249Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from decimal import Decimal\n",
    "import re\n",
    "import string\n",
    "import unicodedata as unicode\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are your variables of interest?\n",
    "\n",
    "For the capstone, my variables or features of interest are the words that represent risk factors which signify trigger words for early intervention in mental health specifically depression\n",
    "\n",
    "### What outliers did you remove?\n",
    "\n",
    "In the context of textual parsing, by limiting word frequency to more than 2 occurrences and using trigrams(three-word pairs), any word combinations that did not meet that criteria were outliers.\n",
    "\n",
    "### What types of data imputation did you perform?\n",
    "\n",
    "Extending the stopwords list after visual examination of the postings was a form of data imputation that was performed. Uppercase was converted to lowercase and accents in words were stripped. During scraping, html tags and scripting elements were stripped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:55:50.412982Z",
     "start_time": "2018-05-23T18:55:49.936915Z"
    }
   },
   "outputs": [],
   "source": [
    "# General imports for pickling and evaluating runtime performance\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Sklearn imports both text preprocessing and sklearn's NLP models\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#imports for LDA topic model visualisation\n",
    "from __future__ import print_function\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:55:53.338593Z",
     "start_time": "2018-05-23T18:55:53.302562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read scraped data into pandas dataframe\n",
    "postings = pd.read_excel(\"postings.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:55:54.378575Z",
     "start_time": "2018-05-23T18:55:54.363572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>badge</th>\n",
       "      <th>likes</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['3 March 2018']</td>\n",
       "      <td>Doolhof</td>\n",
       "      <td>Community Champion</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Right now I feel like I don't have the energy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['3 March 2018']</td>\n",
       "      <td>quirkywords</td>\n",
       "      <td>Community Champion</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Mrs dool.\\nI am sending you a big reassuring h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['3 March 2018']</td>\n",
       "      <td>Summer Rose</td>\n",
       "      <td>Valued Contributor</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hi Doolhof\\nI'm sorry that you feel so low.  Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['4 March 2018']</td>\n",
       "      <td>Doolhof</td>\n",
       "      <td>Community Champion</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hi Quirky,\\nThanks for the virtual hug, I need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['4 March 2018']</td>\n",
       "      <td>Doolhof</td>\n",
       "      <td>Community Champion</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hi Summer Rose,\\nThanks for your encouragement...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       author               badge  likes  \\\n",
       "1  ['3 March 2018']      Doolhof  Community Champion    4.0   \n",
       "2  ['3 March 2018']  quirkywords  Community Champion    3.0   \n",
       "3  ['3 March 2018']  Summer Rose  Valued Contributor    2.0   \n",
       "4  ['4 March 2018']      Doolhof  Community Champion    2.0   \n",
       "5  ['4 March 2018']      Doolhof  Community Champion    3.0   \n",
       "\n",
       "                                             content  \n",
       "1  Right now I feel like I don't have the energy ...  \n",
       "2  Mrs dool.\\nI am sending you a big reassuring h...  \n",
       "3  Hi Doolhof\\nI'm sorry that you feel so low.  Y...  \n",
       "4  Hi Quirky,\\nThanks for the virtual hug, I need...  \n",
       "5  Hi Summer Rose,\\nThanks for your encouragement...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only interested in the content of the postings at the moment so in our first iteration, EDA will focus on the posting content only.\n",
    "# Scraping was done more broadly as we could factor in likes, badge and use the dates for other modelling in later stages.\n",
    "postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:55:55.753380Z",
     "start_time": "2018-05-23T18:55:55.748412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 150 posts from one of the discussion threads in the beyondblue mental health forum's depression subforum. This is a preliminary dataset and we will be expanding our dataset in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text Preprocessing and Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looked at the posts from within Excel and decided that we will only preprocess the post content for now. The dates can be dealt with when we want to perform any time-series modelling later. We scraped it given our final goal is geared towards an application where time and order of conversational flow matter. The rest of the fields besides content and date are pretty clean due to the html tag stripping done as part of the Beautiful Soup scraping process. So the EDA now focuses on the content of the posting...\n",
    "\n",
    "Initial exploration of the text data was done by visual examination of each individual posts in the excel spreadsheet and we found that there were words that could be excluded in addition to the standard stopwords list in NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:55:59.462574Z",
     "start_time": "2018-05-23T18:55:59.454603Z"
    }
   },
   "outputs": [],
   "source": [
    "# extending standard NLTK library's english stopwords to include other words like pronouns and meaningless words from visual examination in Excel\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = [\"just\", \"like\", \"pamela\",\"karen\",\"dools\",\"hi\",\"hello\",\"chloe\",\"pamelar\",\"pammy\",\"mrs\", \"mr\", \"quirky\", \"doolhof\", \\\n",
    "                \"summer\", \"rose\",\"demonblaster\",\"quirkywords\",\"db\",\"Shell\",\"etc\",\"D\",\"Shelley\", \"anne\",\"Quercus\",\"Nat\",\"Dr\",\"PamelaR\",\"Bev\",\"Mary\", \\\n",
    "               \"Agapanthus\",\"Claire\", \"Weekes\", \"Paul\",\"blondguy\",\"em\",\"BB\",\"grandy\",\"TonyWK\",\"Tony\",\"Ggrand\",\"white\", \"knight\",\"Mam\", \\\n",
    "                \"Laters\",\"birdy\", \"Hiya\",\"Croix\", \"Croix's\",\"White\",\"Knight\",\"SN\",\"DB\",\"Queensland\",\"Doolsy\",\"bye\",\"Hi/bye\",\"Deebi\",\"Weetbix\", \\\n",
    "                \"Willow\", \"(WW)\",\"Chloe\", \"Chlo\",\"BTW\"]\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:56:00.210576Z",
     "start_time": "2018-05-23T18:56:00.200575Z"
    }
   },
   "outputs": [],
   "source": [
    "def getcvt_freq_words(sparse_counts, columns):\n",
    "   # X_all is a sparse matrix, so sum() returns a 'matrix' datatype ...\n",
    "   #   which we then convert into a 1-D ndarray for sorting\n",
    "   word_counts = np.asarray(X_cvt.sum(axis=0)).reshape(-1)\n",
    "\n",
    "   # argsort() returns smallest first, so we reverse the result\n",
    "   largest_count_indices = word_counts.argsort()[::-1]\n",
    "\n",
    "   # pretty-print the results! Remember to always ask whether they make sense ...\n",
    "   freq_words = pd.Series(word_counts[largest_count_indices],\n",
    "                          index=columns[largest_count_indices])\n",
    "\n",
    "   return freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:56:00.929588Z",
     "start_time": "2018-05-23T18:56:00.918609Z"
    }
   },
   "outputs": [],
   "source": [
    "def gettfidf_freq_words(sparse_counts, columns):\n",
    "   # X_all is a sparse matrix, so sum() returns a 'matrix' datatype ...\n",
    "   #   which we then convert into a 1-D ndarray for sorting\n",
    "   word_counts = np.asarray(X_tfidf.sum(axis=0)).reshape(-1)\n",
    "\n",
    "   # argsort() returns smallest first, so we reverse the result\n",
    "   largest_count_indices = word_counts.argsort()[::-1]\n",
    "\n",
    "   # pretty-print the results! Remember to always ask whether they make sense ...\n",
    "   freq_words = pd.Series(word_counts[largest_count_indices],\n",
    "                          index=columns[largest_count_indices])\n",
    "\n",
    "   return freq_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ETL: Transforming Text into Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:56:03.341571Z",
     "start_time": "2018-05-23T18:56:03.338572Z"
    }
   },
   "outputs": [],
   "source": [
    "# ETL algorithms to normalise text into word vectors - testing countvectorizer and tfidfvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:59:39.700133Z",
     "start_time": "2018-05-23T18:59:39.634136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 65)\n",
      "Requires 9750 ints to do a .toarray()!\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer\n",
    "cvt_vectorizer = CountVectorizer(analyzer=\"word\", stop_words= stopwords, ngram_range=(3,3),min_df = 2, max_df=0.5, max_features=10000, lowercase=True, \\\n",
    "                      strip_accents=\"unicode\")\n",
    "X_cvt =  cvt_vectorizer.fit_transform(postings.content)\n",
    "columns = np.array(cvt_vectorizer.get_feature_names())          # ndarray (for indexing below)\n",
    "print(X_cvt.shape)\n",
    "print(\"Requires {} ints to do a .toarray()!\".format(X_cvt.shape[0] * X_cvt.shape[1]))\n",
    "freq_words_cvt = getcvt_freq_words(X_cvt, columns)\n",
    "freq_words_cvt.to_csv(\"freq_words_cvt.csv\")\n",
    "freq_words_cvt = pd.read_csv(\"freq_words_cvt.csv\", header=None, names=[\"word\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:59:40.891136Z",
     "start_time": "2018-05-23T18:59:40.876135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mental health issues</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cuddling black dog</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sending virtual hugs</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>darn black dog</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>going try make</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sorry hear struggling</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dance around house</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positivity motivation humour</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>help feel better</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hope okay today</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>feel cheerful happy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hoping today better</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>got nan dog</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hour drive away</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good day going</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>huge gum leaves</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>go round emotions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fight tired unwell</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>feeling really depressed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>write gratitude journal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>explain people around</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>feel bit better</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>laughter good soul</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>exhausting full stop</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>enjoy time family</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dog feral cat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>depression explain people</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>country fire service</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>channel end game</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cat hanging upside</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>loads virtual hugs</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>love go camping</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>wave everyone else</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>wanting run away</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>unwell tired trying</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tired unwell tired</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>time depression explain</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>things know help</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>thanks much comments</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>thanks going try</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>taken long time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>snarling german shepherd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>self help books</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>scenery driving along</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>said really enjoyed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>reading self help</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ray ct scan</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>please ever give</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>picnic drs office</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>pain mental health</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>outside look stars</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>oh well hope</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>need find works</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>need find ways</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>need fight depression</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>much kind words</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>merry go round</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>many wonderful people</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>wondering time depression</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>ants huge gum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word  count\n",
       "0           mental health issues      8\n",
       "1             cuddling black dog      6\n",
       "2           sending virtual hugs      5\n",
       "3                 darn black dog      4\n",
       "4                 going try make      4\n",
       "5          sorry hear struggling      3\n",
       "6             dance around house      3\n",
       "7   positivity motivation humour      3\n",
       "8               help feel better      2\n",
       "9                hope okay today      2\n",
       "10           feel cheerful happy      2\n",
       "11           hoping today better      2\n",
       "12                   got nan dog      2\n",
       "13               hour drive away      2\n",
       "14                good day going      2\n",
       "15               huge gum leaves      2\n",
       "16             go round emotions      2\n",
       "17            fight tired unwell      2\n",
       "18      feeling really depressed      2\n",
       "19       write gratitude journal      2\n",
       "20         explain people around      2\n",
       "21               feel bit better      2\n",
       "22            laughter good soul      2\n",
       "23          exhausting full stop      2\n",
       "24             enjoy time family      2\n",
       "25                 dog feral cat      2\n",
       "26     depression explain people      2\n",
       "27          country fire service      2\n",
       "28              channel end game      2\n",
       "29            cat hanging upside      2\n",
       "..                           ...    ...\n",
       "35            loads virtual hugs      2\n",
       "36               love go camping      2\n",
       "37            wave everyone else      2\n",
       "38              wanting run away      2\n",
       "39           unwell tired trying      2\n",
       "40            tired unwell tired      2\n",
       "41       time depression explain      2\n",
       "42              things know help      2\n",
       "43          thanks much comments      2\n",
       "44              thanks going try      2\n",
       "45               taken long time      2\n",
       "46      snarling german shepherd      2\n",
       "47               self help books      2\n",
       "48         scenery driving along      2\n",
       "49           said really enjoyed      2\n",
       "50             reading self help      2\n",
       "51                   ray ct scan      2\n",
       "52              please ever give      2\n",
       "53             picnic drs office      2\n",
       "54            pain mental health      2\n",
       "55            outside look stars      2\n",
       "56                  oh well hope      2\n",
       "57               need find works      2\n",
       "58                need find ways      2\n",
       "59         need fight depression      2\n",
       "60               much kind words      2\n",
       "61                merry go round      2\n",
       "62         many wonderful people      2\n",
       "63     wondering time depression      2\n",
       "64                 ants huge gum      2\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_cvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:59:29.899142Z",
     "start_time": "2018-05-23T18:59:29.827134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 65)\n",
      "Requires 9750 ints to do a .toarray()!\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words= stopwords, ngram_range=(3,3),max_df=0.5, max_features=10000,\n",
    "                             min_df=2,\n",
    "                             use_idf=True)\n",
    "X_tfidf =  tfidf_vectorizer.fit_transform(postings.content)\n",
    "columns = np.array(tfidf_vectorizer.get_feature_names())          # ndarray (for indexing below)\n",
    "print(X_tfidf.shape)\n",
    "print(\"Requires {} ints to do a .toarray()!\".format(X_tfidf.shape[0] * X_tfidf.shape[1]))\n",
    "freq_words_tfidf = gettfidf_freq_words(X_tfidf, columns)\n",
    "freq_words_tfidf.to_csv(\"freq_words_tfidf.csv\")\n",
    "freq_words_tfidf = pd.read_csv(\"freq_words_tfidf.csv\", header=None, names=[\"word\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:59:46.695704Z",
     "start_time": "2018-05-23T18:59:46.674707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mental health issues</td>\n",
       "      <td>5.005902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sending virtual hugs</td>\n",
       "      <td>4.049465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cuddling black dog</td>\n",
       "      <td>3.846836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positivity motivation humour</td>\n",
       "      <td>2.685494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sorry hear struggling</td>\n",
       "      <td>2.266668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dance around house</td>\n",
       "      <td>2.192610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>darn black dog</td>\n",
       "      <td>2.111416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>got nan dog</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>love go camping</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>enjoy time family</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>going try make</td>\n",
       "      <td>1.911019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>feel cheerful happy</td>\n",
       "      <td>1.789786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>need fight depression</td>\n",
       "      <td>1.789786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hoping today better</td>\n",
       "      <td>1.728078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>better kind thoughts</td>\n",
       "      <td>1.728078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>many wonderful people</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>much kind words</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>loads virtual hugs</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>picnic drs office</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>please ever give</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>channel end game</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reading self help</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wave everyone else</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wanting run away</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>said really enjoyed</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>care good lady</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>self help books</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ray ct scan</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cat hanging upside</td>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>need find ways</td>\n",
       "      <td>1.619795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>oh well hope</td>\n",
       "      <td>1.435185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>exhausting full stop</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>taken long time</td>\n",
       "      <td>1.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>need find works</td>\n",
       "      <td>1.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>snarling german shepherd</td>\n",
       "      <td>1.380598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ants huge gum</td>\n",
       "      <td>1.311448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>huge gum leaves</td>\n",
       "      <td>1.311448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>feeling really depressed</td>\n",
       "      <td>1.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>help feel better</td>\n",
       "      <td>1.233921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>country fire service</td>\n",
       "      <td>1.233921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>pain mental health</td>\n",
       "      <td>1.233921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>merry go round</td>\n",
       "      <td>1.207107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>go round emotions</td>\n",
       "      <td>1.207107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>feel bit better</td>\n",
       "      <td>1.207107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>things know help</td>\n",
       "      <td>1.159487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>write gratitude journal</td>\n",
       "      <td>1.159487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>good day going</td>\n",
       "      <td>1.159487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>thanks going try</td>\n",
       "      <td>1.152716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>laughter good soul</td>\n",
       "      <td>1.117277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>thanks much comments</td>\n",
       "      <td>1.076353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>black dog right</td>\n",
       "      <td>1.072175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>outside look stars</td>\n",
       "      <td>1.062641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>fight tired unwell</td>\n",
       "      <td>1.043442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tired unwell tired</td>\n",
       "      <td>1.043442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>unwell tired trying</td>\n",
       "      <td>1.043442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>wondering time depression</td>\n",
       "      <td>0.908248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>explain people around</td>\n",
       "      <td>0.908248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>depression explain people</td>\n",
       "      <td>0.908248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>time depression explain</td>\n",
       "      <td>0.908248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>hour drive away</td>\n",
       "      <td>0.877018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word     count\n",
       "0           mental health issues  5.005902\n",
       "1           sending virtual hugs  4.049465\n",
       "2             cuddling black dog  3.846836\n",
       "3   positivity motivation humour  2.685494\n",
       "4          sorry hear struggling  2.266668\n",
       "5             dance around house  2.192610\n",
       "6                 darn black dog  2.111416\n",
       "7                    got nan dog  2.000000\n",
       "8                love go camping  2.000000\n",
       "9              enjoy time family  2.000000\n",
       "10                going try make  1.911019\n",
       "11           feel cheerful happy  1.789786\n",
       "12         need fight depression  1.789786\n",
       "13           hoping today better  1.728078\n",
       "14          better kind thoughts  1.728078\n",
       "15         many wonderful people  1.707107\n",
       "16               much kind words  1.707107\n",
       "17            loads virtual hugs  1.707107\n",
       "18             picnic drs office  1.707107\n",
       "19              please ever give  1.707107\n",
       "20              channel end game  1.707107\n",
       "21             reading self help  1.707107\n",
       "22            wave everyone else  1.707107\n",
       "23              wanting run away  1.707107\n",
       "24           said really enjoyed  1.707107\n",
       "25                care good lady  1.707107\n",
       "26               self help books  1.707107\n",
       "27                   ray ct scan  1.707107\n",
       "28            cat hanging upside  1.707107\n",
       "29                need find ways  1.619795\n",
       "..                           ...       ...\n",
       "35                  oh well hope  1.435185\n",
       "36          exhausting full stop  1.414214\n",
       "37               taken long time  1.408248\n",
       "38               need find works  1.408248\n",
       "39      snarling german shepherd  1.380598\n",
       "40                 ants huge gum  1.311448\n",
       "41               huge gum leaves  1.311448\n",
       "42      feeling really depressed  1.295704\n",
       "43              help feel better  1.233921\n",
       "44          country fire service  1.233921\n",
       "45            pain mental health  1.233921\n",
       "46                merry go round  1.207107\n",
       "47             go round emotions  1.207107\n",
       "48               feel bit better  1.207107\n",
       "49              things know help  1.159487\n",
       "50       write gratitude journal  1.159487\n",
       "51                good day going  1.159487\n",
       "52              thanks going try  1.152716\n",
       "53            laughter good soul  1.117277\n",
       "54          thanks much comments  1.076353\n",
       "55               black dog right  1.072175\n",
       "56            outside look stars  1.062641\n",
       "57            fight tired unwell  1.043442\n",
       "58            tired unwell tired  1.043442\n",
       "59           unwell tired trying  1.043442\n",
       "60     wondering time depression  0.908248\n",
       "61         explain people around  0.908248\n",
       "62     depression explain people  0.908248\n",
       "63       time depression explain  0.908248\n",
       "64               hour drive away  0.877018\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 LDA (Latent Dirichlet Allocation) topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T18:59:57.705661Z",
     "start_time": "2018-05-23T18:59:53.323065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.0125    , 0.0125    , ..., 0.0125    , 0.0125    ,\n",
       "        0.0125    ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.0125    , 0.0125    , 0.0125    , ..., 0.0125    , 0.0125    ,\n",
       "        0.0125    ],\n",
       "       ...,\n",
       "       [0.01666667, 0.01666667, 0.68333333, ..., 0.01666667, 0.01666667,\n",
       "        0.01666667],\n",
       "       [0.025     , 0.025     , 0.025     , ..., 0.025     , 0.525     ,\n",
       "        0.025     ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA topic model fit based on CountVectorizer \n",
    "\n",
    "lda_cvt = LatentDirichletAllocation(n_components=20, random_state=0, n_jobs=-1, learning_method=\"online\")\n",
    "lda_cvt.fit_transform(X_cvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T17:02:14.707903Z",
     "start_time": "2018-05-20T17:02:14.698894Z"
    }
   },
   "outputs": [],
   "source": [
    "#import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T17:02:21.756192Z",
     "start_time": "2018-05-20T17:02:14.713892Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_components': [2, 5, 7, 10, 20, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [2,5,7,10,20,30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search for CVT\n",
    "model.fit(X_cvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T17:02:21.768199Z",
     "start_time": "2018-05-20T17:02:21.761197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.9, 'n_components': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters for cvt\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:00:09.237664Z",
     "start_time": "2018-05-23T19:00:04.974628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.0125    , 0.0125    , ..., 0.0125    , 0.0125    ,\n",
       "        0.0125    ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.0125    , 0.0125    , 0.0125    , ..., 0.0125    , 0.0125    ,\n",
       "        0.0125    ],\n",
       "       ...,\n",
       "       [0.01666667, 0.01666667, 0.01666667, ..., 0.01666667, 0.01666667,\n",
       "        0.01666667],\n",
       "       [0.025     , 0.025     , 0.025     , ..., 0.025     , 0.525     ,\n",
       "        0.025     ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimised LDA topic model fit based on CountVectorizer \n",
    "\n",
    "lda_cvt = LatentDirichletAllocation(n_components=20, random_state=0, n_jobs=-1, learning_method=\"online\", learning_decay=0.9)\n",
    "lda_cvt.fit_transform(X_cvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:00:18.535514Z",
     "start_time": "2018-05-23T19:00:14.221267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01837593, 0.01837593, 0.01837593, ..., 0.01837593, 0.01837593,\n",
       "        0.01837593],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.01830127, 0.01830127, 0.01830127, ..., 0.01830127, 0.01830127,\n",
       "        0.01830127],\n",
       "       ...,\n",
       "       [0.02071068, 0.02071068, 0.02071068, ..., 0.02071068, 0.02071068,\n",
       "        0.02071068],\n",
       "       [0.025     , 0.025     , 0.025     , ..., 0.025     , 0.525     ,\n",
       "        0.025     ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA topic model fit based on tfidf_vectorizer \n",
    "lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0, n_jobs=-1, learning_method='online')\n",
    "lda_tfidf.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T17:02:38.592321Z",
     "start_time": "2018-05-20T17:02:32.113324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_components': [2, 5, 7, 10, 20, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [2,5,7,10,20,30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search for tfidf\n",
    "model.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T17:02:38.600324Z",
     "start_time": "2018-05-20T17:02:38.594323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.9, 'n_components': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters for tfidf\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:00:24.223979Z",
     "start_time": "2018-05-23T19:00:19.943302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01837593, 0.01837593, 0.01837593, ..., 0.01837593, 0.01837593,\n",
       "        0.01837593],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ],\n",
       "       [0.01830127, 0.01830127, 0.01830127, ..., 0.01830127, 0.01830127,\n",
       "        0.01830127],\n",
       "       ...,\n",
       "       [0.02071068, 0.02071068, 0.02071068, ..., 0.02071068, 0.02071068,\n",
       "        0.02071068],\n",
       "       [0.025     , 0.025     , 0.025     , ..., 0.025     , 0.525     ,\n",
       "        0.025     ],\n",
       "       [0.05      , 0.05      , 0.05      , ..., 0.05      , 0.05      ,\n",
       "        0.05      ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimised LDA topic model fit based on tfidf_vectorizer \n",
    "lda_tfidf = LatentDirichletAllocation(n_components=20, random_state=0, n_jobs=-1, learning_method='online', learning_decay=0.9)\n",
    "lda_tfidf.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Visualising LDA (Latent Dirichlet Allocation) topic models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:00:30.802848Z",
     "start_time": "2018-05-23T19:00:28.007851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1148027357163516964343417949\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1148027357163516964343417949_data = {\"mdsDat\": {\"Freq\": [10.101904734615003, 9.674920627218267, 9.213015876251426, 7.468571411933787, 6.824126986056338, 6.357460316569975, 5.87968255365452, 5.635238081072685, 4.746349207693556, 4.6907936523732605, 4.357460309768882, 4.0796825745605645, 3.8574603153652354, 3.7463492138819117, 3.190793652080877, 2.6352380968806552, 2.635238096099629, 1.6352380981135801, 1.6352380979778893, 1.6352380978319307], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"x\": [7.240583896636963, -9.646949768066406, 9.710748672485352, -24.184043884277344, -65.99597930908203, -55.838462829589844, -52.419456481933594, -29.879636764526367, -34.15970993041992, 13.39537239074707, 25.601421356201172, -43.8309211730957, -10.824325561523438, 7.080747604370117, -41.7886848449707, -31.88805389404297, -16.134815216064453, -11.162503242492676, -8.588459014892578, 29.763134002685547], \"y\": [62.883758544921875, 46.85895919799805, 11.567298889160156, 31.333545684814453, 24.003578186035156, 47.858726501464844, 2.7909281253814697, 10.594707489013672, -12.518440246582031, -9.988531112670898, 46.56504821777344, 27.192461013793945, -20.805816650390625, 34.48488998413086, 69.48165130615234, 49.533172607421875, 69.64692687988281, 0.976201057434082, 21.183185577392578, 20.532960891723633]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"Freq\": [2.0, 3.0, 2.0, 4.0, 4.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0522445573613046, 1.928646861776485, 1.222247123774703, 1.3550607698870771, 1.0526026440448675, 0.4263279008034961, 0.2692631117288994, 0.26107747656761965, 0.2602394684524624, 0.11443212995174254, 0.11206503975217215, 0.10655820093132523, 0.1019897235650853, 0.10071269313065509, 0.09995511495790056, 0.10132767900645166, 0.10013070474199928, 0.11304163653353434, 0.09786601935797376, 0.09457047059047727, 0.09605979124521889, 0.09606656847815516, 0.09703029690979832, 0.10771487402590604, 0.09555478965524752, 0.09554832845907545, 0.09700677989178805, 0.09776737605578834, 0.0965966146164592, 0.0973112407571515, 0.09971123519429353, 0.10732244804287297, 0.09857377534388166, 0.10373770098496583, 0.10851807991745104, 0.10358375001306501, 0.1007052287816304, 0.09890027450763382, 0.7139943807082426, 0.6984893955804724, 0.6759646752042753, 0.6421644228422363, 0.4224918567851938, 0.4241632148649912, 0.4499795643930421, 0.5593839221735646, 0.4053208689337775, 0.40875686880374473, 0.39958534439454246, 0.2946449844599516, 0.42216373225873144, 0.1748701792604998, 0.16777903563986687, 0.170508480380214, 0.17909109732811804, 0.16648527292269538, 0.16496522806834382, 0.16868866042489317, 0.16410915803526574, 0.1589103962731407, 0.16000829613764395, 0.16027468337446085, 0.16540491769331275, 0.15894025969010486, 0.16646700555225588, 0.15404318438192996, 0.16463846515767788, 0.161151970634689, 0.15962892605461224, 0.16633408447077003, 0.1686687188921568, 0.1634179441344022, 2.9305780478758248, 0.3785020384745439, 0.37393860188361916, 0.36458777269631076, 0.37105788401744755, 0.37100343029991595, 0.3583895667717825, 0.3575701101331092, 0.3508462146958509, 0.3521646523981544, 0.15612491406264398, 0.15589724901448052, 0.17453924326551737, 0.14769278833410676, 0.15057881336763973, 0.14191866814563617, 0.14300464151228878, 0.14578192869220805, 0.14443640944283043, 0.14580385852435312, 0.1488166619133022, 0.1384871315256255, 0.13761779952029224, 0.14038535633935342, 0.1423358216300652, 0.13355115463910666, 0.1351614193924526, 0.13212625681281132, 0.146567002943986, 0.14772239559130107, 0.14820100840406686, 0.15730745622443876, 0.1384052535409464, 0.13854567908932441, 0.14258432084607733, 0.15288334979362397, 0.14613823751820693, 0.14710068960059444, 0.15431408027197738, 0.14737809373333888, 0.986602912939675, 0.5809246658301812, 0.5452099058475426, 0.5436505382016488, 0.5212019361242491, 0.3551572113131054, 0.35112276282335025, 0.14841152683575784, 0.13795930680702292, 0.13816744275278545, 0.13777016382562154, 0.13745586839349416, 0.13548389389886173, 0.131258859825889, 0.13343220295994004, 0.13133674740581672, 0.1300819365181762, 0.13221798965957293, 0.12869385962505048, 0.13595627959226397, 0.12684722207161767, 0.12891737102547143, 0.13257576449515002, 0.1293038458816686, 0.12213777452325873, 0.12832736173142276, 0.12182811762227858, 0.12535644998006487, 0.13004532168470664, 0.1235000102349837, 0.12999845682172326, 0.13879267169542228, 0.1375377279788143, 0.14127880787957411, 0.13204367838429973, 0.1313616148129477, 0.1372912195170675, 0.13522115260432668, 0.5034573423207049, 0.5067682351330657, 0.33108814110568446, 0.3524859343341662, 0.34335822576996156, 0.33559049642220945, 0.3227563277086057, 0.3366742875743523, 0.35623989761357566, 0.1675466990806498, 0.15376054261938435, 0.14240985263938719, 0.13214611722991584, 0.134752207481451, 0.1385153806403047, 0.1411969701277587, 0.12824658407144524, 0.13144423051520246, 0.1262437816476259, 0.12949894797211184, 0.125474372372258, 0.126613396054949, 0.128676068136209, 0.12605384877748926, 0.11985490496487396, 0.14240986399589012, 0.12984849352748642, 0.11798491340712182, 0.12304549159091809, 0.11757441062300446, 0.12664433516469914, 0.1312229958774047, 0.12641031220519736, 0.13506703960904365, 0.12596548964369567, 0.4824400106110661, 0.48153208035542155, 0.4581846048290158, 0.5008470047884391, 0.4678895278044355, 0.3023468760121844, 0.3164885806018749, 0.12379252144706629, 0.1317580764180907, 0.11924807674390446, 0.13627748695783798, 0.12397267849828073, 0.11894567655201342, 0.11504247814469673, 0.1156914097834579, 0.12194832335418879, 0.1302854557957639, 0.12196214582111879, 0.1164631773895998, 0.11183687541286143, 0.10926439520119419, 0.11440174479662597, 0.1179234999434507, 0.10940747207820689, 0.11538467853712966, 0.11183977258072895, 0.11049193619743626, 0.11946159348881644, 0.10523560224646207, 0.10864461616553718, 0.11904428884859582, 0.11430255058099191, 0.11750644187364052, 0.11951369465908047, 0.12616177106303259, 0.11551121674622053, 0.12116581955923633, 0.11665708258073132, 0.12183884148718643, 0.11587839074727047, 1.144173193127531, 1.0203160246743532, 0.38480268653954885, 0.2602034438020216, 0.23151463825876326, 0.23217192434628303, 0.2141518597480327, 0.18795463433652823, 0.09908628664593454, 0.09869370305858517, 0.10037430402777979, 0.09567230489995043, 0.09285818041632299, 0.09321266382059436, 0.09338446008006664, 0.0934589973724849, 0.10392925749547217, 0.08965449142351974, 0.09843483445564034, 0.09764056638711269, 0.09198252913554629, 0.09089643203425762, 0.08663058668940805, 0.0908658216616883, 0.08805044489759846, 0.10208033112263115, 0.09315684920791734, 0.10226188700409819, 0.09766942106804448, 0.1004074301970018, 0.09111647209069977, 0.0998126881918834, 0.1145378520337988, 0.09302255286327153, 0.09842276773960588, 0.09372034318039317, 0.09301075387141283, 1.774977092577518, 0.38333255431720004, 0.3502893296019323, 0.23277904639551533, 0.23515920677302757, 0.22951823680275507, 0.2312792200431004, 0.23306884482009738, 0.11471554154375141, 0.1096500435097937, 0.09163301168350842, 0.09505499168489638, 0.09654380132551403, 0.09295105125836793, 0.0969359018607993, 0.0874817244242951, 0.09676869685900978, 0.08957547274953756, 0.086853083122144, 0.09119182630515475, 0.086641881648336, 0.08487991977064756, 0.08454634272056465, 0.08096562449723306, 0.08257116810998043, 0.08451915746594645, 0.08597954741618807, 0.08328472047209007, 0.0831679049182049, 0.08544313710252183, 0.09806970746035562, 0.0872609396285872, 0.08633373539969796, 0.08464116253846439, 0.09379506656799089, 0.09336034805062518, 0.4222454365349388, 0.2571819681288479, 0.25747481535022226, 0.24935549264239212, 0.2687916045872565, 0.1075529866978805, 0.10141754230549804, 0.10106627864996313, 0.09703647086850667, 0.09890009012162757, 0.10276927189813434, 0.1016687014881237, 0.10034843003062104, 0.10896284766557131, 0.09580989331332612, 0.09922155771454877, 0.09131089627444666, 0.09281379420288467, 0.09175471080717103, 0.11871130842111266, 0.09253011634813761, 0.10624543476878634, 0.09901861474560916, 0.09903121899140042, 0.09404221948547085, 0.09779346452979104, 0.09280857997832646, 0.09254484212698308, 0.10380738447470354, 0.09066674437633275, 0.093945568468679, 0.09898465560195915, 0.09391659111609763, 0.09402512942748578, 0.09831496341550391, 0.09820124822141646, 0.09711595903922991, 0.09730738523409638, 0.1027832254057087, 0.09434910701426316, 0.09666663184176494, 0.8249858027617072, 0.7295882416259059, 0.8440972244960862, 0.17887062820374894, 0.17936075505688504, 0.18200994225715647, 0.17741854269358032, 0.0877317577555312, 0.07912948814981471, 0.08638191759005237, 0.08441546392661266, 0.07552238071452422, 0.07204676085513928, 0.07763406762668432, 0.0716304034982919, 0.07058941980739589, 0.07526631539609228, 0.06780620940212491, 0.06855516810589975, 0.06893254087112731, 0.06939650409227932, 0.07125494187926182, 0.07635438519529862, 0.06982885503565878, 0.06797978744963501, 0.06993191146519372, 0.06678231079705156, 0.06872252557129001, 0.07092041771569337, 0.06736031543914031, 0.07106617340108218, 0.07221484957039463, 0.09297611999510133, 0.07368181995228658, 0.07030554345321609, 0.39843992661575633, 0.2463096425404097, 0.23869623884822336, 0.3787422084163567, 0.0988173702011431, 0.10007736338114803, 0.0931155820574865, 0.09129817408908425, 0.09405992917762306, 0.09114698130553196, 0.0973849291056668, 0.09322491559837268, 0.09002394781418362, 0.09598019202666949, 0.08599328681685971, 0.08897502758823324, 0.09613440390156179, 0.0858006601807245, 0.08566450176195459, 0.09702256691796493, 0.08617756468574489, 0.086669902641657, 0.09001603274997598, 0.08866113556178329, 0.08437044764408215, 0.0867644921194933, 0.08468229024636946, 0.07950603465078693, 0.08523726728232489, 0.08178138394548547, 0.084288534373028, 0.09044391412216271, 0.08967100973245588, 0.09434640516140506, 0.09265514663062904, 0.09087573020999698, 0.08790661739798494, 0.09315010298503035, 0.09286293222606141, 0.08821448055683402, 0.08851460338147635, 0.0870252062956779, 0.294945388058037, 0.2402480299770166, 0.23600463343350275, 0.22051994265890834, 0.23385590215416677, 0.11817686513851128, 0.09627324201982414, 0.08955099252343657, 0.09291841964414219, 0.09315574689252826, 0.09040823805877295, 0.0852010146977928, 0.08671970735253162, 0.0852035836214789, 0.08371099380399318, 0.08105963777117522, 0.08093825111031666, 0.07982655453250485, 0.07830446043064815, 0.07710408916312242, 0.07996031334085528, 0.08385244394284215, 0.08040400973252165, 0.08025933066402287, 0.08265941822114137, 0.0784950656745976, 0.07682681679470671, 0.08831860541316616, 0.07779194155468497, 0.08145252901622682, 0.08251197660171739, 0.08668431130972809, 0.09188442027234275, 0.08913105159216238, 0.08884717388862476, 0.08472858770967251, 0.09049334842377474, 0.08264085376466968, 0.08487710306468355, 0.08428055351327184, 0.08249480164560688, 0.08227263603079368, 0.33653371476741706, 0.29518504580708615, 0.21469813854054012, 0.20563452868882845, 0.11222849675668892, 0.09735209904511477, 0.09458486544014703, 0.08417353688680734, 0.09054763443980042, 0.0841083909612618, 0.08210081332784418, 0.07942768401371864, 0.080924873988983, 0.07960297589737936, 0.07929257131812976, 0.07578561489896667, 0.07807831342607412, 0.07865079964349979, 0.07642487632317911, 0.07747388840082399, 0.08470115314768184, 0.07747173879201949, 0.08149733039320645, 0.07433101070592726, 0.07479723393115537, 0.07967199966097559, 0.07595406265902495, 0.10237193390386007, 0.08073729160616283, 0.07983903100339373, 0.08322862549709133, 0.08148195154008896, 0.08204582696555769, 0.08418676764344797, 0.08012943992430482, 0.08138130821416428, 0.08031523028638947, 0.7871117892984376, 0.6950089775386776, 0.1527256275212938, 0.08319147716335454, 0.08804458560774048, 0.08304282982912721, 0.08326629509545602, 0.07432867446015817, 0.07360622022840035, 0.07060002957202768, 0.07101321797524145, 0.06960850288629017, 0.07011169300709721, 0.06846645419162561, 0.06739389550271098, 0.07067054878594617, 0.06710628638237967, 0.06502325187906888, 0.06357579250622498, 0.06500578038311979, 0.06604313685257707, 0.061856297662267384, 0.06562260731763418, 0.06579099744679315, 0.06176436832934986, 0.06335366396176896, 0.0628215800273547, 0.062448891321694316, 0.06167876463619566, 0.06999878880610508, 0.07947605034845114, 0.0630392696921133, 0.07407602489913342, 0.06454446457824262, 0.06348269381096583, 0.0634151017013563, 0.9222545142613404, 0.6546399521553059, 0.05702772028078192, 0.057299477720650846, 0.054270321849151044, 0.05439261509136416, 0.05255629925068607, 0.05345907657501116, 0.05599503590757923, 0.05630508220557641, 0.05067244068994163, 0.05316985775607579, 0.05157612172249231, 0.052144808545345735, 0.05451925777635668, 0.051617212796849644, 0.05249075179006337, 0.050492219962998325, 0.050051514342616364, 0.05143899507301192, 0.04879544314156529, 0.056808799354347274, 0.05055178492377167, 0.05701615412764828, 0.04949629957583333, 0.04949656886111301, 0.050945805589223556, 0.05547858021328603, 0.048095198016206896, 0.05540889122282008, 0.0506468383019725, 0.05307900266513018, 0.05147465375823792, 0.05048163281341604, 0.0506536380893646, 0.05467051995806407, 0.05219145105744483, 0.05166506440781802, 0.05292600859989945, 0.05291024363781824, 0.052158852263869376, 0.052687575872563015, 0.05145990900609571, 0.15332248696224682, 0.15838693375568355, 0.1473052078183192, 0.0642808453152192, 0.06423002968905932, 0.06591908704290475, 0.06179720614199155, 0.06018935158205603, 0.059038690718583885, 0.06284371438079801, 0.058467729549741033, 0.05814423423238714, 0.056099457421829944, 0.05776958694423315, 0.057636783348733546, 0.06405847536363055, 0.05685558105230446, 0.056732768731628684, 0.05659243008336874, 0.05454430279925451, 0.058763325152174604, 0.059137714862669374, 0.057894037462400405, 0.0544358477403327, 0.0565648723965026, 0.05417650093366254, 0.05392385625752103, 0.05783226005855972, 0.05234920291127262, 0.05418538502955961, 0.059902795042736566, 0.05856742010577361, 0.05988314030982153, 0.07560016506477304, 0.05873111889427781, 0.058430317706227954, 0.060446858697722844, 0.0572813658492, 0.058538932750165175, 0.05726381280063575, 0.8714718566161808, 0.12629838636859192, 0.05708512354719986, 0.051436653352959896, 0.05550678809941005, 0.05655624781177512, 0.054076614074732576, 0.050968983840593354, 0.04980970093491037, 0.050047211439030556, 0.0517513348786599, 0.0508144337369422, 0.05613061245183099, 0.05094719480565115, 0.04763130735548704, 0.04770370167569205, 0.046475167883402536, 0.04886134045017537, 0.052281465437279975, 0.0468896655330932, 0.04819844832683105, 0.04714193300736696, 0.047335219882252054, 0.04425143704214786, 0.045552947175268844, 0.050519822289174086, 0.04701907549059279, 0.043591405699619355, 0.04698062190320619, 0.04360643006569853, 0.04711597835710609, 0.04717547509453387, 0.04836832158090819, 0.04818739468816116, 0.05107432264514954, 0.04825351366044002, 0.04885382307487215, 0.048428911675759155, 0.048274717909004054, 0.04735930404619496, 0.047663216344311386, 0.046634913034085526, 0.04137514874876803, 0.0416916293075267, 0.04189083769898396, 0.043095200654117885, 0.040023079564608134, 0.04120468844771825, 0.03989755671455726, 0.04311195703079206, 0.04142008487884863, 0.03924435790682067, 0.043946640247296886, 0.03880394213293286, 0.03752777414884019, 0.03938417303492863, 0.03639140354858054, 0.037943446494887664, 0.03717762197343304, 0.038003743468361474, 0.03619176171329517, 0.03885377529019955, 0.03558817567640578, 0.03626609062603811, 0.04100537303501153, 0.035601755048234415, 0.03998743845964193, 0.03726211119745864, 0.035717085000459683, 0.03725709864281658, 0.03613337972186217, 0.03722345645370343, 0.037929264261685254, 0.04212708048702474, 0.039576065601248725, 0.03854870514891596, 0.04016986585317081, 0.038966311487956255, 0.03852678403529215, 0.03851625537614991, 0.04320799914147982, 0.042486999607254126, 0.04015310608041514, 0.041070325541668146, 0.042917954176904034, 0.04219728332711929, 0.04224019726378988, 0.03951895886211316, 0.04063396336600642, 0.039908961880880556, 0.03893707775637276, 0.03896327684232862, 0.03822127328661363, 0.039794756219643845, 0.03846344572606842, 0.037063912090676235, 0.03810357297600024, 0.03836400936699806, 0.03869440417318736, 0.037286313020971636, 0.03670269569771878, 0.039294179010272746, 0.036238734679845445, 0.038263928354161494, 0.03871077590382709, 0.036225990871285606, 0.03505361832524351, 0.03887023782575896, 0.03638956825270311, 0.03620382914705987, 0.04037985086380438, 0.037863412680464184, 0.038715275463559684, 0.040008849304014375, 0.039182681603529425, 0.04187710927931956, 0.03883643328499414, 0.0481599107984057, 0.03817736016126129, 0.03825586995310024, 0.05040093361028679, 0.04251217722286832, 0.040154137522242624, 0.040433962162273865, 0.041517579956304326, 0.03852675212480839, 0.0703100608825636, 0.041788628245473865, 0.03918993237558149, 0.03732432576575337, 0.038742544472136105, 0.038149162849387394, 0.03948711621839171, 0.03838199265906447, 0.03863945721424188, 0.038288646472273674, 0.03693148681849006, 0.03653413624905404, 0.03674323827775259, 0.03692118622786138, 0.037821091124446135, 0.03467465164637304, 0.036669378998700826, 0.03746565607218174, 0.037482314236957415, 0.03452347897074993, 0.03939778513483079, 0.03991562068215673, 0.03527162125224755, 0.034350271928013654, 0.07453755128395272, 0.03691464289212383, 0.03650041456804329, 0.039016455518705924, 0.03990061875073155, 0.03866636650283693, 0.039047469263336174, 0.03708962613226488, 0.0369793866786233], \"Term\": [\"got nan dog\", \"enjoy time family\", \"cat hanging upside\", \"mental health issues\", \"going try make\", \"cuddling black dog\", \"wave everyone else\", \"makes lot sense\", \"feel bit better\", \"oh well hope\", \"dog feral cat\", \"thanks going try\", \"thanks much comments\", \"sending virtual hugs\", \"darn black dog\", \"love go camping\", \"snarling german shepherd\", \"joined craft group\", \"much kind words\", \"hour drive away\", \"care good lady\", \"positivity motivation humour\", \"outside look stars\", \"better kind thoughts\", \"huge gum leaves\", \"wanting run away\", \"taken long time\", \"scenery driving along\", \"self help books\", \"loads virtual hugs\", \"going try make\", \"thanks going try\", \"hour drive away\", \"darn black dog\", \"oh well hope\", \"good day going\", \"write gratitude journal\", \"scenery driving along\", \"channel end game\", \"outside look stars\", \"go round emotions\", \"feeling really depressed\", \"hoping today better\", \"reading self help\", \"love go camping\", \"things know help\", \"merry go round\", \"sorry hear struggling\", \"joined craft group\", \"many wonderful people\", \"need find ways\", \"need fight depression\", \"exhausting full stop\", \"positivity motivation humour\", \"said really enjoyed\", \"taken long time\", \"loads virtual hugs\", \"laughter good soul\", \"feel cheerful happy\", \"pain mental health\", \"need find works\", \"dog feral cat\", \"fight tired unwell\", \"cat hanging upside\", \"cuddling black dog\", \"got nan dog\", \"feel bit better\", \"enjoy time family\", \"unwell tired trying\", \"fight tired unwell\", \"tired unwell tired\", \"need find works\", \"taken long time\", \"time depression explain\", \"explain people around\", \"got nan dog\", \"wondering time depression\", \"depression explain people\", \"thanks much comments\", \"much kind words\", \"mental health issues\", \"better kind thoughts\", \"hoping today better\", \"care good lady\", \"pain mental health\", \"self help books\", \"reading self help\", \"channel end game\", \"merry go round\", \"scenery driving along\", \"things know help\", \"need fight depression\", \"loads virtual hugs\", \"need find ways\", \"ants huge gum\", \"snarling german shepherd\", \"picnic drs office\", \"laughter good soul\", \"black dog right\", \"dog feral cat\", \"cuddling black dog\", \"hour drive away\", \"mental health issues\", \"need fight depression\", \"help feel better\", \"need find ways\", \"pain mental health\", \"country fire service\", \"feel cheerful happy\", \"black dog right\", \"dance around house\", \"cuddling black dog\", \"merry go round\", \"taken long time\", \"unwell tired trying\", \"self help books\", \"go round emotions\", \"many wonderful people\", \"joined craft group\", \"good day going\", \"feeling really depressed\", \"outside look stars\", \"laughter good soul\", \"hoping today better\", \"reading self help\", \"channel end game\", \"huge gum leaves\", \"write gratitude journal\", \"things know help\", \"love go camping\", \"need find works\", \"depression explain people\", \"explain people around\", \"thanks much comments\", \"hope okay today\", \"loads virtual hugs\", \"tired unwell tired\", \"cat hanging upside\", \"makes lot sense\", \"got nan dog\", \"going try make\", \"darn black dog\", \"sending virtual hugs\", \"please ever give\", \"huge gum leaves\", \"picnic drs office\", \"ants huge gum\", \"exhausting full stop\", \"ray ct scan\", \"outside look stars\", \"love go camping\", \"things know help\", \"go round emotions\", \"need fight depression\", \"good day going\", \"self help books\", \"wanting run away\", \"snarling german shepherd\", \"joined craft group\", \"much kind words\", \"hoping today better\", \"pain mental health\", \"reading self help\", \"need find ways\", \"hope okay today\", \"said really enjoyed\", \"scenery driving along\", \"taken long time\", \"many wonderful people\", \"feeling really depressed\", \"feel cheerful happy\", \"care good lady\", \"country fire service\", \"positivity motivation humour\", \"sorry hear struggling\", \"makes lot sense\", \"explain people around\", \"wave everyone else\", \"oh well hope\", \"enjoy time family\", \"better kind thoughts\", \"sorry hear struggling\", \"laughter good soul\", \"explain people around\", \"depression explain people\", \"wondering time depression\", \"time depression explain\", \"positivity motivation humour\", \"sending virtual hugs\", \"said really enjoyed\", \"feeling really depressed\", \"black dog right\", \"things know help\", \"good day going\", \"feel cheerful happy\", \"ants huge gum\", \"write gratitude journal\", \"wanting run away\", \"love go camping\", \"need find ways\", \"snarling german shepherd\", \"need fight depression\", \"exhausting full stop\", \"channel end game\", \"scenery driving along\", \"unwell tired trying\", \"country fire service\", \"many wonderful people\", \"taken long time\", \"self help books\", \"ray ct scan\", \"makes lot sense\", \"dance around house\", \"going try make\", \"oh well hope\", \"hope okay today\", \"loads virtual hugs\", \"wanting run away\", \"positivity motivation humour\", \"dance around house\", \"feeling really depressed\", \"ray ct scan\", \"scenery driving along\", \"country fire service\", \"self help books\", \"tired unwell tired\", \"feel cheerful happy\", \"need fight depression\", \"write gratitude journal\", \"care good lady\", \"laughter good soul\", \"fight tired unwell\", \"ants huge gum\", \"better kind thoughts\", \"go round emotions\", \"reading self help\", \"exhausting full stop\", \"picnic drs office\", \"merry go round\", \"pain mental health\", \"taken long time\", \"said really enjoyed\", \"depression explain people\", \"many wonderful people\", \"channel end game\", \"explain people around\", \"wondering time depression\", \"wave everyone else\", \"makes lot sense\", \"cat hanging upside\", \"sorry hear struggling\", \"hour drive away\", \"dog feral cat\", \"thanks going try\", \"thanks much comments\", \"feel bit better\", \"oh well hope\", \"said really enjoyed\", \"many wonderful people\", \"go round emotions\", \"laughter good soul\", \"merry go round\", \"care good lady\", \"self help books\", \"joined craft group\", \"hope okay today\", \"wanting run away\", \"love go camping\", \"feeling really depressed\", \"much kind words\", \"channel end game\", \"explain people around\", \"hoping today better\", \"need find works\", \"wondering time depression\", \"outside look stars\", \"good day going\", \"scenery driving along\", \"help feel better\", \"reading self help\", \"sorry hear struggling\", \"please ever give\", \"dance around house\", \"depression explain people\", \"fight tired unwell\", \"feel cheerful happy\", \"dog feral cat\", \"going try make\", \"tired unwell tired\", \"enjoy time family\", \"makes lot sense\", \"thanks much comments\", \"enjoy time family\", \"reading self help\", \"cat hanging upside\", \"self help books\", \"go round emotions\", \"hoping today better\", \"merry go round\", \"sorry hear struggling\", \"picnic drs office\", \"dance around house\", \"scenery driving along\", \"need find ways\", \"exhausting full stop\", \"things know help\", \"black dog right\", \"love go camping\", \"wondering time depression\", \"said really enjoyed\", \"joined craft group\", \"laughter good soul\", \"help feel better\", \"need fight depression\", \"good day going\", \"many wonderful people\", \"snarling german shepherd\", \"better kind thoughts\", \"ants huge gum\", \"hope okay today\", \"feel cheerful happy\", \"ray ct scan\", \"hour drive away\", \"unwell tired trying\", \"dog feral cat\", \"tired unwell tired\", \"mental health issues\", \"going try make\", \"joined craft group\", \"things know help\", \"outside look stars\", \"black dog right\", \"darn black dog\", \"help feel better\", \"self help books\", \"care good lady\", \"many wonderful people\", \"much kind words\", \"pain mental health\", \"hope okay today\", \"loads virtual hugs\", \"tired unwell tired\", \"go round emotions\", \"huge gum leaves\", \"scenery driving along\", \"hoping today better\", \"love go camping\", \"dog feral cat\", \"snarling german shepherd\", \"fight tired unwell\", \"picnic drs office\", \"ants huge gum\", \"said really enjoyed\", \"country fire service\", \"need fight depression\", \"channel end game\", \"sorry hear struggling\", \"wanting run away\", \"time depression explain\", \"explain people around\", \"wondering time depression\", \"need find works\", \"hour drive away\", \"cat hanging upside\", \"sending virtual hugs\", \"got nan dog\", \"mental health issues\", \"wave everyone else\", \"going try make\", \"dog feral cat\", \"thanks much comments\", \"cuddling black dog\", \"write gratitude journal\", \"hoping today better\", \"help feel better\", \"need find ways\", \"dance around house\", \"loads virtual hugs\", \"sorry hear struggling\", \"positivity motivation humour\", \"feel cheerful happy\", \"go round emotions\", \"wondering time depression\", \"feeling really depressed\", \"merry go round\", \"ants huge gum\", \"many wonderful people\", \"self help books\", \"things know help\", \"care good lady\", \"exhausting full stop\", \"depression explain people\", \"much kind words\", \"snarling german shepherd\", \"outside look stars\", \"reading self help\", \"need fight depression\", \"black dog right\", \"channel end game\", \"ray ct scan\", \"tired unwell tired\", \"mental health issues\", \"got nan dog\", \"cat hanging upside\", \"snarling german shepherd\", \"feel cheerful happy\", \"outside look stars\", \"cuddling black dog\", \"merry go round\", \"help feel better\", \"hoping today better\", \"love go camping\", \"need find ways\", \"reading self help\", \"time depression explain\", \"said really enjoyed\", \"self help books\", \"ray ct scan\", \"many wonderful people\", \"channel end game\", \"depression explain people\", \"joined craft group\", \"feeling really depressed\", \"fight tired unwell\", \"taken long time\", \"exhausting full stop\", \"ants huge gum\", \"laughter good soul\", \"good day going\", \"loads virtual hugs\", \"better kind thoughts\", \"scenery driving along\", \"huge gum leaves\", \"much kind words\", \"hope okay today\", \"wave everyone else\", \"tired unwell tired\", \"thanks much comments\", \"dog feral cat\", \"makes lot sense\", \"dance around house\", \"thanks going try\", \"going try make\", \"enjoy time family\", \"mental health issues\", \"oh well hope\", \"care good lady\", \"channel end game\", \"exhausting full stop\", \"things know help\", \"country fire service\", \"hope okay today\", \"loads virtual hugs\", \"write gratitude journal\", \"outside look stars\", \"pain mental health\", \"huge gum leaves\", \"joined craft group\", \"much kind words\", \"go round emotions\", \"good day going\", \"hoping today better\", \"wanting run away\", \"merry go round\", \"love go camping\", \"scenery driving along\", \"feeling really depressed\", \"black dog right\", \"said really enjoyed\", \"taken long time\", \"laughter good soul\", \"need find ways\", \"snarling german shepherd\", \"sorry hear struggling\", \"better kind thoughts\", \"ray ct scan\", \"fight tired unwell\", \"dog feral cat\", \"got nan dog\", \"feel bit better\", \"hour drive away\", \"makes lot sense\", \"mental health issues\", \"thanks much comments\", \"cuddling black dog\", \"oh well hope\", \"thanks going try\", \"going try make\", \"love go camping\", \"much kind words\", \"many wonderful people\", \"pain mental health\", \"laughter good soul\", \"go round emotions\", \"merry go round\", \"scenery driving along\", \"hope okay today\", \"write gratitude journal\", \"channel end game\", \"hoping today better\", \"feeling really depressed\", \"care good lady\", \"outside look stars\", \"reading self help\", \"good day going\", \"taken long time\", \"snarling german shepherd\", \"need find ways\", \"depression explain people\", \"help feel better\", \"ray ct scan\", \"self help books\", \"things know help\", \"country fire service\", \"said really enjoyed\", \"feel bit better\", \"need find works\", \"wondering time depression\", \"tired unwell tired\", \"explain people around\", \"unwell tired trying\", \"dog feral cat\", \"hour drive away\", \"cuddling black dog\", \"enjoy time family\", \"makes lot sense\", \"cuddling black dog\", \"feeling really depressed\", \"self help books\", \"ants huge gum\", \"huge gum leaves\", \"need find works\", \"love go camping\", \"black dog right\", \"need fight depression\", \"outside look stars\", \"go round emotions\", \"good day going\", \"care good lady\", \"joined craft group\", \"please ever give\", \"snarling german shepherd\", \"write gratitude journal\", \"reading self help\", \"wanting run away\", \"loads virtual hugs\", \"hoping today better\", \"pain mental health\", \"laughter good soul\", \"merry go round\", \"better kind thoughts\", \"said really enjoyed\", \"channel end game\", \"need find ways\", \"unwell tired trying\", \"thanks going try\", \"ray ct scan\", \"mental health issues\", \"feel bit better\", \"thanks much comments\", \"oh well hope\", \"cat hanging upside\", \"wave everyone else\", \"merry go round\", \"outside look stars\", \"hoping today better\", \"need fight depression\", \"write gratitude journal\", \"much kind words\", \"country fire service\", \"ants huge gum\", \"scenery driving along\", \"better kind thoughts\", \"snarling german shepherd\", \"wanting run away\", \"laughter good soul\", \"feeling really depressed\", \"exhausting full stop\", \"things know help\", \"self help books\", \"said really enjoyed\", \"many wonderful people\", \"tired unwell tired\", \"channel end game\", \"dance around house\", \"good day going\", \"help feel better\", \"loads virtual hugs\", \"fight tired unwell\", \"joined craft group\", \"positivity motivation humour\", \"time depression explain\", \"explain people around\", \"wondering time depression\", \"pain mental health\", \"ray ct scan\", \"thanks going try\", \"sending virtual hugs\", \"dog feral cat\", \"enjoy time family\", \"oh well hope\", \"darn black dog\", \"going try make\", \"hour drive away\", \"scenery driving along\", \"taken long time\", \"self help books\", \"care good lady\", \"feeling really depressed\", \"black dog right\", \"merry go round\", \"reading self help\", \"hoping today better\", \"pain mental health\", \"said really enjoyed\", \"channel end game\", \"write gratitude journal\", \"much kind words\", \"good day going\", \"explain people around\", \"go round emotions\", \"need find ways\", \"need fight depression\", \"love go camping\", \"country fire service\", \"picnic drs office\", \"time depression explain\", \"joined craft group\", \"exhausting full stop\", \"snarling german shepherd\", \"things know help\", \"ants huge gum\", \"many wonderful people\", \"help feel better\", \"wave everyone else\", \"depression explain people\", \"tired unwell tired\", \"darn black dog\", \"dance around house\", \"positivity motivation humour\", \"thanks much comments\", \"sorry hear struggling\", \"feel bit better\", \"unwell tired trying\", \"got nan dog\", \"need fight depression\", \"write gratitude journal\", \"many wonderful people\", \"ray ct scan\", \"depression explain people\", \"black dog right\", \"feeling really depressed\", \"merry go round\", \"go round emotions\", \"huge gum leaves\", \"outside look stars\", \"fight tired unwell\", \"time depression explain\", \"joined craft group\", \"snarling german shepherd\", \"love go camping\", \"please ever give\", \"wave everyone else\", \"channel end game\", \"feel cheerful happy\", \"exhausting full stop\", \"loads virtual hugs\", \"self help books\", \"help feel better\", \"explain people around\", \"laughter good soul\", \"reading self help\", \"country fire service\", \"care good lady\", \"wondering time depression\", \"need find works\", \"tired unwell tired\", \"makes lot sense\", \"enjoy time family\", \"dog feral cat\", \"feel bit better\", \"sending virtual hugs\", \"darn black dog\", \"wanting run away\", \"help feel better\", \"country fire service\", \"scenery driving along\", \"hoping today better\", \"need find ways\", \"huge gum leaves\", \"love go camping\", \"much kind words\", \"snarling german shepherd\", \"wondering time depression\", \"pain mental health\", \"said really enjoyed\", \"positivity motivation humour\", \"good day going\", \"joined craft group\", \"feel cheerful happy\", \"write gratitude journal\", \"outside look stars\", \"taken long time\", \"loads virtual hugs\", \"merry go round\", \"ray ct scan\", \"self help books\", \"feeling really depressed\", \"sorry hear struggling\", \"care good lady\", \"wave everyone else\", \"black dog right\", \"channel end game\", \"picnic drs office\", \"exhausting full stop\", \"need find works\", \"fight tired unwell\", \"mental health issues\", \"enjoy time family\", \"got nan dog\", \"going try make\", \"thanks going try\", \"oh well hope\", \"cuddling black dog\", \"scenery driving along\", \"better kind thoughts\", \"hoping today better\", \"wanting run away\", \"black dog right\", \"huge gum leaves\", \"feel cheerful happy\", \"write gratitude journal\", \"good day going\", \"merry go round\", \"many wonderful people\", \"much kind words\", \"snarling german shepherd\", \"hope okay today\", \"need find ways\", \"love go camping\", \"need fight depression\", \"exhausting full stop\", \"loads virtual hugs\", \"help feel better\", \"care good lady\", \"ray ct scan\", \"joined craft group\", \"please ever give\", \"ants huge gum\", \"feeling really depressed\", \"self help books\", \"need find works\", \"outside look stars\", \"taken long time\", \"fight tired unwell\", \"picnic drs office\", \"depression explain people\", \"unwell tired trying\", \"explain people around\", \"dog feral cat\", \"wave everyone else\", \"cuddling black dog\", \"positivity motivation humour\", \"hour drive away\", \"better kind thoughts\", \"love go camping\", \"write gratitude journal\", \"care good lady\", \"time depression explain\", \"reading self help\", \"thanks going try\", \"ray ct scan\", \"much kind words\", \"scenery driving along\", \"feeling really depressed\", \"wanting run away\", \"feel cheerful happy\", \"said really enjoyed\", \"huge gum leaves\", \"loads virtual hugs\", \"need fight depression\", \"need find ways\", \"channel end game\", \"outside look stars\", \"laughter good soul\", \"many wonderful people\", \"please ever give\", \"picnic drs office\", \"ants huge gum\", \"snarling german shepherd\", \"tired unwell tired\", \"unwell tired trying\", \"exhausting full stop\", \"go round emotions\", \"going try make\", \"need find works\", \"wondering time depression\", \"makes lot sense\", \"cat hanging upside\", \"got nan dog\", \"cuddling black dog\", \"thanks much comments\", \"mental health issues\"], \"Total\": [2.0, 3.0, 2.0, 4.0, 4.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 4.660025197143904, 3.5245712034508325, 2.744760627609668, 3.056940103159428, 3.5123017588318, 1.988766102770613, 1.9235415855879756, 1.8967922129551251, 1.9907627850045375, 2.0116221201774276, 1.971946898051339, 1.9729399244691939, 1.9316477915327956, 1.9283843161872585, 1.9191096760785538, 1.947146090487758, 1.955061333758537, 2.242974123878801, 1.9445998573199554, 1.90891224112209, 1.97863638441212, 1.9864087481341977, 2.0209665023570027, 2.2458368751204407, 1.9984056674109092, 2.0042769960134112, 2.0512799149419663, 2.072062881613831, 2.0476029778317795, 2.06419732298803, 2.1486767799833384, 2.4851598409624063, 2.241796425941514, 2.794103674413462, 3.5841595831359236, 2.8698534767434265, 2.6995989254554913, 3.3085526731923864, 2.2762303240847683, 2.241796425941514, 2.23304424504162, 2.1486767799833384, 2.0042769960134112, 2.0606444282258916, 2.2191786083110947, 2.8698534767434265, 2.133534311962926, 2.1657841104643007, 2.5370813120609417, 1.9821331362258412, 4.73009681119594, 2.0090791235182452, 1.9316477915327956, 1.965096166399861, 2.06419732298803, 1.932467842954945, 1.9283843161872585, 1.9907627850045375, 1.955061333758537, 1.8967922129551251, 1.947146090487758, 1.9864087481341977, 2.0512799149419663, 1.97863638441212, 2.1026445493076324, 1.9505444580702367, 2.100978067840813, 2.072062881613831, 2.0709539587525394, 2.4851598409624063, 3.5841595831359236, 2.744760627609668, 4.73009681119594, 1.9864087481341977, 1.9896627145201986, 1.97863638441212, 2.06419732298803, 2.0807392729994385, 2.0476029778317795, 2.0709539587525394, 2.266862105249622, 3.5841595831359236, 1.955061333758537, 2.0042769960134112, 2.2762303240847683, 1.932467842954945, 1.971946898051339, 1.90891224112209, 1.9445998573199554, 1.988766102770613, 1.9729399244691939, 2.0116221201774276, 2.072062881613831, 1.9316477915327956, 1.9283843161872585, 1.9907627850045375, 2.044283454136337, 1.9235415855879756, 1.947146090487758, 1.9191096760785538, 2.1486767799833384, 2.1657841104643007, 2.2191786083110947, 2.5370813120609417, 2.046012884401244, 2.0512799149419663, 2.23304424504162, 2.794103674413462, 2.416281736415366, 2.8698534767434265, 4.660025197143904, 3.056940103159428, 2.7011690387244953, 2.053693943108422, 2.044283454136337, 2.100978067840813, 2.1026445493076324, 2.0209665023570027, 2.105314963296152, 2.0116221201774276, 1.9191096760785538, 1.947146090487758, 1.971946898051339, 1.9864087481341977, 1.988766102770613, 1.932467842954945, 1.9757775477997657, 1.9505444580702367, 1.9445998573199554, 1.9821331362258412, 1.9316477915327956, 2.06419732298803, 1.9283843161872585, 1.97863638441212, 2.046012884401244, 1.9984056674109092, 1.8967922129551251, 2.0042769960134112, 1.90891224112209, 1.9729399244691939, 2.0476029778317795, 1.965096166399861, 2.0807392729994385, 2.2458368751204407, 2.242974123878801, 2.416281736415366, 2.2191786083110947, 2.2163292847021547, 3.5123017588318, 3.3085526731923864, 2.0090791235182452, 2.242974123878801, 2.072062881613831, 2.2191786083110947, 2.1657841104643007, 2.133534311962926, 2.0606444282258916, 2.2458368751204407, 2.7011690387244953, 1.9984056674109092, 1.9729399244691939, 2.0709539587525394, 1.947146090487758, 1.988766102770613, 2.0476029778317795, 2.1026445493076324, 1.9235415855879756, 1.9757775477997657, 1.9191096760785538, 1.97863638441212, 1.9505444580702367, 1.9864087481341977, 2.0209665023570027, 1.9907627850045375, 1.8967922129551251, 2.2762303240847683, 2.0807392729994385, 1.90891224112209, 2.0042769960134112, 1.932467842954945, 2.105314963296152, 2.416281736415366, 2.266862105249622, 4.660025197143904, 3.5123017588318, 2.046012884401244, 2.0512799149419663, 1.9757775477997657, 2.2458368751204407, 2.266862105249622, 1.9729399244691939, 2.105314963296152, 1.8967922129551251, 2.0807392729994385, 1.932467842954945, 2.23304424504162, 2.0476029778317795, 1.9864087481341977, 1.9235415855879756, 1.965096166399861, 2.072062881613831, 2.241796425941514, 2.1026445493076324, 2.0090791235182452, 1.971946898051339, 1.9283843161872585, 2.0209665023570027, 2.100978067840813, 1.955061333758537, 2.06419732298803, 2.0042769960134112, 1.9984056674109092, 2.1657841104643007, 1.90891224112209, 1.9907627850045375, 2.2191786083110947, 2.133534311962926, 2.2163292847021547, 2.416281736415366, 2.794103674413462, 2.242974123878801, 2.744760627609668, 2.4851598409624063, 3.5245712034508325, 2.5370813120609417, 2.6995989254554913, 3.5123017588318, 1.9984056674109092, 1.90891224112209, 1.971946898051339, 2.072062881613831, 1.955061333758537, 1.965096166399861, 1.932467842954945, 1.9445998573199554, 2.046012884401244, 1.9757775477997657, 1.9191096760785538, 1.9729399244691939, 1.9821331362258412, 1.9907627850045375, 2.2191786083110947, 1.9316477915327956, 2.1486767799833384, 2.133534311962926, 2.0116221201774276, 1.988766102770613, 1.8967922129551251, 1.9896627145201986, 1.9283843161872585, 2.242974123878801, 2.053693943108422, 2.266862105249622, 2.1657841104643007, 2.241796425941514, 2.0476029778317795, 2.4851598409624063, 4.660025197143904, 2.23304424504162, 3.3085526731923864, 2.416281736415366, 2.5370813120609417, 3.3085526731923864, 1.9283843161872585, 2.794103674413462, 1.932467842954945, 1.971946898051339, 1.9316477915327956, 1.955061333758537, 2.242974123878801, 2.100978067840813, 2.266862105249622, 1.8967922129551251, 1.97863638441212, 2.0209665023570027, 1.947146090487758, 2.0709539587525394, 1.9191096760785538, 2.133534311962926, 1.9984056674109092, 1.9445998573199554, 2.072062881613831, 1.9896627145201986, 1.9864087481341977, 1.988766102770613, 1.90891224112209, 1.9505444580702367, 2.0090791235182452, 2.1026445493076324, 2.046012884401244, 2.0476029778317795, 2.105314963296152, 2.744760627609668, 2.2762303240847683, 2.4851598409624063, 2.23304424504162, 4.73009681119594, 4.660025197143904, 1.9445998573199554, 1.947146090487758, 2.0116221201774276, 2.0709539587525394, 3.056940103159428, 1.9896627145201986, 1.932467842954945, 1.965096166399861, 1.90891224112209, 1.9821331362258412, 2.06419732298803, 2.046012884401244, 2.0512799149419663, 2.23304424504162, 1.971946898051339, 2.044283454136337, 1.8967922129551251, 1.9316477915327956, 1.9191096760785538, 2.4851598409624063, 1.9505444580702367, 2.241796425941514, 2.100978067840813, 2.1026445493076324, 1.9984056674109092, 2.0807392729994385, 1.9864087481341977, 1.9907627850045375, 2.242974123878801, 1.9757775477997657, 2.0606444282258916, 2.2191786083110947, 2.133534311962926, 2.1486767799833384, 2.744760627609668, 2.794103674413462, 2.7011690387244953, 2.8698534767434265, 4.73009681119594, 2.2163292847021547, 4.660025197143904, 2.4851598409624063, 2.5370813120609417, 3.5841595831359236, 1.9235415855879756, 1.9316477915327956, 1.9896627145201986, 1.97863638441212, 2.266862105249622, 2.0512799149419663, 2.242974123878801, 2.2458368751204407, 2.0476029778317795, 1.971946898051339, 2.133534311962926, 1.9729399244691939, 1.955061333758537, 2.1026445493076324, 1.90891224112209, 1.932467842954945, 1.947146090487758, 1.965096166399861, 2.0209665023570027, 2.1657841104643007, 1.9821331362258412, 1.9505444580702367, 2.0116221201774276, 1.9283843161872585, 1.9864087481341977, 2.0709539587525394, 1.9907627850045375, 2.105314963296152, 2.23304424504162, 4.73009681119594, 2.8698534767434265, 2.794103674413462, 1.9505444580702367, 2.0476029778317795, 2.0116221201774276, 3.5841595831359236, 1.955061333758537, 1.9896627145201986, 1.9316477915327956, 1.9191096760785538, 1.97863638441212, 1.9283843161872585, 2.0606444282258916, 1.9984056674109092, 1.932467842954945, 2.105314963296152, 1.90891224112209, 1.9907627850045375, 2.1657841104643007, 1.9445998573199554, 1.9729399244691939, 2.241796425941514, 2.0042769960134112, 2.0209665023570027, 2.1026445493076324, 2.072062881613831, 1.988766102770613, 2.0512799149419663, 2.0090791235182452, 1.8967922129551251, 2.044283454136337, 1.9821331362258412, 2.046012884401244, 2.2163292847021547, 2.23304424504162, 2.5370813120609417, 2.4851598409624063, 2.416281736415366, 2.266862105249622, 3.5245712034508325, 4.660025197143904, 3.3085526731923864, 4.73009681119594, 3.5123017588318, 1.965096166399861, 1.9907627850045375, 2.0209665023570027, 1.947146090487758, 2.0807392729994385, 2.046012884401244, 2.0512799149419663, 1.9235415855879756, 2.0116221201774276, 2.06419732298803, 2.044283454136337, 1.9445998573199554, 1.9821331362258412, 1.971946898051339, 1.988766102770613, 1.9316477915327956, 1.9757775477997657, 1.955061333758537, 1.9191096760785538, 1.8967922129551251, 1.9729399244691939, 2.0709539587525394, 1.9984056674109092, 2.0042769960134112, 2.072062881613831, 1.97863638441212, 1.9505444580702367, 2.242974123878801, 2.0090791235182452, 2.105314963296152, 2.241796425941514, 2.4851598409624063, 2.8698534767434265, 2.6995989254554913, 2.744760627609668, 2.416281736415366, 4.73009681119594, 2.5370813120609417, 3.5841595831359236, 3.5123017588318, 3.5245712034508325, 4.660025197143904, 1.9191096760785538, 1.9821331362258412, 1.90891224112209, 2.06419732298803, 2.072062881613831, 1.971946898051339, 1.955061333758537, 1.8967922129551251, 2.046012884401244, 1.9235415855879756, 1.9907627850045375, 1.9316477915327956, 1.9729399244691939, 1.965096166399861, 2.0116221201774276, 1.9283843161872585, 1.988766102770613, 2.0042769960134112, 1.9505444580702367, 1.97863638441212, 2.1657841104643007, 1.9896627145201986, 2.105314963296152, 1.932467842954945, 1.947146090487758, 2.0807392729994385, 1.9984056674109092, 2.6995989254554913, 2.1486767799833384, 2.133534311962926, 2.23304424504162, 2.2191786083110947, 2.2762303240847683, 2.4851598409624063, 2.744760627609668, 3.5841595831359236, 3.3085526731923864, 2.416281736415366, 3.5841595831359236, 1.9729399244691939, 1.932467842954945, 2.1026445493076324, 2.044283454136337, 2.1486767799833384, 1.9191096760785538, 2.0709539587525394, 1.9864087481341977, 2.0116221201774276, 1.971946898051339, 1.988766102770613, 1.965096166399861, 1.9445998573199554, 2.053693943108422, 1.9505444580702367, 1.9235415855879756, 1.9283843161872585, 1.9757775477997657, 2.0512799149419663, 1.9316477915327956, 2.06419732298803, 2.072062881613831, 1.955061333758537, 2.0090791235182452, 1.9984056674109092, 1.9907627850045375, 1.97863638441212, 2.2762303240847683, 3.5245712034508325, 2.105314963296152, 4.73009681119594, 2.6995989254554913, 2.5370813120609417, 3.5123017588318, 2.794103674413462, 2.2163292847021547, 1.955061333758537, 2.0116221201774276, 1.9316477915327956, 1.9864087481341977, 1.9235415855879756, 1.9821331362258412, 2.0807392729994385, 2.1026445493076324, 1.8967922129551251, 2.0090791235182452, 1.9505444580702367, 1.9757775477997657, 2.072062881613831, 1.9729399244691939, 2.0209665023570027, 1.947146090487758, 1.932467842954945, 1.9984056674109092, 1.90891224112209, 2.23304424504162, 1.9907627850045375, 2.266862105249622, 1.988766102770613, 1.9896627145201986, 2.0512799149419663, 2.241796425941514, 1.9445998573199554, 2.2458368751204407, 2.0606444282258916, 2.2191786083110947, 2.133534311962926, 2.06419732298803, 2.105314963296152, 3.5245712034508325, 2.7011690387244953, 2.4851598409624063, 3.3085526731923864, 3.5123017588318, 3.056940103159428, 4.660025197143904, 2.744760627609668, 1.8967922129551251, 2.0042769960134112, 1.932467842954945, 1.965096166399861, 1.9729399244691939, 2.0709539587525394, 1.955061333758537, 1.9283843161872585, 1.9316477915327956, 2.06419732298803, 1.9984056674109092, 1.9907627850045375, 1.9235415855879756, 1.9821331362258412, 1.988766102770613, 2.2191786083110947, 1.971946898051339, 1.97863638441212, 1.9864087481341977, 1.9191096760785538, 2.0807392729994385, 2.100978067840813, 2.0606444282258916, 1.9445998573199554, 2.0209665023570027, 1.9505444580702367, 1.947146090487758, 2.1026445493076324, 1.90891224112209, 1.9896627145201986, 2.2163292847021547, 2.1657841104643007, 2.23304424504162, 3.056940103159428, 2.266862105249622, 2.2458368751204407, 2.5370813120609417, 2.242974123878801, 2.6995989254554913, 2.2762303240847683, 2.8698534767434265, 1.9864087481341977, 1.9235415855879756, 1.90891224112209, 2.105314963296152, 2.1657841104643007, 2.0709539587525394, 1.9729399244691939, 1.955061333758537, 1.971946898051339, 2.044283454136337, 2.0116221201774276, 2.241796425941514, 2.0606444282258916, 1.9445998573199554, 1.9505444580702367, 1.9191096760785538, 2.053693943108422, 2.2163292847021547, 1.9907627850045375, 2.0476029778317795, 2.0209665023570027, 2.0512799149419663, 1.932467842954945, 1.9896627145201986, 2.2191786083110947, 2.072062881613831, 1.9283843161872585, 2.0807392729994385, 1.965096166399861, 2.133534311962926, 2.1486767799833384, 2.23304424504162, 2.416281736415366, 3.3085526731923864, 2.4851598409624063, 2.6995989254554913, 2.7011690387244953, 3.056940103159428, 1.9757775477997657, 1.9896627145201986, 2.0807392729994385, 1.8967922129551251, 1.9316477915327956, 1.97863638441212, 2.044283454136337, 1.9191096760785538, 1.9821331362258412, 1.9505444580702367, 2.133534311962926, 2.06419732298803, 1.9984056674109092, 2.2458368751204407, 1.988766102770613, 1.9445998573199554, 2.0476029778317795, 1.9235415855879756, 2.0116221201774276, 2.0042769960134112, 2.0512799149419663, 1.955061333758537, 2.105314963296152, 1.932467842954945, 1.9729399244691939, 2.242974123878801, 1.965096166399861, 2.2163292847021547, 2.0709539587525394, 1.9907627850045375, 2.100978067840813, 2.0209665023570027, 2.1486767799833384, 2.241796425941514, 4.73009681119594, 3.3085526731923864, 2.8698534767434265, 4.660025197143904, 3.5245712034508325, 3.5123017588318, 3.5841595831359236, 1.8967922129551251, 2.0090791235182452, 1.9316477915327956, 1.9757775477997657, 2.0709539587525394, 2.044283454136337, 2.0476029778317795, 1.9235415855879756, 1.988766102770613, 1.955061333758537, 1.90891224112209, 1.9821331362258412, 1.9505444580702367, 2.046012884401244, 1.97863638441212, 1.9191096760785538, 1.9864087481341977, 2.0209665023570027, 2.0512799149419663, 1.9896627145201986, 1.965096166399861, 2.105314963296152, 1.9445998573199554, 2.053693943108422, 2.1026445493076324, 1.9729399244691939, 1.932467842954945, 2.1486767799833384, 2.0116221201774276, 2.0042769960134112, 2.241796425941514, 2.100978067840813, 2.1657841104643007, 2.2762303240847683, 2.2191786083110947, 2.4851598409624063, 2.2163292847021547, 3.5841595831359236, 2.2458368751204407, 2.744760627609668, 2.0090791235182452, 1.9191096760785538, 1.9235415855879756, 1.965096166399861, 2.0606444282258916, 1.9283843161872585, 3.5245712034508325, 2.105314963296152, 1.9821331362258412, 1.8967922129551251, 1.9729399244691939, 1.9757775477997657, 2.0476029778317795, 1.9984056674109092, 2.044283454136337, 2.0512799149419663, 1.9864087481341977, 1.97863638441212, 1.9907627850045375, 2.0116221201774276, 2.072062881613831, 1.90891224112209, 2.053693943108422, 2.100978067840813, 2.1026445493076324, 1.9505444580702367, 2.23304424504162, 2.2762303240847683, 2.0209665023570027, 1.971946898051339, 4.660025197143904, 2.1486767799833384, 2.133534311962926, 2.416281736415366, 2.794103674413462, 2.8698534767434265, 3.5841595831359236, 2.5370813120609417, 4.73009681119594], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.8693, 1.6895, 1.4834, 1.4789, 1.0874, 0.7524, 0.3262, 0.3093, 0.2578, -0.5743, -0.5753, -0.6261, -0.6488, -0.6597, -0.6624, -0.6633, -0.6793, -0.6954, -0.6968, -0.7125, -0.7327, -0.7366, -0.7439, -0.7449, -0.748, -0.751, -0.759, -0.7613, -0.7614, -0.7621, -0.7779, -0.8498, -0.8318, -1.001, -1.2049, -1.0292, -0.9962, -1.2177, 1.1762, 1.1695, 1.1407, 1.1279, 0.7788, 0.755, 0.7399, 0.7005, 0.6748, 0.6682, 0.4873, 0.4295, -0.0807, -0.1058, -0.1078, -0.1089, -0.109, -0.116, -0.1231, -0.1326, -0.142, -0.1439, -0.1633, -0.1816, -0.1822, -0.186, -0.2005, -0.203, -0.2108, -0.2183, -0.2273, -0.3685, -0.7207, -0.4855, 1.9058, 0.7267, 0.7129, 0.6932, 0.6684, 0.6603, 0.6417, 0.6281, 0.5187, 0.0644, -0.143, -0.1693, -0.1836, -0.1869, -0.1877, -0.2145, -0.2254, -0.2286, -0.2299, -0.2399, -0.249, -0.2508, -0.2554, -0.2673, -0.2801, -0.2829, -0.2831, -0.2913, -0.3006, -0.3007, -0.3218, -0.396, -0.3089, -0.3105, -0.3666, -0.521, -0.4209, -0.5863, -1.0232, -0.6476, 1.5873, 1.3317, 1.2728, 1.2426, 1.1997, 0.8557, 0.8034, -0.0122, -0.0382, -0.0512, -0.0667, -0.0763, -0.092, -0.0949, -0.1007, -0.1036, -0.1102, -0.113, -0.1142, -0.1257, -0.127, -0.1365, -0.142, -0.1435, -0.1483, -0.154, -0.1572, -0.1617, -0.1621, -0.1726, -0.1785, -0.1894, -0.1972, -0.2448, -0.2273, -0.2312, -0.6475, -0.6029, 1.3008, 1.1972, 0.8508, 0.8448, 0.8429, 0.8351, 0.8308, 0.787, 0.6589, 0.2059, 0.1328, 0.0077, -0.0055, -0.0071, -0.0087, -0.0161, -0.0233, -0.0254, -0.0367, -0.0418, -0.0591, -0.0682, -0.0693, -0.0749, -0.0769, -0.0869, -0.0894, -0.099, -0.1058, -0.1148, -0.1261, -0.2284, -0.2019, -0.8563, -0.6433, 1.3107, 1.3063, 1.2941, 1.255, 1.1776, 0.8798, 0.8606, 0.0262, -0.004, -0.0298, -0.0409, -0.0488, -0.0599, -0.0611, -0.0768, -0.0772, -0.0898, -0.0917, -0.0923, -0.1142, -0.1151, -0.1161, -0.1246, -0.1276, -0.1287, -0.1304, -0.1396, -0.142, -0.1425, -0.1526, -0.1699, -0.1711, -0.1816, -0.251, -0.3422, -0.2106, -0.3647, -0.3033, -0.6093, -0.3307, 1.9752, 1.5975, 1.1863, 0.8408, 0.6915, 0.6448, 0.6222, 0.4866, -0.1369, -0.1471, -0.1811, -0.1941, -0.1949, -0.2187, -0.2215, -0.2251, -0.2275, -0.2365, -0.2495, -0.2506, -0.2514, -0.2519, -0.2526, -0.2527, -0.2529, -0.2561, -0.2594, -0.2649, -0.2653, -0.2721, -0.2786, -0.3811, -0.8722, -0.3446, -0.6813, -0.416, -0.4724, 2.2534, 1.2606, 0.7996, 0.7597, 0.7496, 0.746, 0.7416, 0.6119, -0.0316, -0.1527, -0.154, -0.1596, -0.1652, -0.1659, -0.1856, -0.2121, -0.2171, -0.2289, -0.2325, -0.2472, -0.2578, -0.2767, -0.2818, -0.2841, -0.2861, -0.2923, -0.3207, -0.3253, -0.3274, -0.3282, -0.4556, -0.3852, -0.4837, -0.3966, -1.0445, -1.0342, 1.5206, 1.0235, 0.992, 0.9309, 0.6166, 0.1301, 0.1005, 0.0803, 0.0686, 0.05, 0.0478, 0.0459, 0.0302, 0.0277, 0.0234, 0.0223, 0.0141, 0.0123, 0.0073, 0.0064, -0.0005, -0.0015, -0.0071, -0.0077, -0.0086, -0.0098, -0.0158, -0.0208, -0.0252, -0.0337, -0.0403, -0.0621, -0.0753, -0.0813, -0.2815, -0.3005, -0.2777, -0.3363, -0.7813, -0.1088, -0.8277, 1.9568, 1.8133, 1.6136, 0.6843, 0.6828, 0.6679, 0.6479, -0.1923, -0.1956, -0.1972, -0.2215, -0.2404, -0.2499, -0.254, -0.2562, -0.2617, -0.2704, -0.2781, -0.2793, -0.2814, -0.2839, -0.2855, -0.2856, -0.2863, -0.2971, -0.2996, -0.3034, -0.3044, -0.3146, -0.3266, -0.329, -0.3719, -0.8698, -0.6027, -0.6228, 1.545, 1.0154, 1.0018, 0.8859, 0.1484, 0.1435, 0.101, 0.0878, 0.087, 0.0813, 0.0812, 0.0682, 0.0668, 0.0452, 0.0333, 0.0254, 0.0185, 0.0125, -0.0036, -0.0068, -0.0133, -0.0159, -0.0177, -0.0182, -0.0268, -0.0297, -0.0332, -0.0388, -0.0441, -0.0546, -0.0561, -0.0656, -0.0817, -0.1585, -0.1559, -0.1472, -0.1166, -0.5, -0.7824, -0.4912, -0.8453, -0.5645, 1.3026, 1.0845, 1.0517, 1.021, 1.0134, 0.3477, 0.1401, 0.132, 0.1242, 0.1009, 0.0807, 0.0714, 0.0699, 0.0574, 0.0313, 0.0282, 0.0041, 0.0008, 0.0001, -0.0036, -0.0066, -0.0076, -0.0139, -0.0186, -0.0224, -0.028, -0.0352, -0.0355, -0.0522, -0.053, -0.1029, -0.1567, -0.2423, -0.2116, -0.2314, -0.1514, -0.7573, -0.2251, -0.5439, -0.5307, -0.5556, -0.8376, 1.5142, 1.3508, 1.0701, 0.9488, 0.3394, 0.2467, 0.2265, 0.1401, 0.1374, 0.1253, 0.0668, 0.0639, 0.0614, 0.0489, 0.0216, 0.0186, 0.0176, 0.0171, 0.0156, 0.0149, 0.0138, 0.0094, 0.0035, -0.0029, -0.0042, -0.0074, -0.0148, -0.0171, -0.0262, -0.0304, -0.0344, -0.0493, -0.0678, -0.1299, -0.2786, -0.53, -0.4631, 2.1628, 1.644, 0.7258, 0.139, 0.1113, 0.0809, 0.0338, 0.0333, -0.0526, -0.0527, -0.0594, -0.0595, -0.0608, -0.0726, -0.0779, -0.085, -0.0852, -0.1028, -0.1278, -0.1299, -0.1515, -0.1569, -0.1642, -0.1654, -0.1705, -0.1723, -0.1754, -0.1775, -0.1838, -0.1974, -0.5077, -0.2241, -0.8722, -0.4491, -0.4036, -0.7299, 2.3365, 2.2254, -0.0897, -0.1135, -0.1273, -0.153, -0.1551, -0.1681, -0.1703, -0.1753, -0.1776, -0.187, -0.1879, -0.1898, -0.1928, -0.1985, -0.2058, -0.2074, -0.2086, -0.2148, -0.2218, -0.2265, -0.2284, -0.2379, -0.2485, -0.2489, -0.2506, -0.2541, -0.2547, -0.2572, -0.261, -0.2882, -0.2795, -0.266, -0.2823, -0.7213, -0.5016, -0.4284, -0.6905, -0.7505, -0.626, -1.0375, -0.5317, 1.1208, 1.0982, 1.0622, 0.2162, 0.2114, 0.1889, 0.1819, 0.1693, 0.1483, 0.1444, 0.1046, 0.1029, 0.1014, 0.1007, 0.0951, 0.0911, 0.0899, 0.0844, 0.078, 0.0756, 0.0692, 0.0659, 0.064, 0.0604, 0.0603, 0.0526, 0.0496, 0.0428, 0.0398, 0.0329, 0.0253, 0.0258, 0.0175, -0.0635, -0.017, -0.0128, -0.1008, -0.0314, -0.195, -0.0464, 2.4444, 0.8808, 0.1188, 0.0223, 0.0005, -0.0091, -0.0092, -0.0199, -0.0338, -0.0376, -0.0402, -0.0423, -0.0512, -0.0638, -0.0731, -0.0747, -0.0845, -0.1022, -0.1108, -0.1123, -0.1129, -0.122, -0.1328, -0.1405, -0.1406, -0.1463, -0.1495, -0.1534, -0.1545, -0.1719, -0.1767, -0.1825, -0.1961, -0.2787, -0.5348, -0.3054, -0.3758, -0.3851, -0.5121, 0.3824, 0.3818, 0.3153, 0.2881, 0.2776, 0.2583, 0.254, 0.2432, 0.24, 0.2238, 0.2116, 0.2047, 0.1831, 0.1795, 0.1766, 0.1657, 0.1623, 0.1458, 0.1428, 0.1261, 0.1248, 0.124, 0.121, 0.1188, 0.117, 0.1115, 0.1025, 0.0983, 0.0956, 0.0927, 0.0811, 0.0893, 0.0577, 0.0341, -0.6076, -0.3127, -0.1967, -0.6403, -0.3914, -0.3993, -0.4198, 0.3315, 0.2571, 0.24, 0.24, 0.2369, 0.2329, 0.2323, 0.2282, 0.2227, 0.2218, 0.221, 0.1841, 0.1809, 0.1735, 0.1729, 0.1664, 0.1596, 0.1492, 0.1429, 0.1363, 0.1329, 0.1322, 0.1307, 0.1305, 0.1185, 0.1159, 0.1037, 0.101, 0.101, 0.0995, 0.0967, 0.0972, 0.0891, 0.0722, 0.0767, 0.03, 0.0691, -0.1964, 0.0388, -0.1598, 0.428, 0.3036, 0.2442, 0.2298, 0.2087, 0.2003, 0.1988, 0.1938, 0.1899, 0.1851, 0.183, 0.1662, 0.1649, 0.1609, 0.1449, 0.1323, 0.1284, 0.1215, 0.1211, 0.1155, 0.1099, 0.1051, 0.0879, 0.0866, 0.0863, 0.0792, 0.076, 0.0699, 0.0651, 0.0632, -0.0221, 0.0494, 0.0452, -0.0126, -0.1355, -0.1937, -0.4061, -0.1121, -0.738], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.6023, -2.0614, -2.5175, -2.4143, -2.6669, -3.5707, -4.0303, -4.0611, -4.0643, -4.886, -4.9069, -4.9573, -5.0011, -5.0137, -5.0212, -5.0076, -5.0195, -4.8982, -5.0423, -5.0766, -5.061, -5.0609, -5.0509, -4.9465, -5.0662, -5.0663, -5.0512, -5.0434, -5.0554, -5.048, -5.0237, -4.9501, -5.0351, -4.9841, -4.939, -4.9856, -5.0137, -5.0318, -3.0119, -3.0338, -3.0666, -3.1179, -3.5366, -3.5326, -3.4736, -3.2559, -3.5781, -3.5696, -3.5923, -3.897, -3.5374, -4.4187, -4.4601, -4.444, -4.3949, -4.4679, -4.477, -4.4547, -4.4822, -4.5144, -4.5075, -4.5059, -4.4744, -4.5142, -4.468, -4.5455, -4.479, -4.5004, -4.5099, -4.4688, -4.4548, -4.4864, -1.5509, -3.5976, -3.6097, -3.6351, -3.6175, -3.6176, -3.6522, -3.6545, -3.6735, -3.6697, -4.4832, -4.4846, -4.3717, -4.5387, -4.5194, -4.5786, -4.571, -4.5517, -4.561, -4.5516, -4.5311, -4.6031, -4.6094, -4.5894, -4.5756, -4.6394, -4.6274, -4.6501, -4.5464, -4.5385, -4.5353, -4.4756, -4.6037, -4.6026, -4.5739, -4.5042, -4.5493, -4.5427, -4.4948, -4.5408, -2.4297, -2.9593, -3.0228, -3.0256, -3.0678, -3.4514, -3.4628, -4.3239, -4.397, -4.3955, -4.3983, -4.4006, -4.4151, -4.4468, -4.4303, -4.4462, -4.4558, -4.4395, -4.4665, -4.4116, -4.4809, -4.4648, -4.4368, -4.4618, -4.5188, -4.4693, -4.5213, -4.4928, -4.456, -4.5077, -4.4564, -4.3909, -4.4, -4.3732, -4.4408, -4.446, -4.4018, -4.417, -3.0122, -3.0056, -3.4313, -3.3687, -3.3949, -3.4178, -3.4568, -3.4146, -3.3581, -4.1124, -4.1983, -4.275, -4.3498, -4.3302, -4.3027, -4.2835, -4.3797, -4.3551, -4.3955, -4.37, -4.4016, -4.3925, -4.3764, -4.397, -4.4474, -4.275, -4.3673, -4.4631, -4.4211, -4.4666, -4.3923, -4.3568, -4.3942, -4.3279, -4.3977, -2.984, -2.9859, -3.0356, -2.9465, -3.0146, -3.4513, -3.4056, -4.3442, -4.2819, -4.3816, -4.2482, -4.3428, -4.3842, -4.4175, -4.4119, -4.3593, -4.2931, -4.3591, -4.4053, -4.4458, -4.4691, -4.4231, -4.3928, -4.4678, -4.4146, -4.4458, -4.4579, -4.3799, -4.5066, -4.4748, -4.3834, -4.424, -4.3964, -4.3794, -4.3253, -4.4135, -4.3657, -4.4036, -4.3602, -4.4103, -2.0423, -2.1569, -3.132, -3.5233, -3.6401, -3.6372, -3.718, -3.8485, -4.4887, -4.4927, -4.4758, -4.5238, -4.5536, -4.5498, -4.548, -4.5472, -4.441, -4.5888, -4.4953, -4.5034, -4.5631, -4.575, -4.6231, -4.5753, -4.6068, -4.459, -4.5504, -4.4572, -4.5031, -4.4755, -4.5726, -4.4814, -4.3438, -4.5519, -4.4955, -4.5444, -4.552, -1.5607, -3.0934, -3.1835, -3.5922, -3.582, -3.6063, -3.5986, -3.5909, -4.2998, -4.345, -4.5245, -4.4878, -4.4723, -4.5102, -4.4682, -4.5708, -4.4699, -4.5472, -4.578, -4.5293, -4.5805, -4.601, -4.605, -4.6482, -4.6286, -4.6053, -4.5882, -4.62, -4.6214, -4.5944, -4.4566, -4.5734, -4.584, -4.6038, -4.5011, -4.5058, -2.825, -3.3208, -3.3197, -3.3517, -3.2767, -4.1926, -4.2514, -4.2548, -4.2955, -4.2765, -4.2381, -4.2489, -4.2619, -4.1796, -4.3082, -4.2732, -4.3563, -4.34, -4.3515, -4.0939, -4.3431, -4.2048, -4.2753, -4.2752, -4.3269, -4.2877, -4.3401, -4.3429, -4.2281, -4.3634, -4.3279, -4.2756, -4.3282, -4.327, -4.2824, -4.2836, -4.2947, -4.2927, -4.238, -4.3236, -4.2993, -2.1435, -2.2663, -2.1206, -3.6722, -3.6694, -3.6548, -3.6803, -4.3845, -4.4877, -4.4, -4.4231, -4.5344, -4.5815, -4.5068, -4.5873, -4.6019, -4.5378, -4.6422, -4.6312, -4.6257, -4.619, -4.5926, -4.5234, -4.6128, -4.6396, -4.6113, -4.6574, -4.6287, -4.5973, -4.6488, -4.5952, -4.5792, -4.3265, -4.5591, -4.606, -2.7976, -3.2785, -3.3099, -2.8483, -4.1918, -4.1792, -4.2513, -4.271, -4.2412, -4.2726, -4.2064, -4.2501, -4.285, -4.221, -4.3308, -4.2968, -4.2194, -4.3331, -4.3347, -4.2102, -4.3287, -4.323, -4.2851, -4.3003, -4.3499, -4.3219, -4.3462, -4.4093, -4.3397, -4.3811, -4.3509, -4.2804, -4.289, -4.2381, -4.2562, -4.2756, -4.3088, -4.2509, -4.254, -4.3053, -4.3019, -4.3189, -3.0324, -3.2376, -3.2554, -3.3233, -3.2645, -3.9471, -4.152, -4.2244, -4.1875, -4.185, -4.2149, -4.2742, -4.2566, -4.2742, -4.2919, -4.3241, -4.3256, -4.3394, -4.3586, -4.3741, -4.3377, -4.2902, -4.3322, -4.334, -4.3045, -4.3562, -4.3777, -4.2383, -4.3652, -4.3192, -4.3063, -4.257, -4.1987, -4.2291, -4.2323, -4.2798, -4.214, -4.3047, -4.278, -4.2851, -4.3065, -4.3092, -2.8445, -2.9756, -3.294, -3.3371, -3.9427, -4.0849, -4.1137, -4.2303, -4.1574, -4.2311, -4.2553, -4.2884, -4.2697, -4.2862, -4.2901, -4.3353, -4.3055, -4.2982, -4.3269, -4.3133, -4.2241, -4.3133, -4.2627, -4.3547, -4.3484, -4.2853, -4.3331, -4.0346, -4.272, -4.2832, -4.2416, -4.2628, -4.256, -4.2302, -4.2796, -4.2641, -4.2773, -1.9656, -2.0901, -3.6054, -4.2129, -4.1562, -4.2146, -4.212, -4.3255, -4.3353, -4.377, -4.3711, -4.3911, -4.3839, -4.4077, -4.4234, -4.376, -4.4277, -4.4593, -4.4818, -4.4595, -4.4437, -4.5092, -4.4501, -4.4475, -4.5107, -4.4853, -4.4937, -4.4997, -4.5121, -4.3855, -4.2585, -4.4902, -4.3289, -4.4666, -4.4832, -4.4843, -1.6467, -1.9894, -4.43, -4.4252, -4.4795, -4.4773, -4.5116, -4.4946, -4.4482, -4.4427, -4.5481, -4.5, -4.5304, -4.5195, -4.4749, -4.5296, -4.5129, -4.5517, -4.5604, -4.5331, -4.5859, -4.4338, -4.5505, -4.4302, -4.5716, -4.5716, -4.5427, -4.4575, -4.6003, -4.4587, -4.5486, -4.5017, -4.5324, -4.5519, -4.5485, -4.4722, -4.5186, -4.5287, -4.5046, -4.5049, -4.5192, -4.5091, -4.5327, -3.2497, -3.2172, -3.2897, -4.1189, -4.1197, -4.0938, -4.1583, -4.1847, -4.204, -4.1415, -4.2137, -4.2193, -4.2551, -4.2257, -4.228, -4.1224, -4.2417, -4.2438, -4.2463, -4.2832, -4.2087, -4.2023, -4.2236, -4.2852, -4.2468, -4.2899, -4.2946, -4.2246, -4.3243, -4.2898, -4.1895, -4.212, -4.1898, -3.9567, -4.2092, -4.2144, -4.1804, -4.2342, -4.2125, -4.2345, -1.512, -3.4435, -4.2377, -4.3418, -4.2657, -4.247, -4.2918, -4.351, -4.374, -4.3692, -4.3357, -4.354, -4.2545, -4.3514, -4.4187, -4.4172, -4.4433, -4.3932, -4.3256, -4.4344, -4.4069, -4.429, -4.4249, -4.4923, -4.4633, -4.3598, -4.4316, -4.5073, -4.4325, -4.507, -4.4296, -4.4283, -4.4033, -4.4071, -4.3489, -4.4057, -4.3934, -4.4021, -4.4053, -3.9472, -3.9408, -3.9627, -4.0823, -4.0747, -4.0699, -4.0416, -4.1156, -4.0865, -4.1187, -4.0412, -4.0812, -4.1352, -4.022, -4.1465, -4.1799, -4.1316, -4.2107, -4.1689, -4.1893, -4.1673, -4.2162, -4.1452, -4.233, -4.2141, -4.0913, -4.2326, -4.1164, -4.187, -4.2294, -4.1872, -4.2178, -4.1881, -4.1693, -4.0643, -4.1268, -4.1531, -4.1119, -4.1423, -4.1537, -4.1539, -4.039, -4.0558, -4.1123, -4.0897, -4.0457, -4.0627, -4.0616, -4.1282, -4.1004, -4.1184, -4.1431, -4.1424, -4.1616, -4.1213, -4.1553, -4.1924, -4.1647, -4.1579, -4.1493, -4.1864, -4.2022, -4.1339, -4.2149, -4.1605, -4.1489, -4.2152, -4.2481, -4.1448, -4.2107, -4.2158, -4.1067, -4.171, -4.1488, -4.1159, -4.1368, -4.0703, -4.1456, -3.9305, -4.1628, -4.1607, -3.885, -4.0552, -4.1123, -4.1053, -4.0789, -4.1537, -3.5521, -4.0724, -4.1366, -4.1854, -4.1481, -4.1635, -4.129, -4.1574, -4.1507, -4.1599, -4.1959, -4.2068, -4.2011, -4.1962, -4.1721, -4.259, -4.2031, -4.1816, -4.1811, -4.2634, -4.1313, -4.1182, -4.2419, -4.2684, -3.4937, -4.1964, -4.2077, -4.141, -4.1186, -4.15, -4.1402, -4.1917, -4.1946]}, \"token.table\": {\"Topic\": [4, 5, 15, 10, 14, 1, 10, 8, 7, 2, 1, 2, 17, 1, 4, 14, 3, 2, 1, 7, 4, 4, 6, 4, 5, 1, 10, 2, 2, 15], \"Freq\": [0.4755915593671237, 0.49774047636751456, 0.357896526588234, 0.2790054339949507, 0.2790054339949507, 0.3271244990919101, 0.402388604353408, 0.6044939275729353, 0.3704253956284541, 0.44607083338533654, 0.6437733430794921, 0.34844984529828765, 0.34844984529828765, 0.36433049568729453, 0.48916895451882286, 0.41385902352741905, 0.6342364902340109, 0.4654027117134641, 0.2847135777800033, 0.2847135777800033, 0.4759687953466861, 0.4869274720099841, 0.4452683144880554, 0.37021007780846055, 0.44583661904698596, 0.567444913027106, 0.39415370538032624, 0.4478191608699459, 0.4393228529727466, 0.45119649273342827], \"Term\": [\"ants huge gum\", \"better kind thoughts\", \"cat hanging upside\", \"cuddling black dog\", \"cuddling black dog\", \"darn black dog\", \"dog feral cat\", \"enjoy time family\", \"feel bit better\", \"fight tired unwell\", \"going try make\", \"got nan dog\", \"got nan dog\", \"hour drive away\", \"huge gum leaves\", \"makes lot sense\", \"mental health issues\", \"need find works\", \"oh well hope\", \"oh well hope\", \"picnic drs office\", \"please ever give\", \"positivity motivation humour\", \"sending virtual hugs\", \"sorry hear struggling\", \"thanks going try\", \"thanks much comments\", \"tired unwell tired\", \"unwell tired trying\", \"wave everyone else\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [15, 13, 6, 4, 1, 2, 5, 19, 20, 11, 14, 16, 3, 18, 10, 12, 8, 17, 9, 7]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1148027357163516964343417949\", ldavis_el1148027357163516964343417949_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1148027357163516964343417949\", ldavis_el1148027357163516964343417949_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1148027357163516964343417949\", ldavis_el1148027357163516964343417949_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics          x          y\n",
       "topic                                                  \n",
       "14     10.101905        1       1   7.240584  62.883759\n",
       "12      9.674921        1       2  -9.646950  46.858959\n",
       "5       9.213016        1       3   9.710749  11.567299\n",
       "3       7.468571        1       4 -24.184044  31.333546\n",
       "0       6.824127        1       5 -65.995979  24.003578\n",
       "1       6.357460        1       6 -55.838463  47.858727\n",
       "4       5.879683        1       7 -52.419456   2.790928\n",
       "18      5.635238        1       8 -29.879637  10.594707\n",
       "19      4.746349        1       9 -34.159710 -12.518440\n",
       "10      4.690794        1      10  13.395372  -9.988531\n",
       "13      4.357460        1      11  25.601421  46.565048\n",
       "15      4.079683        1      12 -43.830921  27.192461\n",
       "2       3.857460        1      13 -10.824326 -20.805817\n",
       "17      3.746349        1      14   7.080748  34.484890\n",
       "9       3.190794        1      15 -41.788685  69.481651\n",
       "11      2.635238        1      16 -31.888054  49.533173\n",
       "7       2.635238        1      17 -16.134815  69.646927\n",
       "16      1.635238        1      18 -11.162503   0.976201\n",
       "8       1.635238        1      19  -8.588459  21.183186\n",
       "6       1.635238        1      20  29.763134  20.532961, topic_info=     Category      Freq                          Term     Total  loglift  \\\n",
       "term                                                                       \n",
       "22    Default  2.000000                   got nan dog  2.000000  30.0000   \n",
       "12    Default  3.000000             enjoy time family  3.000000  29.0000   \n",
       "4     Default  2.000000            cat hanging upside  2.000000  28.0000   \n",
       "34    Default  4.000000          mental health issues  4.000000  27.0000   \n",
       "20    Default  4.000000                going try make  4.000000  26.0000   \n",
       "7     Default  3.000000            cuddling black dog  3.000000  25.0000   \n",
       "62    Default  2.000000            wave everyone else  2.000000  24.0000   \n",
       "32    Default  2.000000               makes lot sense  2.000000  23.0000   \n",
       "15    Default  2.000000               feel bit better  2.000000  22.0000   \n",
       "40    Default  3.000000                  oh well hope  3.000000  21.0000   \n",
       "11    Default  2.000000                 dog feral cat  2.000000  20.0000   \n",
       "55    Default  3.000000              thanks going try  3.000000  19.0000   \n",
       "56    Default  2.000000          thanks much comments  2.000000  18.0000   \n",
       "51    Default  2.000000          sending virtual hugs  2.000000  17.0000   \n",
       "9     Default  3.000000                darn black dog  3.000000  16.0000   \n",
       "31    Default  1.000000               love go camping  1.000000  15.0000   \n",
       "52    Default  1.000000      snarling german shepherd  1.000000  14.0000   \n",
       "28    Default  1.000000            joined craft group  1.000000  13.0000   \n",
       "36    Default  1.000000               much kind words  1.000000  12.0000   \n",
       "26    Default  2.000000               hour drive away  2.000000  11.0000   \n",
       "3     Default  1.000000                care good lady  1.000000  10.0000   \n",
       "45    Default  2.000000  positivity motivation humour  2.000000   9.0000   \n",
       "41    Default  2.000000            outside look stars  2.000000   8.0000   \n",
       "1     Default  2.000000          better kind thoughts  2.000000   7.0000   \n",
       "27    Default  2.000000               huge gum leaves  2.000000   6.0000   \n",
       "61    Default  1.000000              wanting run away  1.000000   5.0000   \n",
       "54    Default  2.000000               taken long time  2.000000   4.0000   \n",
       "49    Default  1.000000         scenery driving along  1.000000   3.0000   \n",
       "50    Default  1.000000               self help books  1.000000   2.0000   \n",
       "30    Default  2.000000            loads virtual hugs  2.000000   1.0000   \n",
       "...       ...       ...                           ...       ...      ...   \n",
       "49    Topic20  0.037324         scenery driving along  1.896792   0.1851   \n",
       "17    Topic20  0.038743      feeling really depressed  1.972940   0.1830   \n",
       "61    Topic20  0.038149              wanting run away  1.975778   0.1662   \n",
       "16    Topic20  0.039487           feel cheerful happy  2.047603   0.1649   \n",
       "48    Topic20  0.038382           said really enjoyed  1.998406   0.1609   \n",
       "27    Topic20  0.038639               huge gum leaves  2.044283   0.1449   \n",
       "30    Topic20  0.038289            loads virtual hugs  2.051280   0.1323   \n",
       "37    Topic20  0.036931         need fight depression  1.986409   0.1284   \n",
       "38    Topic20  0.036534                need find ways  1.978636   0.1215   \n",
       "5     Topic20  0.036743              channel end game  1.990763   0.1211   \n",
       "41    Topic20  0.036921            outside look stars  2.011622   0.1155   \n",
       "29    Topic20  0.037821            laughter good soul  2.072063   0.1099   \n",
       "33    Topic20  0.034675         many wonderful people  1.908912   0.1051   \n",
       "44    Topic20  0.036669              please ever give  2.053694   0.0879   \n",
       "43    Topic20  0.037466             picnic drs office  2.100978   0.0866   \n",
       "0     Topic20  0.037482                 ants huge gum  2.102645   0.0863   \n",
       "52    Topic20  0.034523      snarling german shepherd  1.950544   0.0792   \n",
       "59    Topic20  0.039398            tired unwell tired  2.233044   0.0760   \n",
       "60    Topic20  0.039916           unwell tired trying  2.276230   0.0699   \n",
       "13    Topic20  0.035272          exhausting full stop  2.020967   0.0651   \n",
       "19    Topic20  0.034350             go round emotions  1.971947   0.0632   \n",
       "20    Topic20  0.074538                going try make  4.660025  -0.0221   \n",
       "39    Topic20  0.036915               need find works  2.148677   0.0494   \n",
       "63    Topic20  0.036500     wondering time depression  2.133534   0.0452   \n",
       "32    Topic20  0.039016               makes lot sense  2.416282  -0.0126   \n",
       "4     Topic20  0.039901            cat hanging upside  2.794104  -0.1355   \n",
       "22    Topic20  0.038666                   got nan dog  2.869853  -0.1937   \n",
       "7     Topic20  0.039047            cuddling black dog  3.584160  -0.4061   \n",
       "56    Topic20  0.037090          thanks much comments  2.537081  -0.1121   \n",
       "34    Topic20  0.036979          mental health issues  4.730097  -0.7380   \n",
       "\n",
       "      logprob  \n",
       "term           \n",
       "22    30.0000  \n",
       "12    29.0000  \n",
       "4     28.0000  \n",
       "34    27.0000  \n",
       "20    26.0000  \n",
       "7     25.0000  \n",
       "62    24.0000  \n",
       "32    23.0000  \n",
       "15    22.0000  \n",
       "40    21.0000  \n",
       "11    20.0000  \n",
       "55    19.0000  \n",
       "56    18.0000  \n",
       "51    17.0000  \n",
       "9     16.0000  \n",
       "31    15.0000  \n",
       "52    14.0000  \n",
       "28    13.0000  \n",
       "36    12.0000  \n",
       "26    11.0000  \n",
       "3     10.0000  \n",
       "45     9.0000  \n",
       "41     8.0000  \n",
       "1      7.0000  \n",
       "27     6.0000  \n",
       "61     5.0000  \n",
       "54     4.0000  \n",
       "49     3.0000  \n",
       "50     2.0000  \n",
       "30     1.0000  \n",
       "...       ...  \n",
       "49    -4.1854  \n",
       "17    -4.1481  \n",
       "61    -4.1635  \n",
       "16    -4.1290  \n",
       "48    -4.1574  \n",
       "27    -4.1507  \n",
       "30    -4.1599  \n",
       "37    -4.1959  \n",
       "38    -4.2068  \n",
       "5     -4.2011  \n",
       "41    -4.1962  \n",
       "29    -4.1721  \n",
       "33    -4.2590  \n",
       "44    -4.2031  \n",
       "43    -4.1816  \n",
       "0     -4.1811  \n",
       "52    -4.2634  \n",
       "59    -4.1313  \n",
       "60    -4.1182  \n",
       "13    -4.2419  \n",
       "19    -4.2684  \n",
       "20    -3.4937  \n",
       "39    -4.1964  \n",
       "63    -4.2077  \n",
       "32    -4.1410  \n",
       "4     -4.1186  \n",
       "22    -4.1500  \n",
       "7     -4.1402  \n",
       "56    -4.1917  \n",
       "34    -4.1946  \n",
       "\n",
       "[803 rows x 6 columns], token_table=      Topic      Freq                          Term\n",
       "term                                               \n",
       "0         4  0.475592                 ants huge gum\n",
       "1         5  0.497740          better kind thoughts\n",
       "4        15  0.357897            cat hanging upside\n",
       "7        10  0.279005            cuddling black dog\n",
       "7        14  0.279005            cuddling black dog\n",
       "9         1  0.327124                darn black dog\n",
       "11       10  0.402389                 dog feral cat\n",
       "12        8  0.604494             enjoy time family\n",
       "15        7  0.370425               feel bit better\n",
       "18        2  0.446071            fight tired unwell\n",
       "20        1  0.643773                going try make\n",
       "22        2  0.348450                   got nan dog\n",
       "22       17  0.348450                   got nan dog\n",
       "26        1  0.364330               hour drive away\n",
       "27        4  0.489169               huge gum leaves\n",
       "32       14  0.413859               makes lot sense\n",
       "34        3  0.634236          mental health issues\n",
       "39        2  0.465403               need find works\n",
       "40        1  0.284714                  oh well hope\n",
       "40        7  0.284714                  oh well hope\n",
       "43        4  0.475969             picnic drs office\n",
       "44        4  0.486927              please ever give\n",
       "45        6  0.445268  positivity motivation humour\n",
       "51        4  0.370210          sending virtual hugs\n",
       "53        5  0.445837         sorry hear struggling\n",
       "55        1  0.567445              thanks going try\n",
       "56       10  0.394154          thanks much comments\n",
       "59        2  0.447819            tired unwell tired\n",
       "60        2  0.439323           unwell tired trying\n",
       "62       15  0.451196            wave everyone else, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[15, 13, 6, 4, 1, 2, 5, 19, 20, 11, 14, 16, 3, 18, 10, 12, 8, 17, 9, 7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualising LDA topic model based on CountVectorizer\n",
    "pyLDAvis.sklearn.prepare(lda_cvt, X_cvt, cvt_vectorizer, n_jobs=1, mds='tsne')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:09:04.193867Z",
     "start_time": "2018-05-23T19:08:56.275872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1148027357146604245415092879\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1148027357146604245415092879_data = {\"mdsDat\": {\"Freq\": [9.093576693099187, 7.354536701790602, 6.97451972695798, 6.881090364066926, 6.6589553403365755, 6.4053240872530015, 5.847784496435433, 5.458965482389513, 5.454297063855912, 5.120400133958642, 4.661105891961275, 4.43665390609506, 4.353512634805283, 3.5652373045218537, 3.565237291949513, 3.4019801233615685, 3.4014746545178927, 2.613704801199201, 2.6137047785751726, 2.1379385228693972], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"x\": [57.34080123901367, 21.855695724487305, -127.4720230102539, -7.209629535675049, 13.966713905334473, 30.86600112915039, -44.387264251708984, -51.53215026855469, -100.03620910644531, 34.5999641418457, -42.709590911865234, 67.37528991699219, -62.15419006347656, -11.369043350219727, 26.812639236450195, 41.1734733581543, 50.96506881713867, -0.5522763729095459, 76.186279296875, 77.53898620605469], \"y\": [-45.16096496582031, 146.3616943359375, -247.40443420410156, -64.42118072509766, -0.8077471256256104, -61.979270935058594, -219.84718322753906, -22.693822860717773, 68.5303726196289, -111.54214477539062, -74.72268676757812, -77.22211456298828, -197.0957794189453, -31.883073806762695, 91.62633514404297, -8.37791633605957, 33.77305221557617, -116.36698150634766, 361.4445495605469, -20.779542922973633]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"Freq\": [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.31166134818945457, 0.29366420600123305, 0.285291721710533, 0.2663944194674041, 0.27816669231277896, 0.3534486789410681, 0.23162634068821877, 0.22973205842772632, 0.21581110232751546, 0.20639658577168582, 0.22799497807992072, 0.1885284572301919, 0.19984485505456745, 0.19043644085043399, 0.2034977599158269, 0.17735875077163815, 0.17737870642640383, 0.16137457165926788, 0.1950727799738547, 0.14972065714566402, 0.14148889620091215, 0.152714389573428, 0.13062690777479763, 0.12360284386712703, 0.12056856806651395, 0.12705465299985158, 0.11763549406290011, 0.11509265445832877, 0.11878423767870266, 0.11910486194052182, 0.11802944272732925, 0.12389162695214309, 1.4862032722165168, 0.20407232425085373, 0.19169999322692335, 0.17225117532095507, 0.16527975571564882, 0.16793421004567907, 0.16350766148962706, 0.1634741638864693, 0.17081463114756043, 0.10736906474761428, 0.1620212532766783, 0.09604135835611756, 0.09590130855566122, 0.09262963480883663, 0.08967879688187651, 0.08797032879925966, 0.09016174091685164, 0.09085427584601732, 0.09154563472873047, 0.09087248895288255, 0.08771176550720362, 0.08969228716322314, 0.08730228447825483, 0.09116691105059997, 0.0888510910754347, 0.08755889946002206, 0.08516198284026795, 0.08540372343443836, 0.08465657436132164, 0.08215494865962764, 0.08314551455149458, 0.09676880765093317, 0.08635905671717692, 0.0949274110686606, 0.09066068924547191, 0.08989798281050324, 0.09404728690795754, 0.09049004207969168, 0.6528258067474083, 0.5522157494778639, 0.5960181410246372, 0.20707322302909095, 0.21127122944166982, 0.20369321161449905, 0.20307021809519343, 0.17466367473001854, 0.17099963895414896, 0.09061023917885874, 0.08911666380840351, 0.08428522416073414, 0.08408904020790714, 0.08784672827176208, 0.09852511415312411, 0.09428908406223319, 0.0823881114200124, 0.08603636960690313, 0.08360309067189692, 0.08277443407442378, 0.0804542929069167, 0.08150042179136337, 0.08316487238914826, 0.08162070362872331, 0.07934229119816846, 0.07913970038842973, 0.08001384404411425, 0.08020917452948902, 0.08099580540430883, 0.07794466191748914, 0.08294455210493142, 0.09480747354573892, 0.08599739179927703, 0.0820567864742263, 1.4405465863262457, 0.7614352953436186, 0.27758179969463126, 0.25498327669622395, 0.15732638294015788, 0.15417743996362374, 0.16016118215396394, 0.15539467795015624, 0.09310152933002609, 0.08899044200969339, 0.0785361211271307, 0.0743680708560108, 0.07835359906741485, 0.07543777335574031, 0.07959200316422763, 0.07528551946818222, 0.07048874770558909, 0.07375899905552659, 0.07401001098440489, 0.07031733952765895, 0.07081975831996522, 0.06861662944774018, 0.06869358387266428, 0.06859456628107005, 0.07099894416341691, 0.06711152519068672, 0.0670136053409307, 0.06888735590193124, 0.06977979834367655, 0.07269820104767391, 0.06934445518710916, 0.075769952975802, 0.07006725236047039, 0.07150348901622784, 0.5247521839746693, 0.36799617017890396, 0.2514436673039809, 0.23427518169390454, 0.2074710075150169, 0.10552355538831647, 0.10833067767433009, 0.09795733405610446, 0.09823980729649301, 0.09633175056048195, 0.09487295737905514, 0.0966676261941655, 0.0977338637322931, 0.09249085113395943, 0.0980918184628675, 0.09338304666927703, 0.09030702038593233, 0.09400962748890004, 0.09124331343656308, 0.09992272836374393, 0.0933276670490534, 0.09388568868362687, 0.09426401256287359, 0.09243149538041609, 0.09034356938843348, 0.08684239348654874, 0.09166282181069545, 0.09019090461804434, 0.09150390075200801, 0.0924648172345183, 0.09779206753815207, 0.0986843575045707, 0.10045204997676535, 0.0934007279044975, 0.0976167951441934, 0.09614493622401647, 0.09193761316042753, 0.45424310753968444, 0.3383329238150257, 0.30822958260831, 0.18427536585536877, 0.18997697653086126, 0.16722503106540312, 0.11430871083969485, 0.1033932670000123, 0.09884713328030155, 0.09996440556247822, 0.09392096601236546, 0.09875025938150478, 0.10246919609439395, 0.09252177919152739, 0.09405765057086983, 0.09253226627871339, 0.09047311124144343, 0.08728233785445255, 0.09063510566197631, 0.09024368124349792, 0.08754196413787863, 0.09031849804763428, 0.08485245600411001, 0.0826069860211317, 0.09192809623081792, 0.08672095731252506, 0.08485025794018672, 0.0877746800802899, 0.08300707787665754, 0.08525984608310073, 0.08679621563896942, 0.08915173878531298, 0.0924387157345762, 0.09067463467922929, 0.09571850769699111, 0.08763796824717647, 0.08850733279218573, 0.08791654193576173, 0.5738749695313692, 0.5892188234762302, 0.3698017471707677, 0.5019282092629762, 0.31468872343355636, 0.12416146693037844, 0.11483172944986153, 0.10964714862268903, 0.07097925063911126, 0.07247850758036241, 0.074678051866664, 0.06749135385044105, 0.0634203352296447, 0.06417846939845304, 0.07159780312432279, 0.06198593899987339, 0.06315465349564954, 0.06378895240742236, 0.062434214279261505, 0.06459779220743794, 0.06163455581333896, 0.06051797034958184, 0.06330912094787998, 0.060194316559390185, 0.059898619132911925, 0.06192346073372586, 0.05936309449362345, 0.06084621088761617, 0.0614566128612413, 0.06084191835093052, 0.06822396222639189, 0.06797540926046283, 0.06144171777563703, 0.06873269323784206, 0.06378422468357606, 0.06570491830165696, 0.06560740951862619, 0.06264101085397786, 0.5898967507132468, 0.546843991334179, 0.19378560040648365, 0.1565256686230118, 0.15300697555668932, 0.12142426989835493, 0.1219027550883223, 0.10878467142492645, 0.07184614898835001, 0.073093692537565, 0.07565748050318057, 0.07165770015094078, 0.07213193841245019, 0.06964665888243059, 0.07107949605140458, 0.07306957767728307, 0.07110050141985694, 0.06771771635670727, 0.06616996216701536, 0.06798117429795107, 0.07444368327914912, 0.06614767869494177, 0.07431151582891785, 0.06785611162054507, 0.06663938865748376, 0.06696060929989495, 0.06759805797889433, 0.06306455065074608, 0.06253583228323548, 0.06781548018848942, 0.06803543525341052, 0.06511212638827293, 0.08338022927805837, 0.07266073766442267, 0.07164891591038093, 0.06822568745683448, 0.06770912705352634, 0.2998491020657128, 0.21811348089832633, 0.18153536810303197, 0.1795383766349085, 0.16714418017868907, 0.17085491625431032, 0.15436644799413116, 0.08657724450062373, 0.0863566769060554, 0.08947458738901626, 0.08322705182195446, 0.08402376014501801, 0.079184262148774, 0.07918664963781641, 0.08059570725170132, 0.07668511937785046, 0.0825729356946268, 0.07522252775128556, 0.07793092517257275, 0.07418933731511278, 0.07459155151177213, 0.07413149957766155, 0.07682214892475878, 0.07165912786270895, 0.07349116498107583, 0.08208169382445983, 0.07229840530302156, 0.06988361889271504, 0.07431365030192201, 0.07533534227586418, 0.07524915072803816, 0.08283676642811924, 0.07570048817079388, 0.08056281084862177, 0.08539569682637244, 0.07874519715646135, 0.0841028601337099, 0.07680489547753214, 0.07832880239408167, 0.07888322464221895, 0.07666915734365419, 0.07646268070685289, 0.47786884065313, 0.20051782182105987, 0.19382981788946116, 0.18989069863541397, 0.1841138834728548, 0.11237967448904991, 0.08914188940411998, 0.07800612597103915, 0.07728519330223638, 0.07779103476112419, 0.07725011975572883, 0.07542944621787721, 0.07078772926324085, 0.07354804746290698, 0.06854526731312269, 0.07181265546502175, 0.07115699111573569, 0.07237071405332667, 0.07136066960497901, 0.07276307647850173, 0.06901884137075161, 0.07180477594328155, 0.06987856688836246, 0.07134664923924654, 0.06913374627638721, 0.06951216009606091, 0.06885520183160097, 0.06644581789094084, 0.07101604345673147, 0.0685581487634133, 0.1104236274328198, 0.07048957733671471, 0.07174744104834994, 0.07275447488522911, 0.5721574725137473, 0.4356371879341826, 0.15944725485209835, 0.1156800017068378, 0.07739042730780053, 0.07729556340018606, 0.07264258654184984, 0.06847356280038576, 0.06914563934077547, 0.06522271351845917, 0.06475461148535719, 0.06606137396905061, 0.06511768225192188, 0.0626944315698016, 0.062426877785806266, 0.06574259953232989, 0.06407850571575704, 0.06369219932235126, 0.06047284281129988, 0.06048909598745479, 0.06104665759691485, 0.0589359306255963, 0.06120330566155519, 0.058084679064628005, 0.057457458629126545, 0.05914256983457792, 0.05620065412579578, 0.06143786307686495, 0.07393408210164275, 0.0573320023159, 0.05784287351001452, 0.057759950649986354, 0.05864345943300743, 0.06597072616499773, 0.06004369515695552, 0.05905596305236643, 0.0589930842362006, 0.22936182017306053, 0.18333889348498933, 0.1353795267654474, 0.1888925633299764, 0.07248780419496485, 0.07449189919769834, 0.07355403191857598, 0.07221808238649426, 0.07155698432901646, 0.07001286329677252, 0.06674600120768645, 0.06784463052348971, 0.06930994499697586, 0.07144219774881616, 0.06700870824704719, 0.06386513307678497, 0.0679571698250513, 0.06414567933858598, 0.06700281674894377, 0.06400851336798982, 0.06409177221719488, 0.06939132668341447, 0.06599430829717677, 0.06280056417677563, 0.06303268208712483, 0.06351858963624513, 0.06622795163862032, 0.06344577543449055, 0.06451214772499347, 0.06376378448006659, 0.07022609976043341, 0.06732130729782967, 0.06458255477392316, 0.06933564046823705, 0.06896722307128145, 0.06764272665319142, 0.06912188689777249, 0.0654326879107912, 0.06588523807264283, 0.06566184369136271, 0.06477661558900866, 0.2751427365400102, 0.17553258647147846, 0.1510878056215807, 0.1370929317061107, 0.08322504086805652, 0.07106241761948681, 0.06881847574402394, 0.07402980103142535, 0.0687999868632929, 0.06876521384338699, 0.06804593649573529, 0.06924984346475882, 0.06707890581737697, 0.06600907542744075, 0.06430320440314208, 0.06383515198681645, 0.06616243875487236, 0.06527467683662137, 0.06661789337760457, 0.06333922958348902, 0.06712386145544486, 0.06482790314949151, 0.06513811560923326, 0.062483337309925224, 0.06508168325416089, 0.06493836837844888, 0.0633409870485381, 0.06219222115830133, 0.06663046677270534, 0.05911416020729121, 0.06196069089760782, 0.0751665267866353, 0.06551210895341243, 0.06882929296044255, 0.06653560961589682, 0.06566400713699282, 0.15041200262442958, 0.14560255544961956, 0.1398882519867846, 0.06259999897792694, 0.0586856586628266, 0.06099598725713651, 0.05967947422519382, 0.0610442442528786, 0.060833070835227654, 0.0568679678378887, 0.054979018186929414, 0.05715876108045352, 0.05473471707689539, 0.05399284903998565, 0.05486083387148674, 0.055804536429867664, 0.05561849537695168, 0.05327479694325164, 0.05438052592958946, 0.0560660370754069, 0.0516949515891832, 0.056160075273143824, 0.054920351356153106, 0.053876220427645914, 0.055216617288745114, 0.05374294796593838, 0.053716777837306495, 0.051448663122682974, 0.05145709989950543, 0.05097364068910649, 0.06480363495836546, 0.055523824350555376, 0.056886632942472606, 0.055773951819532125, 0.0574033024936213, 0.05548829619842924, 0.05559144240295449, 0.05439719517259922, 0.6974502579117446, 0.12498684751953808, 0.05649232612735696, 0.055547727097290285, 0.05596894246420538, 0.050418136373408785, 0.053515058361722934, 0.05090251216299292, 0.051213925916947685, 0.054930380786571836, 0.04929245475610295, 0.049527498841698644, 0.050286753963743235, 0.050439699151673886, 0.04713668259620575, 0.047866043322452166, 0.04999520188089705, 0.047208325126514905, 0.04668558389176646, 0.048353942458315514, 0.04662670498516325, 0.051738551352179495, 0.045079905024163015, 0.04584285065700938, 0.047697934109991354, 0.04653080840827104, 0.046652390116757314, 0.04649275413988047, 0.04684366982829957, 0.04599254896947871, 0.046402742278444285, 0.04834650316047004, 0.04768699526803707, 0.05054394405259676, 0.047773411690475706, 0.04792600424357098, 0.04775242763520992, 0.4353901766012725, 0.3553988333090824, 0.04927346734407263, 0.05144160369025557, 0.049084313832596066, 0.049508273000000984, 0.04793496910653473, 0.048649088834825195, 0.04667787674430917, 0.04594017150741723, 0.04582675528385284, 0.045410040620407696, 0.04699666635589556, 0.04505450171165027, 0.047106089006693795, 0.04619006424317061, 0.04376021556305614, 0.043782336706023606, 0.04689100174072354, 0.044563141160578315, 0.04926347390676475, 0.04586167026931664, 0.04447547013640899, 0.04535340587183922, 0.04363240482349426, 0.0436266212096759, 0.0445986449482968, 0.042766119508816446, 0.043540229245814265, 0.04155553049161945, 0.04324583982760075, 0.047874755951338104, 0.044462730291394484, 0.04723678358775344, 0.04444466009517564, 0.044018531268558064, 0.0450666358668923, 0.04571585800567563, 0.045094802094993884, 0.0455234671547381, 0.0457294793234288, 0.04548922981531244, 0.044639990022208455, 0.3739697877930621, 0.3600840234402792, 0.12342659937901751, 0.04997046455345164, 0.051167559543930714, 0.0483294283510327, 0.047419084343187014, 0.04804235002408486, 0.04866622464322436, 0.045916245412694845, 0.04716891323272134, 0.05029669762859455, 0.044923473367189624, 0.0465063626005542, 0.04637071099388409, 0.04663043817984558, 0.0475266030296263, 0.04443038000503, 0.04552137902899482, 0.04443825554967387, 0.04608412759174727, 0.045113627952269676, 0.04445065330917726, 0.04393181573098218, 0.04397240306707013, 0.04247094459756779, 0.04619647884067714, 0.045093578214825375, 0.044135180933049856, 0.04422407777613681, 0.04696011689906402, 0.0480242937454738, 0.04699744498998541, 0.04653875054326, 0.044640989442948274, 0.044508305475007594, 0.1096852183354579, 0.052520845327542684, 0.047050200327819834, 0.045482452602512484, 0.04739167354293711, 0.046049333906755543, 0.04529507074223213, 0.045919817529221744, 0.043858180267688146, 0.045830350135287704, 0.044979673046539825, 0.04265600276720068, 0.0412531498095595, 0.043996163739563963, 0.041694495761636086, 0.040868238072692846, 0.04830921560111387, 0.04329383308041085, 0.04091862254109072, 0.0417100858690112, 0.04000397188291586, 0.03978451164126738, 0.045075969327115856, 0.04018731429111457, 0.040961114531133574, 0.04314013853537231, 0.04177636852298019, 0.04271078281990147, 0.039866219172765775, 0.04051490550137641, 0.043956984563413834, 0.04095560438632927, 0.04283449052127525, 0.044157521533330385, 0.04350477477791535, 0.04235133123991785, 0.04237542844255349, 0.04233975741610546, 0.04171200540087269, 0.11406546834162445, 0.04722830667709146, 0.046617223660437426, 0.046050484945071454, 0.044543794464133986, 0.057192542385021264, 0.043957907607733694, 0.04252119200230661, 0.043006091144679945, 0.043805657659278345, 0.042058998114927595, 0.04071929298074186, 0.04286675584040652, 0.04406676879262258, 0.04264812225753561, 0.04152735653604541, 0.04292359169153371, 0.04349451699737551, 0.04290340946945009, 0.04238710563181229, 0.04075377748911042, 0.03957733396283193, 0.042918128563282226, 0.04010578656006088, 0.04070673318140201, 0.04499374873988593, 0.03929813602544163, 0.04261314148315038, 0.0388767624394699, 0.03895551101301409, 0.04022648243167971, 0.0475532142383542, 0.040171615797763946, 0.04076118491931989, 0.042093451089163264, 0.04454984240100902, 0.04089420069818456, 0.04256385334876341, 0.041898696997102455, 0.04217642187984467, 0.03957887810356923, 0.03891843669909682, 0.039313194568279294, 0.037620751738514126, 0.038653054228722235, 0.0372210404447746, 0.0369882713166438, 0.03655693319067067, 0.03869236373040651, 0.03619968725344366, 0.036648430874013246, 0.03566667943082213, 0.03560545352018087, 0.03678057125340738, 0.035010996726598693, 0.03569067800944161, 0.03478098653342369, 0.03645231988547318, 0.035463506728629954, 0.03415456553275873, 0.035891654548877876, 0.03523282864492352, 0.03319497527222549, 0.035459385112760596, 0.034903182285655615, 0.035141744136915155, 0.03544438847468489, 0.033163001509138944, 0.035050069111070684, 0.03221219722590666, 0.03599378706442185, 0.03504268756188741, 0.04411486944909799, 0.03835977220166771, 0.03557448828511561, 0.03497077193453745], \"Term\": [\"got nan dog\", \"enjoy time family\", \"mental health issues\", \"cat hanging upside\", \"going try make\", \"wave everyone else\", \"thanks going try\", \"oh well hope\", \"cuddling black dog\", \"makes lot sense\", \"feel bit better\", \"darn black dog\", \"dog feral cat\", \"sending virtual hugs\", \"thanks much comments\", \"love go camping\", \"positivity motivation humour\", \"hour drive away\", \"wanting run away\", \"snarling german shepherd\", \"joined craft group\", \"much kind words\", \"channel end game\", \"scenery driving along\", \"please ever give\", \"many wonderful people\", \"ray ct scan\", \"taken long time\", \"better kind thoughts\", \"self help books\", \"said really enjoyed\", \"explain people around\", \"depression explain people\", \"time depression explain\", \"wondering time depression\", \"sending virtual hugs\", \"laughter good soul\", \"feeling really depressed\", \"black dog right\", \"things know help\", \"sorry hear struggling\", \"joined craft group\", \"hope okay today\", \"outside look stars\", \"dance around house\", \"taken long time\", \"need find works\", \"ants huge gum\", \"darn black dog\", \"better kind thoughts\", \"huge gum leaves\", \"positivity motivation humour\", \"unwell tired trying\", \"good day going\", \"wanting run away\", \"feel cheerful happy\", \"write gratitude journal\", \"snarling german shepherd\", \"need find ways\", \"country fire service\", \"exhausting full stop\", \"going try make\", \"mental health issues\", \"need fight depression\", \"feel cheerful happy\", \"need find ways\", \"help feel better\", \"black dog right\", \"pain mental health\", \"country fire service\", \"dance around house\", \"unwell tired trying\", \"cuddling black dog\", \"merry go round\", \"taken long time\", \"go round emotions\", \"good day going\", \"joined craft group\", \"need find works\", \"self help books\", \"laughter good soul\", \"depression explain people\", \"tired unwell tired\", \"outside look stars\", \"many wonderful people\", \"explain people around\", \"feeling really depressed\", \"huge gum leaves\", \"fight tired unwell\", \"wondering time depression\", \"reading self help\", \"write gratitude journal\", \"things know help\", \"thanks much comments\", \"channel end game\", \"going try make\", \"darn black dog\", \"makes lot sense\", \"cat hanging upside\", \"got nan dog\", \"dog feral cat\", \"thanks much comments\", \"cuddling black dog\", \"need find ways\", \"loads virtual hugs\", \"hoping today better\", \"dance around house\", \"help feel better\", \"write gratitude journal\", \"wondering time depression\", \"depression explain people\", \"tired unwell tired\", \"go round emotions\", \"ants huge gum\", \"positivity motivation humour\", \"sorry hear struggling\", \"merry go round\", \"feel cheerful happy\", \"feeling really depressed\", \"black dog right\", \"things know help\", \"much kind words\", \"exhausting full stop\", \"outside look stars\", \"snarling german shepherd\", \"many wonderful people\", \"self help books\", \"need fight depression\", \"care good lady\", \"reading self help\", \"ray ct scan\", \"mental health issues\", \"got nan dog\", \"cat hanging upside\", \"enjoy time family\", \"cat hanging upside\", \"reading self help\", \"sorry hear struggling\", \"go round emotions\", \"merry go round\", \"hoping today better\", \"self help books\", \"picnic drs office\", \"dance around house\", \"wondering time depression\", \"scenery driving along\", \"exhausting full stop\", \"things know help\", \"hour drive away\", \"black dog right\", \"joined craft group\", \"need find ways\", \"laughter good soul\", \"help feel better\", \"unwell tired trying\", \"good day going\", \"tired unwell tired\", \"better kind thoughts\", \"love go camping\", \"fight tired unwell\", \"snarling german shepherd\", \"need fight depression\", \"ants huge gum\", \"said really enjoyed\", \"ray ct scan\", \"going try make\", \"dog feral cat\", \"mental health issues\", \"sending virtual hugs\", \"please ever give\", \"huge gum leaves\", \"ants huge gum\", \"exhausting full stop\", \"outside look stars\", \"picnic drs office\", \"go round emotions\", \"things know help\", \"good day going\", \"wanting run away\", \"pain mental health\", \"need fight depression\", \"joined craft group\", \"love go camping\", \"snarling german shepherd\", \"time depression explain\", \"much kind words\", \"taken long time\", \"ray ct scan\", \"self help books\", \"explain people around\", \"hope okay today\", \"country fire service\", \"unwell tired trying\", \"scenery driving along\", \"need find ways\", \"reading self help\", \"hoping today better\", \"feel cheerful happy\", \"sorry hear struggling\", \"positivity motivation humour\", \"makes lot sense\", \"wave everyone else\", \"oh well hope\", \"enjoy time family\", \"said really enjoyed\", \"positivity motivation humour\", \"ray ct scan\", \"picnic drs office\", \"wanting run away\", \"loads virtual hugs\", \"better kind thoughts\", \"hope okay today\", \"tired unwell tired\", \"fight tired unwell\", \"country fire service\", \"scenery driving along\", \"feeling really depressed\", \"dance around house\", \"laughter good soul\", \"feel cheerful happy\", \"ants huge gum\", \"self help books\", \"write gratitude journal\", \"depression explain people\", \"need fight depression\", \"pain mental health\", \"explain people around\", \"taken long time\", \"time depression explain\", \"hour drive away\", \"wondering time depression\", \"go round emotions\", \"care good lady\", \"merry go round\", \"unwell tired trying\", \"exhausting full stop\", \"wave everyone else\", \"thanks going try\", \"makes lot sense\", \"cat hanging upside\", \"sorry hear struggling\", \"dog feral cat\", \"thanks much comments\", \"darn black dog\", \"going try make\", \"hour drive away\", \"oh well hope\", \"thanks going try\", \"good day going\", \"write gratitude journal\", \"scenery driving along\", \"go round emotions\", \"outside look stars\", \"channel end game\", \"feeling really depressed\", \"merry go round\", \"things know help\", \"sorry hear struggling\", \"joined craft group\", \"need find works\", \"reading self help\", \"fight tired unwell\", \"hoping today better\", \"pain mental health\", \"taken long time\", \"love go camping\", \"tired unwell tired\", \"many wonderful people\", \"laughter good soul\", \"help feel better\", \"need fight depression\", \"exhausting full stop\", \"need find ways\", \"positivity motivation humour\", \"dog feral cat\", \"loads virtual hugs\", \"cuddling black dog\", \"feel bit better\", \"cat hanging upside\", \"got nan dog\", \"enjoy time family\", \"feel bit better\", \"oh well hope\", \"care good lady\", \"many wonderful people\", \"said really enjoyed\", \"go round emotions\", \"laughter good soul\", \"merry go round\", \"joined craft group\", \"fight tired unwell\", \"explain people around\", \"need find works\", \"self help books\", \"wanting run away\", \"wondering time depression\", \"hope okay today\", \"depression explain people\", \"tired unwell tired\", \"good day going\", \"much kind words\", \"dance around house\", \"help feel better\", \"sorry hear struggling\", \"feeling really depressed\", \"unwell tired trying\", \"outside look stars\", \"love go camping\", \"scenery driving along\", \"time depression explain\", \"please ever give\", \"channel end game\", \"pain mental health\", \"going try make\", \"dog feral cat\", \"enjoy time family\", \"makes lot sense\", \"thanks much comments\", \"channel end game\", \"hope okay today\", \"exhausting full stop\", \"country fire service\", \"things know help\", \"care good lady\", \"good day going\", \"pain mental health\", \"outside look stars\", \"loads virtual hugs\", \"write gratitude journal\", \"huge gum leaves\", \"joined craft group\", \"go round emotions\", \"much kind words\", \"fight tired unwell\", \"hour drive away\", \"wanting run away\", \"black dog right\", \"merry go round\", \"taken long time\", \"tired unwell tired\", \"laughter good soul\", \"scenery driving along\", \"need find works\", \"sorry hear struggling\", \"better kind thoughts\", \"time depression explain\", \"feeling really depressed\", \"hoping today better\", \"explain people around\", \"feel bit better\", \"ray ct scan\", \"dog feral cat\", \"got nan dog\", \"makes lot sense\", \"mental health issues\", \"thanks much comments\", \"oh well hope\", \"cuddling black dog\", \"thanks going try\", \"going try make\", \"got nan dog\", \"unwell tired trying\", \"fight tired unwell\", \"much kind words\", \"tired unwell tired\", \"thanks much comments\", \"explain people around\", \"time depression explain\", \"taken long time\", \"need find works\", \"pain mental health\", \"better kind thoughts\", \"merry go round\", \"care good lady\", \"scenery driving along\", \"self help books\", \"reading self help\", \"hoping today better\", \"depression explain people\", \"channel end game\", \"things know help\", \"ants huge gum\", \"wondering time depression\", \"loads virtual hugs\", \"need fight depression\", \"laughter good soul\", \"black dog right\", \"snarling german shepherd\", \"picnic drs office\", \"need find ways\", \"mental health issues\", \"hour drive away\", \"dog feral cat\", \"cuddling black dog\", \"makes lot sense\", \"cuddling black dog\", \"need find works\", \"feeling really depressed\", \"self help books\", \"ants huge gum\", \"huge gum leaves\", \"black dog right\", \"love go camping\", \"good day going\", \"go round emotions\", \"outside look stars\", \"unwell tired trying\", \"joined craft group\", \"snarling german shepherd\", \"please ever give\", \"need fight depression\", \"care good lady\", \"wanting run away\", \"write gratitude journal\", \"pain mental health\", \"better kind thoughts\", \"laughter good soul\", \"fight tired unwell\", \"merry go round\", \"reading self help\", \"time depression explain\", \"loads virtual hugs\", \"thanks going try\", \"wondering time depression\", \"depression explain people\", \"country fire service\", \"ray ct scan\", \"mental health issues\", \"feel bit better\", \"thanks much comments\", \"oh well hope\", \"snarling german shepherd\", \"feel cheerful happy\", \"outside look stars\", \"cuddling black dog\", \"time depression explain\", \"help feel better\", \"merry go round\", \"fight tired unwell\", \"depression explain people\", \"need find ways\", \"tired unwell tired\", \"reading self help\", \"hoping today better\", \"ray ct scan\", \"self help books\", \"joined craft group\", \"love go camping\", \"taken long time\", \"ants huge gum\", \"many wonderful people\", \"need find works\", \"said really enjoyed\", \"laughter good soul\", \"good day going\", \"better kind thoughts\", \"unwell tired trying\", \"channel end game\", \"huge gum leaves\", \"exhausting full stop\", \"feeling really depressed\", \"thanks much comments\", \"wave everyone else\", \"loads virtual hugs\", \"thanks going try\", \"dog feral cat\", \"makes lot sense\", \"going try make\", \"dance around house\", \"mental health issues\", \"enjoy time family\", \"oh well hope\", \"love go camping\", \"many wonderful people\", \"much kind words\", \"pain mental health\", \"laughter good soul\", \"go round emotions\", \"scenery driving along\", \"hope okay today\", \"merry go round\", \"write gratitude journal\", \"tired unwell tired\", \"depression explain people\", \"unwell tired trying\", \"need find works\", \"taken long time\", \"good day going\", \"feeling really depressed\", \"wondering time depression\", \"explain people around\", \"help feel better\", \"channel end game\", \"outside look stars\", \"country fire service\", \"snarling german shepherd\", \"care good lady\", \"hoping today better\", \"need find ways\", \"huge gum leaves\", \"ray ct scan\", \"time depression explain\", \"reading self help\", \"feel bit better\", \"hour drive away\", \"dog feral cat\", \"cuddling black dog\", \"enjoy time family\", \"taken long time\", \"scenery driving along\", \"self help books\", \"black dog right\", \"merry go round\", \"feeling really depressed\", \"pain mental health\", \"care good lady\", \"explain people around\", \"tired unwell tired\", \"time depression explain\", \"reading self help\", \"good day going\", \"go round emotions\", \"much kind words\", \"country fire service\", \"depression explain people\", \"write gratitude journal\", \"unwell tired trying\", \"hoping today better\", \"joined craft group\", \"picnic drs office\", \"ants huge gum\", \"need find ways\", \"channel end game\", \"need fight depression\", \"exhausting full stop\", \"snarling german shepherd\", \"help feel better\", \"wanting run away\", \"darn black dog\", \"said really enjoyed\", \"wave everyone else\", \"dance around house\", \"thanks much comments\", \"positivity motivation humour\", \"feel bit better\", \"sorry hear struggling\", \"got nan dog\", \"need fight depression\", \"write gratitude journal\", \"fight tired unwell\", \"depression explain people\", \"time depression explain\", \"black dog right\", \"many wonderful people\", \"huge gum leaves\", \"ray ct scan\", \"merry go round\", \"go round emotions\", \"outside look stars\", \"feeling really depressed\", \"joined craft group\", \"tired unwell tired\", \"explain people around\", \"snarling german shepherd\", \"need find works\", \"please ever give\", \"wondering time depression\", \"wave everyone else\", \"help feel better\", \"unwell tired trying\", \"feel cheerful happy\", \"laughter good soul\", \"exhausting full stop\", \"country fire service\", \"loads virtual hugs\", \"love go camping\", \"channel end game\", \"feel bit better\", \"makes lot sense\", \"enjoy time family\", \"darn black dog\", \"sending virtual hugs\", \"dog feral cat\", \"wave everyone else\", \"cat hanging upside\", \"merry go round\", \"country fire service\", \"tired unwell tired\", \"outside look stars\", \"fight tired unwell\", \"ants huge gum\", \"pain mental health\", \"better kind thoughts\", \"help feel better\", \"write gratitude journal\", \"need fight depression\", \"wanting run away\", \"laughter good soul\", \"much kind words\", \"time depression explain\", \"scenery driving along\", \"hoping today better\", \"snarling german shepherd\", \"dance around house\", \"explain people around\", \"wondering time depression\", \"exhausting full stop\", \"need find works\", \"things know help\", \"feeling really depressed\", \"good day going\", \"unwell tired trying\", \"joined craft group\", \"self help books\", \"positivity motivation humour\", \"hour drive away\", \"thanks going try\", \"said really enjoyed\", \"loads virtual hugs\", \"darn black dog\", \"oh well hope\", \"sending virtual hugs\", \"going try make\", \"enjoy time family\", \"mental health issues\", \"dog feral cat\", \"thanks going try\", \"going try make\", \"better kind thoughts\", \"time depression explain\", \"love go camping\", \"write gratitude journal\", \"tired unwell tired\", \"unwell tired trying\", \"care good lady\", \"wanting run away\", \"much kind words\", \"ray ct scan\", \"scenery driving along\", \"huge gum leaves\", \"reading self help\", \"feeling really depressed\", \"feel cheerful happy\", \"need find works\", \"laughter good soul\", \"outside look stars\", \"loads virtual hugs\", \"ants huge gum\", \"need fight depression\", \"wondering time depression\", \"need find ways\", \"fight tired unwell\", \"said really enjoyed\", \"picnic drs office\", \"please ever give\", \"channel end game\", \"makes lot sense\", \"cat hanging upside\", \"cuddling black dog\", \"got nan dog\", \"thanks much comments\", \"mental health issues\", \"wanting run away\", \"huge gum leaves\", \"help feel better\", \"scenery driving along\", \"wondering time depression\", \"need find ways\", \"much kind words\", \"country fire service\", \"snarling german shepherd\", \"hoping today better\", \"ants huge gum\", \"good day going\", \"joined craft group\", \"love go camping\", \"fight tired unwell\", \"taken long time\", \"positivity motivation humour\", \"feel cheerful happy\", \"need find works\", \"outside look stars\", \"write gratitude journal\", \"merry go round\", \"sorry hear struggling\", \"pain mental health\", \"black dog right\", \"said really enjoyed\", \"loads virtual hugs\", \"ray ct scan\", \"feeling really depressed\", \"explain people around\", \"wave everyone else\", \"picnic drs office\", \"thanks going try\", \"going try make\", \"enjoy time family\", \"oh well hope\", \"got nan dog\", \"cuddling black dog\", \"sending virtual hugs\", \"joined craft group\", \"tired unwell tired\", \"help feel better\", \"fight tired unwell\", \"pain mental health\", \"dog feral cat\", \"self help books\", \"things know help\", \"huge gum leaves\", \"care good lady\", \"many wonderful people\", \"time depression explain\", \"much kind words\", \"hope okay today\", \"outside look stars\", \"go round emotions\", \"ants huge gum\", \"loads virtual hugs\", \"explain people around\", \"country fire service\", \"need find works\", \"scenery driving along\", \"picnic drs office\", \"snarling german shepherd\", \"wondering time depression\", \"sorry hear struggling\", \"wanting run away\", \"hour drive away\", \"good day going\", \"taken long time\", \"need fight depression\", \"darn black dog\", \"depression explain people\", \"said really enjoyed\", \"sending virtual hugs\", \"mental health issues\", \"wave everyone else\", \"cat hanging upside\", \"going try make\", \"got nan dog\", \"scenery driving along\", \"better kind thoughts\", \"black dog right\", \"wanting run away\", \"huge gum leaves\", \"good day going\", \"fight tired unwell\", \"merry go round\", \"feel cheerful happy\", \"write gratitude journal\", \"unwell tired trying\", \"many wonderful people\", \"need find works\", \"hoping today better\", \"snarling german shepherd\", \"much kind words\", \"tired unwell tired\", \"hope okay today\", \"depression explain people\", \"help feel better\", \"explain people around\", \"need find ways\", \"joined craft group\", \"ants huge gum\", \"need fight depression\", \"exhausting full stop\", \"loads virtual hugs\", \"taken long time\", \"please ever give\", \"time depression explain\", \"ray ct scan\", \"hour drive away\", \"cuddling black dog\", \"dog feral cat\", \"wave everyone else\", \"positivity motivation humour\"], \"Total\": [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.563064450828463, 1.5037615359901504, 1.483658869097858, 1.3886718780568712, 1.4612065515699784, 1.9180445152845655, 1.4882252455315617, 1.4776126795451492, 1.4795852598363308, 1.4402916550722988, 1.616946521628298, 1.4016581039244898, 1.513198646615843, 1.467481610581133, 1.607845245268764, 1.4247688530131664, 1.4368066079896062, 1.4990727122671508, 1.851998002176059, 1.432170647580749, 1.4586854304988013, 1.6853302427071133, 1.4554148192072491, 1.4217966886320763, 1.4193455566029802, 1.5174173501243682, 1.422270408367, 1.4304842719490056, 1.4798640342720342, 1.4885302149149113, 1.4934605411030424, 2.1339147675450336, 2.7203354851781283, 1.477759050887166, 1.5174173501243682, 1.4798640342720342, 1.43073548709982, 1.4795852598363308, 1.450048303745966, 1.4885302149149113, 1.607845245268764, 1.4554148192072491, 2.379399259217311, 1.4150179503103346, 1.4247688530131664, 1.4310577967911946, 1.4217966886320763, 1.4016581039244898, 1.4368066079896062, 1.4687549115638387, 1.4882252455315617, 1.483658869097858, 1.432725963844259, 1.467481610581133, 1.4337492347458547, 1.5037615359901504, 1.4776126795451492, 1.4586854304988013, 1.4293755870527565, 1.4612065515699784, 1.4570674301846922, 1.422270408367, 1.4402916550722988, 1.7710708006265887, 1.5177789969360864, 2.1339147675450336, 1.851998002176059, 1.8128617790803625, 2.27835076134387, 2.3267564350804015, 1.8952892947314586, 1.7710708006265887, 2.379399259217311, 1.4798640342720342, 1.5223305039787198, 1.5006462669823812, 1.607845245268764, 1.43073548709982, 1.422270408367, 1.4612065515699784, 1.483658869097858, 1.432725963844259, 1.4310577967911946, 1.4990727122671508, 1.6853302427071133, 1.616946521628298, 1.4150179503103346, 1.5174173501243682, 1.4776126795451492, 1.4795852598363308, 1.4402916550722988, 1.4630595308959966, 1.4934605411030424, 1.467481610581133, 1.4304842719490056, 1.4337492347458547, 1.4687549115638387, 1.477759050887166, 1.4932472383024782, 1.4570674301846922, 1.5645927857800987, 2.7203354851781283, 2.3267564350804015, 2.27835076134387, 2.613705183956911, 2.27835076134387, 1.4570674301846922, 1.616946521628298, 1.4310577967911946, 1.4150179503103346, 1.5006462669823812, 1.4687549115638387, 1.530210081432528, 1.607845245268764, 1.4612065515699784, 1.399251535308359, 1.4934605411030424, 1.4402916550722988, 1.5471585748720216, 1.4795852598363308, 1.4016581039244898, 1.4798640342720342, 1.4882252455315617, 1.43073548709982, 1.4554148192072491, 1.4217966886320763, 1.432725963844259, 1.432170647580749, 1.4981834933342533, 1.4293755870527565, 1.4304842719490056, 1.477759050887166, 1.4990727122671508, 1.563064450828463, 1.5645927857800987, 2.1339147675450336, 1.8952892947314586, 2.7203354851781283, 1.9180445152845655, 1.5091650008513582, 1.4586854304988013, 1.4990727122671508, 1.4934605411030424, 1.467481610581133, 1.530210081432528, 1.4310577967911946, 1.4402916550722988, 1.4217966886320763, 1.4193455566029802, 1.450048303745966, 1.477759050887166, 1.4016581039244898, 1.4981834933342533, 1.4304842719490056, 1.3886718780568712, 1.4630595308959966, 1.4247688530131664, 1.5645927857800987, 1.4687549115638387, 1.5037615359901504, 1.513198646615843, 1.4885302149149113, 1.4554148192072491, 1.399251535308359, 1.4798640342720342, 1.4570674301846922, 1.5006462669823812, 1.5174173501243682, 1.616946521628298, 1.6853302427071133, 1.8128617790803625, 1.637152733183545, 2.179966286345168, 2.613705183956911, 1.563064450828463, 1.6853302427071133, 1.5645927857800987, 1.530210081432528, 1.4193455566029802, 1.5223305039787198, 1.432170647580749, 1.513198646615843, 1.432725963844259, 1.4293755870527565, 1.4885302149149113, 1.399251535308359, 1.4776126795451492, 1.607845245268764, 1.4882252455315617, 1.5174173501243682, 1.4990727122671508, 1.4687549115638387, 1.422270408367, 1.483658869097858, 1.477759050887166, 1.450048303745966, 1.5037615359901504, 1.4247688530131664, 1.3886718780568712, 1.5471585748720216, 1.4612065515699784, 1.4310577967911946, 1.4932472383024782, 1.4150179503103346, 1.4554148192072491, 1.4934605411030424, 1.637152733183545, 1.859053310054199, 1.8128617790803625, 2.27835076134387, 1.616946521628298, 1.8952892947314586, 1.7710708006265887, 1.851998002176059, 2.1339147675450336, 1.5471585748720216, 2.179966286345168, 1.859053310054199, 1.4217966886320763, 1.422270408367, 1.399251535308359, 1.4310577967911946, 1.467481610581133, 1.5177789969360864, 1.4776126795451492, 1.4150179503103346, 1.4402916550722988, 1.616946521628298, 1.4016581039244898, 1.4368066079896062, 1.4570674301846922, 1.4293755870527565, 1.5006462669823812, 1.450048303745966, 1.4247688530131664, 1.4981834933342533, 1.432725963844259, 1.4337492347458547, 1.4882252455315617, 1.43073548709982, 1.477759050887166, 1.4934605411030424, 1.4798640342720342, 1.6853302427071133, 1.8952892947314586, 1.5223305039787198, 2.379399259217311, 1.7851827492077834, 2.27835076134387, 2.3267564350804015, 2.613705183956911, 1.7851827492077834, 2.179966286345168, 1.4932472383024782, 1.4337492347458547, 1.563064450828463, 1.4310577967911946, 1.4882252455315617, 1.4150179503103346, 1.4016581039244898, 1.4293755870527565, 1.5037615359901504, 1.4368066079896062, 1.4687549115638387, 1.4193455566029802, 1.4612065515699784, 1.513198646615843, 1.483658869097858, 1.432725963844259, 1.4217966886320763, 1.4630595308959966, 1.607845245268764, 1.43073548709982, 1.616946521628298, 1.4776126795451492, 1.4554148192072491, 1.467481610581133, 1.4981834933342533, 1.399251535308359, 1.3886718780568712, 1.5091650008513582, 1.5177789969360864, 1.450048303745966, 2.1339147675450336, 1.8952892947314586, 2.613705183956911, 1.8128617790803625, 1.7710708006265887, 1.5177789969360864, 1.513198646615843, 1.4934605411030424, 1.4885302149149113, 1.4402916550722988, 1.4932472383024782, 1.4217966886320763, 1.450048303745966, 1.467481610581133, 1.5223305039787198, 1.422270408367, 1.4586854304988013, 1.4016581039244898, 1.4310577967911946, 1.4630595308959966, 1.4293755870527565, 1.5471585748720216, 1.4193455566029802, 1.4795852598363308, 1.4150179503103346, 1.4247688530131664, 1.432725963844259, 1.4882252455315617, 1.399251535308359, 1.4368066079896062, 1.616946521628298, 1.432170647580749, 1.3886718780568712, 1.4776126795451492, 1.5006462669823812, 1.5037615359901504, 1.7851827492077834, 1.5645927857800987, 1.8952892947314586, 2.3267564350804015, 1.8128617790803625, 2.7203354851781283, 1.7710708006265887, 2.179966286345168, 2.379399259217311, 1.859053310054199, 2.1339147675450336, 2.3267564350804015, 1.4554148192072491, 1.4293755870527565, 1.4630595308959966, 1.432725963844259, 1.7710708006265887, 1.5037615359901504, 1.3886718780568712, 1.4247688530131664, 1.4368066079896062, 1.450048303745966, 1.432170647580749, 1.4150179503103346, 1.4932472383024782, 1.399251535308359, 1.4687549115638387, 1.4570674301846922, 1.5006462669823812, 1.483658869097858, 1.5177789969360864, 1.4402916550722988, 1.4990727122671508, 1.4612065515699784, 1.5223305039787198, 1.477759050887166, 1.4882252455315617, 1.4795852598363308, 1.4304842719490056, 1.530210081432528, 1.4798640342720342, 2.7203354851781283, 1.5471585748720216, 1.8952892947314586, 2.379399259217311, 1.8128617790803625, 2.379399259217311, 1.4368066079896062, 1.4776126795451492, 1.4687549115638387, 1.4990727122671508, 1.4586854304988013, 1.4795852598363308, 1.4981834933342533, 1.4217966886320763, 1.4310577967911946, 1.467481610581133, 1.4554148192072491, 1.4016581039244898, 1.4304842719490056, 1.5091650008513582, 1.477759050887166, 1.4932472383024782, 1.4193455566029802, 1.422270408367, 1.450048303745966, 1.432170647580749, 1.4882252455315617, 1.4293755870527565, 1.4150179503103346, 1.4570674301846922, 1.3886718780568712, 1.5223305039787198, 1.859053310054199, 1.4612065515699784, 1.483658869097858, 1.4885302149149113, 1.5645927857800987, 2.7203354851781283, 1.7851827492077834, 1.7710708006265887, 2.179966286345168, 1.4304842719490056, 1.5174173501243682, 1.467481610581133, 2.379399259217311, 1.3886718780568712, 1.43073548709982, 1.4150179503103346, 1.4293755870527565, 1.483658869097858, 1.4798640342720342, 1.432725963844259, 1.4570674301846922, 1.5006462669823812, 1.5645927857800987, 1.4687549115638387, 1.4016581039244898, 1.4981834933342533, 1.4247688530131664, 1.4990727122671508, 1.4337492347458547, 1.4368066079896062, 1.563064450828463, 1.4882252455315617, 1.4217966886320763, 1.432170647580749, 1.4554148192072491, 1.5177789969360864, 1.4586854304988013, 1.4934605411030424, 1.4776126795451492, 1.7710708006265887, 1.637152733183545, 1.5223305039787198, 1.859053310054199, 1.8952892947314586, 1.8128617790803625, 2.1339147675450336, 1.607845245268764, 2.7203354851781283, 2.613705183956911, 2.179966286345168, 1.4981834933342533, 1.4337492347458547, 1.4630595308959966, 1.450048303745966, 1.4882252455315617, 1.4310577967911946, 1.399251535308359, 1.513198646615843, 1.4150179503103346, 1.422270408367, 1.432725963844259, 1.483658869097858, 1.4554148192072491, 1.4368066079896062, 1.4247688530131664, 1.4217966886320763, 1.4776126795451492, 1.4612065515699784, 1.5037615359901504, 1.43073548709982, 1.5177789969360864, 1.467481610581133, 1.4885302149149113, 1.4304842719490056, 1.4932472383024782, 1.5006462669823812, 1.4798640342720342, 1.4586854304988013, 1.5645927857800987, 1.3886718780568712, 1.4570674301846922, 1.7851827492077834, 1.5471585748720216, 1.8952892947314586, 2.379399259217311, 2.613705183956911, 1.4247688530131664, 1.399251535308359, 1.4687549115638387, 1.4795852598363308, 1.4150179503103346, 1.4776126795451492, 1.450048303745966, 1.4932472383024782, 1.5037615359901504, 1.432725963844259, 1.3886718780568712, 1.4570674301846922, 1.4217966886320763, 1.4310577967911946, 1.4630595308959966, 1.4885302149149113, 1.483658869097858, 1.422270408367, 1.4554148192072491, 1.5006462669823812, 1.4016581039244898, 1.530210081432528, 1.4990727122671508, 1.4798640342720342, 1.5177789969360864, 1.477759050887166, 1.4934605411030424, 1.4304842719490056, 1.43073548709982, 1.4193455566029802, 1.851998002176059, 1.563064450828463, 1.637152733183545, 1.607845245268764, 1.7710708006265887, 1.6853302427071133, 1.7851827492077834, 1.616946521628298, 2.3267564350804015, 1.477759050887166, 1.422270408367, 1.4293755870527565, 1.483658869097858, 1.3886718780568712, 1.4795852598363308, 1.4337492347458547, 1.4586854304988013, 1.5645927857800987, 1.4150179503103346, 1.4310577967911946, 1.467481610581133, 1.4776126795451492, 1.4016581039244898, 1.432725963844259, 1.5037615359901504, 1.4304842719490056, 1.4368066079896062, 1.5091650008513582, 1.4612065515699784, 1.637152733183545, 1.43073548709982, 1.4554148192072491, 1.5174173501243682, 1.4882252455315617, 1.4934605411030424, 1.4885302149149113, 1.5223305039787198, 1.4981834933342533, 1.5177789969360864, 1.7851827492077834, 1.8128617790803625, 2.613705183956911, 1.851998002176059, 1.9180445152845655, 1.8952892947314586, 1.637152733183545, 2.27835076134387, 1.4150179503103346, 1.4885302149149113, 1.432725963844259, 1.467481610581133, 1.4293755870527565, 1.4990727122671508, 1.450048303745966, 1.432170647580749, 1.43073548709982, 1.422270408367, 1.477759050887166, 1.4193455566029802, 1.4882252455315617, 1.4630595308959966, 1.3886718780568712, 1.399251535308359, 1.5006462669823812, 1.4304842719490056, 1.607845245268764, 1.5037615359901504, 1.4612065515699784, 1.4934605411030424, 1.4368066079896062, 1.4402916550722988, 1.4776126795451492, 1.4217966886320763, 1.4554148192072491, 1.4016581039244898, 1.4687549115638387, 1.6853302427071133, 1.5471585748720216, 1.859053310054199, 1.563064450828463, 1.5223305039787198, 1.851998002176059, 2.179966286345168, 1.9180445152845655, 2.1339147675450336, 2.613705183956911, 2.7203354851781283, 1.8952892947314586, 1.859053310054199, 2.1339147675450336, 1.432170647580749, 1.3886718780568712, 1.4981834933342533, 1.422270408367, 1.432725963844259, 1.4554148192072491, 1.4932472383024782, 1.4193455566029802, 1.4630595308959966, 1.5645927857800987, 1.399251535308359, 1.4586854304988013, 1.4570674301846922, 1.4776126795451492, 1.5174173501243682, 1.4368066079896062, 1.4882252455315617, 1.467481610581133, 1.5223305039787198, 1.4990727122671508, 1.477759050887166, 1.4612065515699784, 1.4798640342720342, 1.4293755870527565, 1.563064450828463, 1.530210081432528, 1.5091650008513582, 1.5177789969360864, 1.8128617790803625, 2.27835076134387, 2.379399259217311, 2.3267564350804015, 1.7710708006265887, 2.7203354851781283, 1.4193455566029802, 1.4586854304988013, 1.43073548709982, 1.399251535308359, 1.4612065515699784, 1.4798640342720342, 1.4630595308959966, 1.4885302149149113, 1.4304842719490056, 1.5006462669823812, 1.4990727122671508, 1.4217966886320763, 1.4016581039244898, 1.4981834933342533, 1.4293755870527565, 1.4247688530131664, 1.6853302427071133, 1.5174173501243682, 1.4368066079896062, 1.467481610581133, 1.422270408367, 1.4150179503103346, 1.616946521628298, 1.450048303745966, 1.4795852598363308, 1.563064450828463, 1.5223305039787198, 1.5645927857800987, 1.4776126795451492, 1.5037615359901504, 1.637152733183545, 1.530210081432528, 1.859053310054199, 2.1339147675450336, 2.613705183956911, 2.179966286345168, 2.3267564350804015, 2.379399259217311, 1.9180445152845655, 1.4016581039244898, 1.432725963844259, 1.43073548709982, 1.4293755870527565, 1.450048303745966, 1.8952892947314586, 1.4687549115638387, 1.4402916550722988, 1.4586854304988013, 1.4932472383024782, 1.4337492347458547, 1.3886718780568712, 1.4630595308959966, 1.513198646615843, 1.467481610581133, 1.4310577967911946, 1.4990727122671508, 1.5223305039787198, 1.5037615359901504, 1.4885302149149113, 1.4368066079896062, 1.399251535308359, 1.530210081432528, 1.4304842719490056, 1.4612065515699784, 1.616946521628298, 1.4193455566029802, 1.5471585748720216, 1.4217966886320763, 1.4247688530131664, 1.477759050887166, 1.851998002176059, 1.483658869097858, 1.563064450828463, 1.9180445152845655, 2.7203354851781283, 1.637152733183545, 2.27835076134387, 2.1339147675450336, 2.3267564350804015, 1.399251535308359, 1.432170647580749, 1.4795852598363308, 1.4193455566029802, 1.4586854304988013, 1.4217966886320763, 1.4293755870527565, 1.4150179503103346, 1.5174173501243682, 1.422270408367, 1.4554148192072491, 1.4337492347458547, 1.4368066079896062, 1.5006462669823812, 1.4304842719490056, 1.4630595308959966, 1.432725963844259, 1.513198646615843, 1.483658869097858, 1.43073548709982, 1.5037615359901504, 1.4798640342720342, 1.4016581039244898, 1.4990727122671508, 1.477759050887166, 1.4934605411030424, 1.5223305039787198, 1.4247688530131664, 1.5091650008513582, 1.3886718780568712, 1.5645927857800987, 1.5471585748720216, 2.379399259217311, 1.8952892947314586, 1.637152733183545, 1.6853302427071133], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7851, 0.7643, 0.7488, 0.7465, 0.7388, 0.7063, 0.5374, 0.5363, 0.4725, 0.4548, 0.4386, 0.3914, 0.3732, 0.3556, 0.3306, 0.314, 0.3057, 0.1687, 0.147, 0.1394, 0.0645, -0.0035, -0.0131, -0.045, -0.0681, -0.0825, -0.0948, -0.1224, -0.1248, -0.1279, -0.1403, -0.4487, 2.0053, 0.63, 0.541, 0.4591, 0.4515, 0.4339, 0.4274, 0.401, 0.3678, 0.0031, -0.077, -0.0803, -0.0886, -0.1277, -0.1536, -0.1586, -0.1587, -0.1731, -0.1786, -0.183, -0.1834, -0.1851, -0.1888, -0.1932, -0.2014, -0.2031, -0.2106, -0.2298, -0.2357, -0.2415, -0.2422, -0.2972, -0.2566, -0.5027, -0.407, -0.3941, -0.5776, -0.6371, 1.5971, 1.4975, 1.2786, 0.6963, 0.6881, 0.6659, 0.5938, 0.5598, 0.5446, -0.1175, -0.1494, -0.1702, -0.1714, -0.1741, -0.1765, -0.179, -0.1805, -0.2071, -0.2092, -0.2205, -0.222, -0.2248, -0.2251, -0.2263, -0.2291, -0.2339, -0.2471, -0.2507, -0.2514, -0.2653, -0.2743, -0.6938, -0.635, -0.6609, 2.0806, 1.5804, 1.0183, 0.8293, 0.4685, 0.4596, 0.4389, 0.4302, -0.1231, -0.2177, -0.2471, -0.2583, -0.2712, -0.2729, -0.2909, -0.3018, -0.3136, -0.3225, -0.3247, -0.3365, -0.3465, -0.3547, -0.3613, -0.3623, -0.373, -0.3822, -0.3845, -0.3894, -0.3909, -0.3917, -0.4399, -0.6616, -0.6213, -0.9624, 1.4131, 1.298, 0.9511, 0.8531, 0.7353, 0.0768, 0.0612, 0.0276, 0.024, 0.0173, 0.0038, 0.0011, -0.0068, -0.0091, -0.0169, -0.0199, -0.0237, -0.0357, -0.039, -0.0418, -0.0468, -0.0644, -0.0667, -0.0699, -0.0702, -0.0704, -0.0724, -0.073, -0.0881, -0.0887, -0.0962, -0.1286, -0.1838, -0.1546, -0.3968, -0.5935, -0.1241, 1.437, 1.2167, 1.1457, 0.7065, 0.6669, 0.6004, 0.165, 0.1192, 0.0766, 0.0473, 0.0468, 0.0425, -0.005, -0.0299, -0.0328, -0.037, -0.0391, -0.0428, -0.0474, -0.0477, -0.0592, -0.0643, -0.0728, -0.074, -0.0751, -0.0763, -0.0772, -0.0859, -0.0879, -0.0893, -0.0972, -0.1623, -0.2532, -0.2473, -0.4218, -0.167, -0.316, -0.2549, 1.6675, 1.5522, 1.4079, 1.3705, 1.0629, 0.401, 0.3226, 0.2927, -0.1647, -0.1689, -0.1727, -0.2471, -0.266, -0.2718, -0.2781, -0.2794, -0.2855, -0.2895, -0.2918, -0.3064, -0.319, -0.3197, -0.3249, -0.3306, -0.3363, -0.3403, -0.3432, -0.3508, -0.3514, -0.3523, -0.3678, -0.4889, -0.3708, -0.7053, -0.4927, -0.7069, -0.7294, -0.892, 1.8006, 1.525, 0.866, 0.6931, 0.584, 0.441, 0.4058, 0.3424, -0.063, -0.0653, -0.0816, -0.0904, -0.1058, -0.1066, -0.1153, -0.1227, -0.1303, -0.1441, -0.1595, -0.1611, -0.1647, -0.1661, -0.1721, -0.1729, -0.1758, -0.1793, -0.1905, -0.1916, -0.1925, -0.1946, -0.1971, -0.1953, -0.3344, -0.3534, -0.6888, -0.3719, -0.3562, 1.287, 0.9718, 0.8014, 0.7936, 0.755, 0.7409, 0.6884, 0.0905, 0.0759, 0.0747, 0.0703, 0.0546, 0.0351, 0.0144, 0.0099, -0.0165, -0.0217, -0.0287, -0.0349, -0.0395, -0.041, -0.0527, -0.0551, -0.063, -0.0642, -0.0718, -0.0774, -0.0805, -0.0811, -0.0829, -0.0862, -0.1616, -0.1198, -0.2493, -0.3962, -0.2277, -0.5677, -0.2293, -0.4174, -0.4979, -0.2796, -0.4201, 1.389, 0.9898, 0.9739, 0.9301, 0.9202, 0.2145, 0.1464, 0.0926, 0.0577, 0.0558, 0.0396, 0.0282, -0.0233, -0.0388, -0.0443, -0.0462, -0.0474, -0.0599, -0.0626, -0.0659, -0.0663, -0.0667, -0.0683, -0.0885, -0.0903, -0.0919, -0.0956, -0.0974, -0.0983, -0.1001, -0.2322, -0.1168, -0.302, -0.5156, 1.9127, 1.3681, 0.8675, 0.5186, 0.1226, 0.101, 0.0662, -0.0072, -0.0099, -0.016, -0.0296, -0.0348, -0.0409, -0.0412, -0.0659, -0.0676, -0.0723, -0.0887, -0.0898, -0.0916, -0.1018, -0.1246, -0.1252, -0.1372, -0.1379, -0.1383, -0.1413, -0.1441, -0.1587, -0.1722, -0.1786, -0.1833, -0.218, -0.6534, -0.3263, -0.3349, -0.5437, 1.2848, 1.0018, 0.732, 0.5818, 0.1626, 0.16, 0.1584, 0.13, 0.0835, 0.0642, 0.0488, 0.0483, 0.0402, 0.0288, 0.0279, 0.0266, 0.0221, 0.0147, 0.0074, 0.0062, 0.0054, 0.0006, -0.0005, -0.0044, -0.008, -0.0164, -0.0166, -0.0198, -0.0267, -0.0277, -0.1123, -0.076, -0.0448, -0.1736, -0.1982, -0.1732, -0.3146, -0.0864, -0.6053, -0.5687, -0.4009, 1.4395, 1.034, 0.8638, 0.7755, 0.2504, 0.1316, 0.122, 0.1167, 0.1105, 0.1049, 0.087, 0.0696, 0.057, 0.0538, 0.036, 0.0308, 0.0281, 0.0258, 0.0174, 0.0167, 0.0157, 0.0146, 0.0052, 0.0033, 0.0011, -0.006, -0.017, -0.0209, -0.022, -0.0224, -0.0235, -0.0334, -0.0278, -0.1813, -0.4427, -0.5498, 1.0856, 1.0711, 0.9826, 0.1712, 0.1512, 0.1466, 0.1436, 0.1368, 0.1263, 0.1073, 0.1048, 0.0956, 0.0768, 0.0566, 0.0505, 0.0503, 0.0502, 0.0494, 0.0469, 0.0468, 0.0339, 0.029, 0.0272, 0.0209, 0.0202, 0.0199, 0.0088, 0.0088, 0.0087, 0.0073, -0.0187, -0.0037, -0.0257, -0.0274, -0.0953, -0.0796, -0.1353, -0.058, 2.1291, 0.8639, 0.108, 0.0862, 0.0565, 0.0182, 0.0144, -0.0042, -0.0153, -0.0154, -0.0232, -0.0297, -0.0396, -0.0435, -0.0584, -0.065, -0.0699, -0.0773, -0.0928, -0.1068, -0.1109, -0.1206, -0.1236, -0.1239, -0.1259, -0.1313, -0.1322, -0.1323, -0.1472, -0.1496, -0.1537, -0.2749, -0.3041, -0.6117, -0.3236, -0.3555, -0.3472, 2.0563, 1.5228, 0.0233, 0.0157, 0.007, -0.0084, -0.0143, -0.0472, -0.0553, -0.0588, -0.0603, -0.0635, -0.0674, -0.0693, -0.0721, -0.0747, -0.0766, -0.0836, -0.085, -0.088, -0.1047, -0.1093, -0.1113, -0.1136, -0.1136, -0.1161, -0.1197, -0.1231, -0.1285, -0.1376, -0.1445, -0.1803, -0.1687, -0.2918, -0.1793, -0.1626, -0.3351, -0.4838, -0.3695, -0.4667, -0.665, -0.7102, -0.3677, 1.7773, 1.6016, 0.9297, 0.0563, 0.0041, -0.001, -0.0273, -0.03, -0.0428, -0.0502, -0.0536, -0.0565, -0.0578, -0.0647, -0.0666, -0.075, -0.0825, -0.0953, -0.1062, -0.1162, -0.1166, -0.1225, -0.1229, -0.1234, -0.1352, -0.1352, -0.1405, -0.1435, -0.1511, -0.1548, -0.2724, -0.4785, -0.5435, -0.531, -0.2997, -0.7319, 1.0841, 0.3203, 0.2297, 0.218, 0.2158, 0.1744, 0.1693, 0.1658, 0.1596, 0.1557, 0.138, 0.1379, 0.1187, 0.1165, 0.1098, 0.093, 0.0923, 0.0876, 0.0858, 0.0838, 0.0734, 0.073, 0.0645, 0.0586, 0.0575, 0.0545, 0.0487, 0.0435, 0.0317, 0.0303, 0.0269, 0.0237, -0.1261, -0.2335, -0.4513, -0.2967, -0.3613, -0.3845, -0.1839, 1.1358, 0.2321, 0.2204, 0.2091, 0.1615, 0.1437, 0.1355, 0.1218, 0.1205, 0.1155, 0.1154, 0.115, 0.1142, 0.1081, 0.1061, 0.1046, 0.0912, 0.089, 0.0876, 0.0857, 0.0818, 0.079, 0.0705, 0.0702, 0.0638, 0.0626, 0.0576, 0.0524, 0.0451, 0.0451, 0.0406, -0.0178, 0.0353, -0.0023, -0.1748, -0.4675, -0.0453, -0.3358, -0.2861, -0.366, 0.2799, 0.2398, 0.2174, 0.2149, 0.2147, 0.2025, 0.1909, 0.1893, 0.1762, 0.1744, 0.1637, 0.1515, 0.1476, 0.1366, 0.1352, 0.1319, 0.1271, 0.1194, 0.1116, 0.1103, 0.1101, 0.1076, 0.1023, 0.1011, 0.0996, 0.0959, 0.0853, 0.085, 0.0828, 0.0816, 0.0733, 0.0577, -0.1425, -0.0548, 0.0162, -0.0299], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4231, -3.4826, -3.5115, -3.58, -3.5368, -3.2973, -3.7199, -3.7281, -3.7906, -3.8352, -3.7357, -3.9258, -3.8675, -3.9157, -3.8494, -3.9868, -3.9867, -4.0813, -3.8916, -4.1562, -4.2128, -4.1364, -4.2927, -4.3479, -4.3728, -4.3204, -4.3974, -4.4193, -4.3877, -4.385, -4.3941, -4.3456, -1.6488, -3.6343, -3.6968, -3.8038, -3.8451, -3.8292, -3.8559, -3.8561, -3.8122, -4.2765, -3.865, -4.388, -4.3894, -4.4241, -4.4565, -4.4758, -4.4511, -4.4435, -4.4359, -4.4433, -4.4787, -4.4564, -4.4834, -4.4401, -4.4658, -4.4804, -4.5082, -4.5054, -4.5142, -4.5441, -4.5322, -4.3804, -4.4942, -4.3996, -4.4456, -4.4541, -4.409, -4.4475, -2.4184, -2.5858, -2.5094, -3.5666, -3.5466, -3.5831, -3.5861, -3.7368, -3.758, -4.3931, -4.4098, -4.4655, -4.4678, -4.4241, -4.3094, -4.3533, -4.4883, -4.4449, -4.4736, -4.4836, -4.512, -4.4991, -4.4789, -4.4976, -4.5259, -4.5285, -4.5175, -4.5151, -4.5053, -4.5437, -4.4815, -4.3479, -4.4454, -4.4923, -1.6134, -2.251, -3.2601, -3.345, -3.8279, -3.8481, -3.81, -3.8402, -4.3525, -4.3977, -4.5227, -4.5772, -4.525, -4.5629, -4.5093, -4.5649, -4.6308, -4.5854, -4.582, -4.6332, -4.6261, -4.6577, -4.6566, -4.658, -4.6235, -4.6799, -4.6813, -4.6537, -4.6409, -4.5999, -4.6471, -4.5585, -4.6368, -4.6165, -2.5905, -2.9453, -3.3262, -3.3969, -3.5184, -4.1945, -4.1682, -4.2689, -4.266, -4.2856, -4.3009, -4.2821, -4.2712, -4.3263, -4.2675, -4.3167, -4.3502, -4.31, -4.3399, -4.249, -4.3173, -4.3113, -4.3073, -4.3269, -4.3498, -4.3893, -4.3353, -4.3515, -4.337, -4.3266, -4.2706, -4.2615, -4.2437, -4.3165, -4.2723, -4.2875, -4.3323, -2.6959, -2.9905, -3.0837, -3.5981, -3.5677, -3.6952, -4.0757, -4.176, -4.221, -4.2098, -4.2721, -4.222, -4.185, -4.2871, -4.2707, -4.287, -4.3095, -4.3454, -4.3077, -4.3121, -4.3424, -4.3112, -4.3737, -4.4005, -4.2936, -4.3519, -4.3737, -4.3398, -4.3956, -4.3689, -4.351, -4.3242, -4.288, -4.3073, -4.2532, -4.3414, -4.3315, -4.3382, -2.3711, -2.3447, -2.8105, -2.505, -2.9719, -3.9019, -3.98, -4.0262, -4.4611, -4.4402, -4.4103, -4.5115, -4.5737, -4.5618, -4.4524, -4.5966, -4.5779, -4.5679, -4.5894, -4.5553, -4.6023, -4.6206, -4.5755, -4.6259, -4.6308, -4.5976, -4.6398, -4.6152, -4.6052, -4.6152, -4.5007, -4.5044, -4.6054, -4.4933, -4.568, -4.5383, -4.5398, -4.5861, -2.2747, -2.3505, -3.3879, -3.6015, -3.6242, -3.8554, -3.8515, -3.9653, -4.3802, -4.363, -4.3285, -4.3828, -4.3762, -4.4113, -4.3909, -4.3633, -4.3906, -4.4393, -4.4625, -4.4355, -4.3447, -4.4628, -4.3464, -4.4373, -4.4554, -4.4506, -4.4411, -4.5105, -4.519, -4.4379, -4.4347, -4.4786, -4.2313, -4.3689, -4.3829, -4.4319, -4.4395, -2.9506, -3.2688, -3.4524, -3.4635, -3.535, -3.513, -3.6145, -4.1928, -4.1954, -4.1599, -4.2323, -4.2227, -4.2821, -4.282, -4.2644, -4.3141, -4.2402, -4.3334, -4.298, -4.3472, -4.3418, -4.348, -4.3123, -4.3819, -4.3567, -4.2461, -4.373, -4.407, -4.3455, -4.3319, -4.333, -4.237, -4.3271, -4.2648, -4.2065, -4.2876, -4.2218, -4.3126, -4.2929, -4.2859, -4.3143, -4.317, -2.4213, -3.2898, -3.3237, -3.3442, -3.3751, -3.8688, -4.1004, -4.2339, -4.2432, -4.2366, -4.2436, -4.2675, -4.331, -4.2927, -4.3632, -4.3166, -4.3258, -4.3089, -4.3229, -4.3035, -4.3563, -4.3167, -4.3439, -4.3231, -4.3546, -4.3492, -4.3587, -4.3943, -4.3278, -4.363, -3.8863, -4.3352, -4.3175, -4.3036, -2.1473, -2.4199, -3.425, -3.7459, -4.1478, -4.1491, -4.2111, -4.2702, -4.2605, -4.3189, -4.3261, -4.3061, -4.3205, -4.3584, -4.3627, -4.3109, -4.3366, -4.3426, -4.3945, -4.3942, -4.3851, -4.4202, -4.3825, -4.4348, -4.4456, -4.4167, -4.4678, -4.3787, -4.1935, -4.4478, -4.439, -4.4404, -4.4252, -4.3075, -4.4016, -4.4182, -4.4193, -3.012, -3.236, -3.5393, -3.2062, -4.1639, -4.1366, -4.1493, -4.1676, -4.1768, -4.1987, -4.2464, -4.2301, -4.2087, -4.1784, -4.2425, -4.2906, -4.2285, -4.2862, -4.2426, -4.2883, -4.287, -4.2076, -4.2578, -4.3074, -4.3037, -4.296, -4.2542, -4.2972, -4.2805, -4.2922, -4.1956, -4.2379, -4.2794, -4.2084, -4.2137, -4.2331, -4.2115, -4.2663, -4.2594, -4.2628, -4.2764, -2.8111, -3.2606, -3.4106, -3.5078, -4.0069, -4.1649, -4.1969, -4.124, -4.1972, -4.1977, -4.2082, -4.1907, -4.2226, -4.2386, -4.2648, -4.2721, -4.2363, -4.2498, -4.2294, -4.2799, -4.2219, -4.2567, -4.2519, -4.2935, -4.2528, -4.255, -4.2799, -4.2982, -4.2293, -4.3489, -4.3019, -4.1087, -4.2462, -4.1968, -4.2307, -4.2439, -3.2153, -3.2478, -3.2878, -4.0919, -4.1565, -4.1179, -4.1397, -4.1171, -4.1205, -4.1879, -4.2217, -4.1828, -4.2262, -4.2398, -4.2239, -4.2068, -4.2102, -4.2532, -4.2327, -4.2021, -4.2833, -4.2005, -4.2228, -4.242, -4.2174, -4.2445, -4.2449, -4.2881, -4.2879, -4.2974, -4.0573, -4.2119, -4.1876, -4.2074, -4.1786, -4.2125, -4.2106, -4.2324, -1.6812, -3.4005, -4.1946, -4.2114, -4.2039, -4.3083, -4.2487, -4.2988, -4.2927, -4.2226, -4.3309, -4.3261, -4.3109, -4.3079, -4.3756, -4.3603, -4.3167, -4.3741, -4.3852, -4.3501, -4.3865, -4.2825, -4.4202, -4.4034, -4.3638, -4.3886, -4.3859, -4.3894, -4.3819, -4.4002, -4.3913, -4.3503, -4.364, -4.3058, -4.3622, -4.359, -4.3626, -2.1056, -2.3086, -4.2844, -4.2413, -4.2883, -4.2797, -4.3119, -4.2972, -4.3385, -4.3545, -4.3569, -4.3661, -4.3317, -4.3739, -4.3294, -4.349, -4.4031, -4.4026, -4.334, -4.3849, -4.2846, -4.3562, -4.3869, -4.3673, -4.406, -4.4061, -4.3841, -4.426, -4.4081, -4.4548, -4.4149, -4.3132, -4.3871, -4.3266, -4.3875, -4.3972, -4.3737, -4.3593, -4.373, -4.3636, -4.3591, -4.3643, -4.3832, -2.2575, -2.2953, -3.366, -4.2702, -4.2465, -4.3036, -4.3226, -4.3096, -4.2967, -4.3548, -4.3279, -4.2637, -4.3767, -4.3421, -4.345, -4.3394, -4.3204, -4.3877, -4.3635, -4.3875, -4.3512, -4.3725, -4.3873, -4.399, -4.3981, -4.4328, -4.3487, -4.3729, -4.3944, -4.3924, -4.3323, -4.3099, -4.3316, -4.3414, -4.383, -4.386, -3.2206, -3.957, -4.067, -4.1009, -4.0598, -4.0885, -4.105, -4.0913, -4.1372, -4.0933, -4.112, -4.165, -4.1985, -4.1341, -4.1878, -4.2079, -4.0406, -4.1502, -4.2066, -4.1875, -4.2292, -4.2347, -4.1099, -4.2247, -4.2056, -4.1538, -4.1859, -4.1638, -4.2327, -4.2165, -4.135, -4.2057, -4.1609, -4.1304, -4.1453, -4.1722, -4.1716, -4.1725, -4.1874, -3.1814, -4.0632, -4.0762, -4.0885, -4.1217, -3.8718, -4.135, -4.1682, -4.1569, -4.1384, -4.1791, -4.2115, -4.1601, -4.1325, -4.1652, -4.1919, -4.1588, -4.1456, -4.1593, -4.1714, -4.2107, -4.2399, -4.1589, -4.2267, -4.2118, -4.1117, -4.247, -4.166, -4.2578, -4.2558, -4.2237, -4.0564, -4.225, -4.2105, -4.1783, -4.1216, -4.2072, -4.1672, -4.183, -4.1763, -4.039, -4.0558, -4.0457, -4.0897, -4.0627, -4.1004, -4.1067, -4.1184, -4.0616, -4.1282, -4.1159, -4.1431, -4.1448, -4.1123, -4.1616, -4.1424, -4.1682, -4.1213, -4.1488, -4.1864, -4.1368, -4.1553, -4.2149, -4.1489, -4.1647, -4.1579, -4.1493, -4.2158, -4.1605, -4.2449, -4.1339, -4.1607, -3.9305, -4.0703, -4.1456, -4.1628]}, \"token.table\": {\"Topic\": [4, 3, 7, 3, 4, 8, 7, 15, 11, 2, 7, 8, 5, 3], \"Freq\": [0.4389139797816543, 0.42027414950483927, 0.5399573859286138, 0.527623937295382, 0.38259862135104744, 0.5601667394801868, 0.46862227826955427, 0.4297828448749707, 0.5516140345279301, 0.3676017187764324, 0.45872269046717895, 0.45872269046717895, 0.521364333325516, 0.5646301659121753], \"Term\": [\"cat hanging upside\", \"cuddling black dog\", \"darn black dog\", \"dog feral cat\", \"enjoy time family\", \"feel bit better\", \"going try make\", \"got nan dog\", \"makes lot sense\", \"mental health issues\", \"oh well hope\", \"oh well hope\", \"sending virtual hugs\", \"thanks much comments\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 6, 11, 19, 4, 2, 15, 5, 16, 13, 18, 14, 3, 12, 8, 10, 7, 17, 20, 9]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1148027357146604245415092879\", ldavis_el1148027357146604245415092879_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1148027357146604245415092879\", ldavis_el1148027357146604245415092879_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1148027357146604245415092879\", ldavis_el1148027357146604245415092879_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=           Freq  cluster  topics           x           y\n",
       "topic                                                   \n",
       "0      9.093577        1       1   57.340801  -45.160965\n",
       "5      7.354537        1       2   21.855696  146.361694\n",
       "10     6.974520        1       3 -127.472023 -247.404434\n",
       "18     6.881090        1       4   -7.209630  -64.421181\n",
       "3      6.658955        1       5   13.966714   -0.807747\n",
       "1      6.405324        1       6   30.866001  -61.979271\n",
       "14     5.847784        1       7  -44.387264 -219.847183\n",
       "4      5.458965        1       8  -51.532150  -22.693823\n",
       "15     5.454297        1       9 -100.036209   68.530373\n",
       "12     5.120400        1      10   34.599964 -111.542145\n",
       "17     4.661106        1      11  -42.709591  -74.722687\n",
       "13     4.436654        1      12   67.375290  -77.222115\n",
       "2      4.353513        1      13  -62.154190 -197.095779\n",
       "11     3.565237        1      14  -11.369043  -31.883074\n",
       "7      3.565237        1      15   26.812639   91.626335\n",
       "9      3.401980        1      16   41.173473   -8.377916\n",
       "6      3.401475        1      17   50.965069   33.773052\n",
       "16     2.613705        1      18   -0.552276 -116.366982\n",
       "19     2.613705        1      19   76.186279  361.444550\n",
       "8      2.137939        1      20   77.538986  -20.779543, topic_info=     Category      Freq                          Term     Total  loglift  \\\n",
       "term                                                                       \n",
       "22    Default  2.000000                   got nan dog  2.000000  30.0000   \n",
       "12    Default  2.000000             enjoy time family  2.000000  29.0000   \n",
       "34    Default  2.000000          mental health issues  2.000000  28.0000   \n",
       "4     Default  2.000000            cat hanging upside  2.000000  27.0000   \n",
       "20    Default  2.000000                going try make  2.000000  26.0000   \n",
       "62    Default  1.000000            wave everyone else  1.000000  25.0000   \n",
       "55    Default  1.000000              thanks going try  1.000000  24.0000   \n",
       "40    Default  2.000000                  oh well hope  2.000000  23.0000   \n",
       "7     Default  2.000000            cuddling black dog  2.000000  22.0000   \n",
       "32    Default  1.000000               makes lot sense  1.000000  21.0000   \n",
       "15    Default  1.000000               feel bit better  1.000000  20.0000   \n",
       "9     Default  1.000000                darn black dog  1.000000  19.0000   \n",
       "11    Default  1.000000                 dog feral cat  1.000000  18.0000   \n",
       "51    Default  1.000000          sending virtual hugs  1.000000  17.0000   \n",
       "56    Default  1.000000          thanks much comments  1.000000  16.0000   \n",
       "31    Default  1.000000               love go camping  1.000000  15.0000   \n",
       "45    Default  1.000000  positivity motivation humour  1.000000  14.0000   \n",
       "26    Default  1.000000               hour drive away  1.000000  13.0000   \n",
       "61    Default  1.000000              wanting run away  1.000000  12.0000   \n",
       "52    Default  1.000000      snarling german shepherd  1.000000  11.0000   \n",
       "28    Default  1.000000            joined craft group  1.000000  10.0000   \n",
       "36    Default  1.000000               much kind words  1.000000   9.0000   \n",
       "5     Default  1.000000              channel end game  1.000000   8.0000   \n",
       "49    Default  1.000000         scenery driving along  1.000000   7.0000   \n",
       "44    Default  1.000000              please ever give  1.000000   6.0000   \n",
       "33    Default  1.000000         many wonderful people  1.000000   5.0000   \n",
       "46    Default  1.000000                   ray ct scan  1.000000   4.0000   \n",
       "54    Default  1.000000               taken long time  1.000000   3.0000   \n",
       "1     Default  1.000000          better kind thoughts  1.000000   2.0000   \n",
       "50    Default  1.000000               self help books  1.000000   1.0000   \n",
       "...       ...       ...                           ...       ...      ...   \n",
       "18    Topic20  0.036988            fight tired unwell  1.429376   0.1909   \n",
       "35    Topic20  0.036557                merry go round  1.415018   0.1893   \n",
       "16    Topic20  0.038692           feel cheerful happy  1.517417   0.1762   \n",
       "64    Topic20  0.036200       write gratitude journal  1.422270   0.1744   \n",
       "60    Topic20  0.036648           unwell tired trying  1.455415   0.1637   \n",
       "33    Topic20  0.035667         many wonderful people  1.433749   0.1515   \n",
       "39    Topic20  0.035605               need find works  1.436807   0.1476   \n",
       "25    Topic20  0.036781           hoping today better  1.500646   0.1366   \n",
       "52    Topic20  0.035011      snarling german shepherd  1.430484   0.1352   \n",
       "36    Topic20  0.035691               much kind words  1.463060   0.1319   \n",
       "59    Topic20  0.034781            tired unwell tired  1.432726   0.1271   \n",
       "24    Topic20  0.036452               hope okay today  1.513199   0.1194   \n",
       "10    Topic20  0.035464     depression explain people  1.483659   0.1116   \n",
       "23    Topic20  0.034155              help feel better  1.430735   0.1103   \n",
       "14    Topic20  0.035892         explain people around  1.503762   0.1101   \n",
       "38    Topic20  0.035233                need find ways  1.479864   0.1076   \n",
       "28    Topic20  0.033195            joined craft group  1.401658   0.1023   \n",
       "0     Topic20  0.035459                 ants huge gum  1.499073   0.1011   \n",
       "37    Topic20  0.034903         need fight depression  1.477759   0.0996   \n",
       "13    Topic20  0.035142          exhausting full stop  1.493461   0.0959   \n",
       "30    Topic20  0.035444            loads virtual hugs  1.522331   0.0853   \n",
       "54    Topic20  0.033163               taken long time  1.424769   0.0850   \n",
       "44    Topic20  0.035050              please ever give  1.509165   0.0828   \n",
       "58    Topic20  0.032212       time depression explain  1.388672   0.0816   \n",
       "46    Topic20  0.035994                   ray ct scan  1.564593   0.0733   \n",
       "26    Topic20  0.035043               hour drive away  1.547159   0.0577   \n",
       "7     Topic20  0.044115            cuddling black dog  2.379399  -0.1425   \n",
       "11    Topic20  0.038360                 dog feral cat  1.895289  -0.0548   \n",
       "62    Topic20  0.035574            wave everyone else  1.637153   0.0162   \n",
       "45    Topic20  0.034971  positivity motivation humour  1.685330  -0.0299   \n",
       "\n",
       "      logprob  \n",
       "term           \n",
       "22    30.0000  \n",
       "12    29.0000  \n",
       "34    28.0000  \n",
       "4     27.0000  \n",
       "20    26.0000  \n",
       "62    25.0000  \n",
       "55    24.0000  \n",
       "40    23.0000  \n",
       "7     22.0000  \n",
       "32    21.0000  \n",
       "15    20.0000  \n",
       "9     19.0000  \n",
       "11    18.0000  \n",
       "51    17.0000  \n",
       "56    16.0000  \n",
       "31    15.0000  \n",
       "45    14.0000  \n",
       "26    13.0000  \n",
       "61    12.0000  \n",
       "52    11.0000  \n",
       "28    10.0000  \n",
       "36     9.0000  \n",
       "5      8.0000  \n",
       "49     7.0000  \n",
       "44     6.0000  \n",
       "33     5.0000  \n",
       "46     4.0000  \n",
       "54     3.0000  \n",
       "1      2.0000  \n",
       "50     1.0000  \n",
       "...       ...  \n",
       "18    -4.1067  \n",
       "35    -4.1184  \n",
       "16    -4.0616  \n",
       "64    -4.1282  \n",
       "60    -4.1159  \n",
       "33    -4.1431  \n",
       "39    -4.1448  \n",
       "25    -4.1123  \n",
       "52    -4.1616  \n",
       "36    -4.1424  \n",
       "59    -4.1682  \n",
       "24    -4.1213  \n",
       "10    -4.1488  \n",
       "23    -4.1864  \n",
       "14    -4.1368  \n",
       "38    -4.1553  \n",
       "28    -4.2149  \n",
       "0     -4.1489  \n",
       "37    -4.1647  \n",
       "13    -4.1579  \n",
       "30    -4.1493  \n",
       "54    -4.2158  \n",
       "44    -4.1605  \n",
       "58    -4.2449  \n",
       "46    -4.1339  \n",
       "26    -4.1607  \n",
       "7     -3.9305  \n",
       "11    -4.0703  \n",
       "62    -4.1456  \n",
       "45    -4.1628  \n",
       "\n",
       "[777 rows x 6 columns], token_table=      Topic      Freq                  Term\n",
       "term                                       \n",
       "4         4  0.438914    cat hanging upside\n",
       "7         3  0.420274    cuddling black dog\n",
       "9         7  0.539957        darn black dog\n",
       "11        3  0.527624         dog feral cat\n",
       "12        4  0.382599     enjoy time family\n",
       "15        8  0.560167       feel bit better\n",
       "20        7  0.468622        going try make\n",
       "22       15  0.429783           got nan dog\n",
       "32       11  0.551614       makes lot sense\n",
       "34        2  0.367602  mental health issues\n",
       "40        7  0.458723          oh well hope\n",
       "40        8  0.458723          oh well hope\n",
       "51        5  0.521364  sending virtual hugs\n",
       "56        3  0.564630  thanks much comments, R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 6, 11, 19, 4, 2, 15, 5, 16, 13, 18, 14, 3, 12, 8, 10, 7, 17, 20, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualising LDA Topic model based on tfidf_vectorizer\n",
    "pyLDAvis.sklearn.prepare(lda_tfidf, X_tfidf, tfidf_vectorizer, n_jobs=-1, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:10.875640Z",
     "start_time": "2018-05-23T19:11:08.887824Z"
    }
   },
   "outputs": [],
   "source": [
    "# import vader sentiment analyser\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:11.064649Z",
     "start_time": "2018-05-23T19:11:10.919645Z"
    }
   },
   "outputs": [],
   "source": [
    "#instantiation of sentiment analyser\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:11.870183Z",
     "start_time": "2018-05-23T19:11:11.866185Z"
    }
   },
   "outputs": [],
   "source": [
    "#create empty dataframe to store sentiment scores for each post\n",
    "polarity_scores = pd.DataFrame(columns=[\"post\", \"sentiment_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:13.648235Z",
     "start_time": "2018-05-23T19:11:12.726145Z"
    }
   },
   "outputs": [],
   "source": [
    "# computing sentimment scores for each post and populating dataframe\n",
    "posts = []\n",
    "polarities = []\n",
    "for post in postings.content:\n",
    "    polarity = sentiment_analyzer.polarity_scores(post)\n",
    "    posts.append(post)\n",
    "    polarities.append(polarity)\n",
    "\n",
    "polarity_scores[\"post\"] = posts\n",
    "polarity_scores[\"sentiment_score\"] = polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:14.803343Z",
     "start_time": "2018-05-23T19:11:14.770347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Right now I feel like I don't have the energy ...</td>\n",
       "      <td>{'neg': 0.26, 'neu': 0.544, 'pos': 0.196, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mrs dool.\\nI am sending you a big reassuring h...</td>\n",
       "      <td>{'neg': 0.158, 'neu': 0.608, 'pos': 0.234, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Doolhof\\nI'm sorry that you feel so low.  Y...</td>\n",
       "      <td>{'neg': 0.127, 'neu': 0.686, 'pos': 0.187, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi Quirky,\\nThanks for the virtual hug, I need...</td>\n",
       "      <td>{'neg': 0.144, 'neu': 0.727, 'pos': 0.129, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi Summer Rose,\\nThanks for your encouragement...</td>\n",
       "      <td>{'neg': 0.156, 'neu': 0.708, 'pos': 0.136, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dools so sorry to hear you're in deep darl.\\nI...</td>\n",
       "      <td>{'neg': 0.045, 'neu': 0.643, 'pos': 0.313, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dear Dools how are you feeling today darl 🤗\\nI...</td>\n",
       "      <td>{'neg': 0.086, 'neu': 0.633, 'pos': 0.282, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dear DB,\\nThank you so very much. I have been ...</td>\n",
       "      <td>{'neg': 0.086, 'neu': 0.738, 'pos': 0.176, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi all \\nYeah it does pull us under its so dam...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.569, 'pos': 0.351, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hi DB,\\nWoke up this morning wondering why I h...</td>\n",
       "      <td>{'neg': 0.199, 'neu': 0.731, 'pos': 0.07, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ah Mrs D, so sorry you are hurting. Here is a ...</td>\n",
       "      <td>{'neg': 0.216, 'neu': 0.633, 'pos': 0.151, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ahh Dools 🤗 how awful how you're feeling. Feel...</td>\n",
       "      <td>{'neg': 0.157, 'neu': 0.506, 'pos': 0.337, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hi Mrs D, \\nI'm relieved you asked for help. I...</td>\n",
       "      <td>{'neg': 0.108, 'neu': 0.72, 'pos': 0.171, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dear Nat, DB and Shell,\\nI have tears of grati...</td>\n",
       "      <td>{'neg': 0.101, 'neu': 0.766, 'pos': 0.133, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hi Dools\\nAwwww, sending you loads of virtual ...</td>\n",
       "      <td>{'neg': 0.032, 'neu': 0.804, 'pos': 0.164, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dear Bev\\nI would give anything to have the wo...</td>\n",
       "      <td>{'neg': 0.106, 'neu': 0.722, 'pos': 0.173, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hi PamelaR,\\nThanks for your message. I have b...</td>\n",
       "      <td>{'neg': 0.067, 'neu': 0.727, 'pos': 0.206, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hi Mary,\\nThanks so much for your very kind wo...</td>\n",
       "      <td>{'neg': 0.101, 'neu': 0.757, 'pos': 0.142, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hello Dools\\nMemory caught up with me.... no i...</td>\n",
       "      <td>{'neg': 0.056, 'neu': 0.735, 'pos': 0.209, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hi Mrs D (and a wave to everyone else of cours...</td>\n",
       "      <td>{'neg': 0.103, 'neu': 0.706, 'pos': 0.192, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>So many beautiful people 🤗\\nI imagined once a ...</td>\n",
       "      <td>{'neg': 0.106, 'neu': 0.558, 'pos': 0.336, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hi Doolhof\\nI've been offline awhile and just ...</td>\n",
       "      <td>{'neg': 0.031, 'neu': 0.817, 'pos': 0.151, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hello Dools\\nI really hope you have relief soo...</td>\n",
       "      <td>{'neg': 0.062, 'neu': 0.671, 'pos': 0.267, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Beautifully said Rose \\nThe stars ... oh yeah ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.535, 'pos': 0.465, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Dear precious people,\\nThank you so very much ...</td>\n",
       "      <td>{'neg': 0.087, 'neu': 0.717, 'pos': 0.197, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hell Dools,\\nYou are so loved and you have a p...</td>\n",
       "      <td>{'neg': 0.106, 'neu': 0.677, 'pos': 0.217, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hi Mrs Dools\\nI have not much to say. So I tho...</td>\n",
       "      <td>{'neg': 0.078, 'neu': 0.74, 'pos': 0.182, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hi Karen,\\nThank you so very much for your ver...</td>\n",
       "      <td>{'neg': 0.068, 'neu': 0.726, 'pos': 0.206, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Thankyou Mrs Dools\\nPainting?  Well a coincide...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yeah.. the walls go up in a second. Its like t...</td>\n",
       "      <td>{'neg': 0.152, 'neu': 0.61, 'pos': 0.238, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Fantastic Dools. What a great day! You've achi...</td>\n",
       "      <td>{'neg': 0.06, 'neu': 0.835, 'pos': 0.104, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Hi Chloe, Karen, Pamela and Everyone,\\nThank y...</td>\n",
       "      <td>{'neg': 0.07, 'neu': 0.774, 'pos': 0.156, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Hey Doolsy and all \\n\\r\\nFinally got here, I'v...</td>\n",
       "      <td>{'neg': 0.129, 'neu': 0.663, 'pos': 0.208, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>LOL Dools, a snarling German Shepherd doesn't ...</td>\n",
       "      <td>{'neg': 0.036, 'neu': 0.75, 'pos': 0.214, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Hi All,\\nI enjoyed a walk in the park before t...</td>\n",
       "      <td>{'neg': 0.052, 'neu': 0.76, 'pos': 0.188, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>lol Doolsy you've got a wicked sense of humour...</td>\n",
       "      <td>{'neg': 0.035, 'neu': 0.729, 'pos': 0.235, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>I'm sitting here with tears running down my fa...</td>\n",
       "      <td>{'neg': 0.05, 'neu': 0.584, 'pos': 0.366, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>hi demonblaster, hello again Pammy, hey Dools\\...</td>\n",
       "      <td>{'neg': 0.09, 'neu': 0.621, 'pos': 0.289, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>You poor thing having that big cry although I ...</td>\n",
       "      <td>{'neg': 0.068, 'neu': 0.625, 'pos': 0.307, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Hi Dools\\nThankyou for the super kind post :-)...</td>\n",
       "      <td>{'neg': 0.047, 'neu': 0.704, 'pos': 0.249, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Hey Paul, Mate,\\nHI. Please know I have not ta...</td>\n",
       "      <td>{'neg': 0.071, 'neu': 0.729, 'pos': 0.2, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Hi Chloe, Pammy, DB, Paul and All reading,\\nTh...</td>\n",
       "      <td>{'neg': 0.046, 'neu': 0.795, 'pos': 0.159, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Hi Dools,\\nThat lady at the Op Shop sounds lov...</td>\n",
       "      <td>{'neg': 0.121, 'neu': 0.718, 'pos': 0.16, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Hello Dools (wave to everyone else)\\nPlease do...</td>\n",
       "      <td>{'neg': 0.034, 'neu': 0.702, 'pos': 0.263, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Hi Chloe, PamelaR and All,\\nIt does seem to he...</td>\n",
       "      <td>{'neg': 0.042, 'neu': 0.8, 'pos': 0.158, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Hi Dools,\\nOnce again, your imaginative mind m...</td>\n",
       "      <td>{'neg': 0.035, 'neu': 0.805, 'pos': 0.16, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Hello Dools, and everyone else.\\nThe upside do...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.727, 'pos': 0.273, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Hi Chloe,\\nIt is a bit of a journey to get to ...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.787, 'pos': 0.134, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Hey Grandy,\\nThanks. I am going to try to make...</td>\n",
       "      <td>{'neg': 0.08, 'neu': 0.784, 'pos': 0.136, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Hi Dools,\\nmaybe you could leave your cat and ...</td>\n",
       "      <td>{'neg': 0.03, 'neu': 0.815, 'pos': 0.154, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Hi Chloe,\\nThanks. I am going to try to make p...</td>\n",
       "      <td>{'neg': 0.02, 'neu': 0.771, 'pos': 0.21, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Hey Dools,\\nthats good that you are going. I a...</td>\n",
       "      <td>{'neg': 0.022, 'neu': 0.721, 'pos': 0.257, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Hello Dools,\\nIm just popping in to wish you a...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.619, 'pos': 0.381, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Hey Grandy,\\nI am very relived that nana is wi...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Hi Chloe,\\nLast night I saw the fires on the n...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Hi Dools,\\nyes the fires are pretty bad, the w...</td>\n",
       "      <td>{'neg': 0.126, 'neu': 0.646, 'pos': 0.228, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Hi Chloe,\\nI hope all goes well with catching ...</td>\n",
       "      <td>{'neg': 0.044, 'neu': 0.701, 'pos': 0.255, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Hi Dools,\\nThis morning was ok. we got a lot d...</td>\n",
       "      <td>{'neg': 0.078, 'neu': 0.648, 'pos': 0.274, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Hello Dools , Chloe,\\nDools I hope you enjoy t...</td>\n",
       "      <td>{'neg': 0.076, 'neu': 0.614, 'pos': 0.31, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Hugs are special, especially tight ones that m...</td>\n",
       "      <td>{'neg': 0.096, 'neu': 0.695, 'pos': 0.209, 'co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  post  \\\n",
       "0    Right now I feel like I don't have the energy ...   \n",
       "1    Mrs dool.\\nI am sending you a big reassuring h...   \n",
       "2    Hi Doolhof\\nI'm sorry that you feel so low.  Y...   \n",
       "3    Hi Quirky,\\nThanks for the virtual hug, I need...   \n",
       "4    Hi Summer Rose,\\nThanks for your encouragement...   \n",
       "5    Dools so sorry to hear you're in deep darl.\\nI...   \n",
       "6    Dear Dools how are you feeling today darl 🤗\\nI...   \n",
       "7    Dear DB,\\nThank you so very much. I have been ...   \n",
       "8    Hi all \\nYeah it does pull us under its so dam...   \n",
       "9    Hi DB,\\nWoke up this morning wondering why I h...   \n",
       "10   Ah Mrs D, so sorry you are hurting. Here is a ...   \n",
       "11   Ahh Dools 🤗 how awful how you're feeling. Feel...   \n",
       "12   Hi Mrs D, \\nI'm relieved you asked for help. I...   \n",
       "13   Dear Nat, DB and Shell,\\nI have tears of grati...   \n",
       "14   Hi Dools\\nAwwww, sending you loads of virtual ...   \n",
       "15   Dear Bev\\nI would give anything to have the wo...   \n",
       "16   Hi PamelaR,\\nThanks for your message. I have b...   \n",
       "17   Hi Mary,\\nThanks so much for your very kind wo...   \n",
       "18   Hello Dools\\nMemory caught up with me.... no i...   \n",
       "19   Hi Mrs D (and a wave to everyone else of cours...   \n",
       "20   So many beautiful people 🤗\\nI imagined once a ...   \n",
       "21   Hi Doolhof\\nI've been offline awhile and just ...   \n",
       "22   Hello Dools\\nI really hope you have relief soo...   \n",
       "23   Beautifully said Rose \\nThe stars ... oh yeah ...   \n",
       "24   Dear precious people,\\nThank you so very much ...   \n",
       "25   Hell Dools,\\nYou are so loved and you have a p...   \n",
       "26   Hi Mrs Dools\\nI have not much to say. So I tho...   \n",
       "27   Hi Karen,\\nThank you so very much for your ver...   \n",
       "28   Thankyou Mrs Dools\\nPainting?  Well a coincide...   \n",
       "29   Yeah.. the walls go up in a second. Its like t...   \n",
       "..                                                 ...   \n",
       "120  Fantastic Dools. What a great day! You've achi...   \n",
       "121  Hi Chloe, Karen, Pamela and Everyone,\\nThank y...   \n",
       "122  Hey Doolsy and all \\n\\r\\nFinally got here, I'v...   \n",
       "123  LOL Dools, a snarling German Shepherd doesn't ...   \n",
       "124  Hi All,\\nI enjoyed a walk in the park before t...   \n",
       "125  lol Doolsy you've got a wicked sense of humour...   \n",
       "126  I'm sitting here with tears running down my fa...   \n",
       "127  hi demonblaster, hello again Pammy, hey Dools\\...   \n",
       "128  You poor thing having that big cry although I ...   \n",
       "129  Hi Dools\\nThankyou for the super kind post :-)...   \n",
       "130  Hey Paul, Mate,\\nHI. Please know I have not ta...   \n",
       "131  Hi Chloe, Pammy, DB, Paul and All reading,\\nTh...   \n",
       "132  Hi Dools,\\nThat lady at the Op Shop sounds lov...   \n",
       "133  Hello Dools (wave to everyone else)\\nPlease do...   \n",
       "134  Hi Chloe, PamelaR and All,\\nIt does seem to he...   \n",
       "135  Hi Dools,\\nOnce again, your imaginative mind m...   \n",
       "136  Hello Dools, and everyone else.\\nThe upside do...   \n",
       "137  Hi Chloe,\\nIt is a bit of a journey to get to ...   \n",
       "138  Hey Grandy,\\nThanks. I am going to try to make...   \n",
       "139  Hi Dools,\\nmaybe you could leave your cat and ...   \n",
       "140  Hi Chloe,\\nThanks. I am going to try to make p...   \n",
       "141  Hey Dools,\\nthats good that you are going. I a...   \n",
       "142  Hello Dools,\\nIm just popping in to wish you a...   \n",
       "143  Hey Grandy,\\nI am very relived that nana is wi...   \n",
       "144  Hi Chloe,\\nLast night I saw the fires on the n...   \n",
       "145  Hi Dools,\\nyes the fires are pretty bad, the w...   \n",
       "146  Hi Chloe,\\nI hope all goes well with catching ...   \n",
       "147  Hi Dools,\\nThis morning was ok. we got a lot d...   \n",
       "148  Hello Dools , Chloe,\\nDools I hope you enjoy t...   \n",
       "149  Hugs are special, especially tight ones that m...   \n",
       "\n",
       "                                       sentiment_score  \n",
       "0    {'neg': 0.26, 'neu': 0.544, 'pos': 0.196, 'com...  \n",
       "1    {'neg': 0.158, 'neu': 0.608, 'pos': 0.234, 'co...  \n",
       "2    {'neg': 0.127, 'neu': 0.686, 'pos': 0.187, 'co...  \n",
       "3    {'neg': 0.144, 'neu': 0.727, 'pos': 0.129, 'co...  \n",
       "4    {'neg': 0.156, 'neu': 0.708, 'pos': 0.136, 'co...  \n",
       "5    {'neg': 0.045, 'neu': 0.643, 'pos': 0.313, 'co...  \n",
       "6    {'neg': 0.086, 'neu': 0.633, 'pos': 0.282, 'co...  \n",
       "7    {'neg': 0.086, 'neu': 0.738, 'pos': 0.176, 'co...  \n",
       "8    {'neg': 0.079, 'neu': 0.569, 'pos': 0.351, 'co...  \n",
       "9    {'neg': 0.199, 'neu': 0.731, 'pos': 0.07, 'com...  \n",
       "10   {'neg': 0.216, 'neu': 0.633, 'pos': 0.151, 'co...  \n",
       "11   {'neg': 0.157, 'neu': 0.506, 'pos': 0.337, 'co...  \n",
       "12   {'neg': 0.108, 'neu': 0.72, 'pos': 0.171, 'com...  \n",
       "13   {'neg': 0.101, 'neu': 0.766, 'pos': 0.133, 'co...  \n",
       "14   {'neg': 0.032, 'neu': 0.804, 'pos': 0.164, 'co...  \n",
       "15   {'neg': 0.106, 'neu': 0.722, 'pos': 0.173, 'co...  \n",
       "16   {'neg': 0.067, 'neu': 0.727, 'pos': 0.206, 'co...  \n",
       "17   {'neg': 0.101, 'neu': 0.757, 'pos': 0.142, 'co...  \n",
       "18   {'neg': 0.056, 'neu': 0.735, 'pos': 0.209, 'co...  \n",
       "19   {'neg': 0.103, 'neu': 0.706, 'pos': 0.192, 'co...  \n",
       "20   {'neg': 0.106, 'neu': 0.558, 'pos': 0.336, 'co...  \n",
       "21   {'neg': 0.031, 'neu': 0.817, 'pos': 0.151, 'co...  \n",
       "22   {'neg': 0.062, 'neu': 0.671, 'pos': 0.267, 'co...  \n",
       "23   {'neg': 0.0, 'neu': 0.535, 'pos': 0.465, 'comp...  \n",
       "24   {'neg': 0.087, 'neu': 0.717, 'pos': 0.197, 'co...  \n",
       "25   {'neg': 0.106, 'neu': 0.677, 'pos': 0.217, 'co...  \n",
       "26   {'neg': 0.078, 'neu': 0.74, 'pos': 0.182, 'com...  \n",
       "27   {'neg': 0.068, 'neu': 0.726, 'pos': 0.206, 'co...  \n",
       "28   {'neg': 0.0, 'neu': 0.799, 'pos': 0.201, 'comp...  \n",
       "29   {'neg': 0.152, 'neu': 0.61, 'pos': 0.238, 'com...  \n",
       "..                                                 ...  \n",
       "120  {'neg': 0.06, 'neu': 0.835, 'pos': 0.104, 'com...  \n",
       "121  {'neg': 0.07, 'neu': 0.774, 'pos': 0.156, 'com...  \n",
       "122  {'neg': 0.129, 'neu': 0.663, 'pos': 0.208, 'co...  \n",
       "123  {'neg': 0.036, 'neu': 0.75, 'pos': 0.214, 'com...  \n",
       "124  {'neg': 0.052, 'neu': 0.76, 'pos': 0.188, 'com...  \n",
       "125  {'neg': 0.035, 'neu': 0.729, 'pos': 0.235, 'co...  \n",
       "126  {'neg': 0.05, 'neu': 0.584, 'pos': 0.366, 'com...  \n",
       "127  {'neg': 0.09, 'neu': 0.621, 'pos': 0.289, 'com...  \n",
       "128  {'neg': 0.068, 'neu': 0.625, 'pos': 0.307, 'co...  \n",
       "129  {'neg': 0.047, 'neu': 0.704, 'pos': 0.249, 'co...  \n",
       "130  {'neg': 0.071, 'neu': 0.729, 'pos': 0.2, 'comp...  \n",
       "131  {'neg': 0.046, 'neu': 0.795, 'pos': 0.159, 'co...  \n",
       "132  {'neg': 0.121, 'neu': 0.718, 'pos': 0.16, 'com...  \n",
       "133  {'neg': 0.034, 'neu': 0.702, 'pos': 0.263, 'co...  \n",
       "134  {'neg': 0.042, 'neu': 0.8, 'pos': 0.158, 'comp...  \n",
       "135  {'neg': 0.035, 'neu': 0.805, 'pos': 0.16, 'com...  \n",
       "136  {'neg': 0.0, 'neu': 0.727, 'pos': 0.273, 'comp...  \n",
       "137  {'neg': 0.079, 'neu': 0.787, 'pos': 0.134, 'co...  \n",
       "138  {'neg': 0.08, 'neu': 0.784, 'pos': 0.136, 'com...  \n",
       "139  {'neg': 0.03, 'neu': 0.815, 'pos': 0.154, 'com...  \n",
       "140  {'neg': 0.02, 'neu': 0.771, 'pos': 0.21, 'comp...  \n",
       "141  {'neg': 0.022, 'neu': 0.721, 'pos': 0.257, 'co...  \n",
       "142  {'neg': 0.0, 'neu': 0.619, 'pos': 0.381, 'comp...  \n",
       "143  {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'comp...  \n",
       "144  {'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'comp...  \n",
       "145  {'neg': 0.126, 'neu': 0.646, 'pos': 0.228, 'co...  \n",
       "146  {'neg': 0.044, 'neu': 0.701, 'pos': 0.255, 'co...  \n",
       "147  {'neg': 0.078, 'neu': 0.648, 'pos': 0.274, 'co...  \n",
       "148  {'neg': 0.076, 'neu': 0.614, 'pos': 0.31, 'com...  \n",
       "149  {'neg': 0.096, 'neu': 0.695, 'pos': 0.209, 'co...  \n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:18.285980Z",
     "start_time": "2018-05-23T19:11:18.277980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      "post               150 non-null object\n",
      "sentiment_score    150 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "polarity_scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Manual evaluation of Vader sentiment analysis by random sampling of 2 positive and 2 neutral posts. negative accuracy confirmed using proof by contradiction</b></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 positive posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:22.551982Z",
     "start_time": "2018-05-23T19:11:22.547014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi all \\nYeah it does pull us under its so damned powerful. I did that this last down, was heavy but not like they mostly were. Went with it. Easy to slide without resistance though\\n Yeah agree there's something in talking to it, liking your wording Dools.. talking to it, challenging thoughts I've been saying but yours is better doesnt sound like effort where challenging does. Will think of you when I word it the same ☺ thanks \\nIm not saying acceptance doesnt work ..just I don't get it yet.. may not.. does sound easier. Maybe going with it is part of acceptance ..dunno \\nI think it's a good release glad you're talking Dools\\nHope you have a restful secure sleep  🤗..eveyone..\\nnigh nite 😊\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.post[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:25.892312Z",
     "start_time": "2018-05-23T19:11:25.886310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9935, 'neg': 0.079, 'neu': 0.569, 'pos': 0.351}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.sentiment_score[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:33.465013Z",
     "start_time": "2018-05-23T19:11:33.460011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Beautifully said Rose \\nThe stars ... oh yeah ..\\nWe're part of what we see\\nThe beauty. Intrigue. Love em\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.post[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:34.637606Z",
     "start_time": "2018-05-23T19:11:34.632639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9313, 'neg': 0.0, 'neu': 0.535, 'pos': 0.465}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.sentiment_score[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 neutral posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:39.133644Z",
     "start_time": "2018-05-23T19:11:39.129676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Dools,\\nThat lady at the Op Shop sounds lovely! Its great that you have people that you can openly talk to. It certainly helps.  \\nI try to think of my depression as a feral cat and my anxiety as a vicious dog. LOL. \\nHow's everyone feeling today? My cat is still sleeping but my dog is awake and I can feel it breathing down my neck :(\\nChloe\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.post[132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:39.600127Z",
     "start_time": "2018-05-23T19:11:39.595124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.2402, 'neg': 0.121, 'neu': 0.718, 'pos': 0.16}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.sentiment_score[132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:40.132124Z",
     "start_time": "2018-05-23T19:11:40.126124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugs are special, especially tight ones that make you feel like the person hugging you never wants to let you go. I'm going to try and see if I can get him to meet me at the shops tomorrow I really need to talk to someone face to face that I know will listen and not judge (and give me hugs lol). \\nDepression todsy is one of the worst days I've had... Feeling very sad and suicidal. Even though I had a good laugh this morning I am still very bad today... Dumb dog and cat getting the better of me again. I just want to go to sleep but I won't be able too .\\nHope you all had lovely days, it might cheer me up to hear that you guys did \\nchloe x\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.post[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:42.839209Z",
     "start_time": "2018-05-23T19:11:42.833245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.9342, 'neg': 0.096, 'neu': 0.695, 'pos': 0.209}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores.sentiment_score[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T19:11:43.956389Z",
     "start_time": "2018-05-23T19:11:43.943427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative score: 9.863999999999995\n",
      "Neutral score: 105.87299999999998\n",
      "Positive score: 34.261\n",
      "Compound score: 121.31860000000003\n"
     ]
    }
   ],
   "source": [
    "#Cumulative sentiment score for thread of 150 posts\n",
    "negative = 0.0\n",
    "neutral = 0.0\n",
    "positive = 0.0\n",
    "compound = 0.0\n",
    "for index, scores in enumerate(polarity_scores.sentiment_score):\n",
    "    negative += scores[\"neg\"]\n",
    "    neutral += scores[\"neu\"]\n",
    "    positive += scores[\"pos\"]\n",
    "    compound += scores[\"compound\"]\n",
    "\n",
    "print(\"Negative score: \" + str(negative))\n",
    "print(\"Neutral score: \" + str(neutral))\n",
    "print(\"Positive score: \" + str(positive))\n",
    "print(\"Compound score: \" + str(compound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Summary of Analysis:\n",
    "### Model Selection and Implementation\n",
    "\n",
    "For the first phase, we selected a 2 vectorized models and optimised them using gridsearch. For topic modelling, LDA was used as a simple model due to its high interpretability to get an initial high-level understanding of the significant terms and themes in the thread. Based on the features of Vader sentiment analysis library in its ability to handle emoticons and textual data that has not undergone significant text normalisation (see feature listing https://github.com/cjhutto/vaderSentiment), we chose Vader for sentiment analysis.\n",
    "\n",
    "### Implementation and Evaluation\n",
    "\n",
    "Overall, in this capstone:\n",
    "\n",
    "<b>Scraping</b>\n",
    "* Scraped 1 thread in beyondblue depression forum using BeautifulSoup\n",
    "* Whilst scraping, used regex to extract the elements without html tags to save time in text preprocessing. Status codes were printed to monitor status of scraping\n",
    "* Scraped dataframe was converted to excel for persistency and use in EDA and modelling\n",
    "\n",
    "<b>EDA and modelling</b>\n",
    "* Visually examined the data and added stopwords to the standard nltk stopword list.\n",
    "* used both countvectorizer and tfidf vectorizer to preprocess and transform the text into word vectors.\n",
    "* Optimised the vectorizers using grid-search cross validation\n",
    "* Fit LDA topic model to both vectorizers and compared them\n",
    "* Separately performed sentiment analysis using Vader on the individual posts without text preprocessing\n",
    "* In conducting sentiment analysis, we computed an aggregate score of the sentiment of each posts to see the overall sentiment of the forum thread.\n",
    "* To check accuracy of sentiment analysis, a random sample of 2 neutral and 2 positive posts were examined to match human labelling vs algorithmic output\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "From the topic models, we are able to make inferences about the commonly used phrases in the forum thread. Also, using sentiment analysis, we inferred that the overall sentiment of the forum thread was mainly neutral and positive. There was hardly any negativity in the forum thread which is expected in a forum of this kind. Positive reinforcement, active listening and constructive suggestions were present in this forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Phase 2: Increase dataset size and rerun tasks in phase 1 with improvements such as pickling the scraped dataframe instead of excel to minimise data loss\n",
    "* Phase 3: Introduce subject matter experts and continue to fine-tune models. Increase dataset to have enough data to create different recurrent neural networks such as LSTM and GRUs that handle vanishing/exploding gradient well.\n",
    "* Phase 4: Refine model for deployment to incorporate it into the chatbot\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T07:06:48.976145Z",
     "start_time": "2018-05-21T07:06:48.971146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn imports both text preprocessing and sklearn's NLP models\n",
    "from sklearn.decomposition import TruncatedSVD # SVD singular value decomposition\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB # mulitinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T07:06:51.854143Z",
     "start_time": "2018-05-21T07:06:51.850142Z"
    }
   },
   "outputs": [],
   "source": [
    "Krunal = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T07:17:10.095146Z",
     "start_time": "2018-05-21T07:17:10.088140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['penalty',\n",
       " 'dual',\n",
       " 'tol',\n",
       " 'C',\n",
       " 'fit_intercept',\n",
       " 'intercept_scaling',\n",
       " 'class_weight',\n",
       " 'random_state',\n",
       " 'solver',\n",
       " 'max_iter',\n",
       " 'multi_class',\n",
       " 'verbose',\n",
       " 'warm_start',\n",
       " 'n_jobs',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " 'fit',\n",
       " 'predict_proba',\n",
       " 'predict_log_proba',\n",
       " '_get_param_names',\n",
       " 'get_params',\n",
       " 'set_params',\n",
       " '__repr__',\n",
       " '__getstate__',\n",
       " '__setstate__',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__',\n",
       " 'decision_function',\n",
       " 'predict',\n",
       " '_predict_proba_lr',\n",
       " '_estimator_type',\n",
       " 'score',\n",
       " 'densify',\n",
       " 'sparsify']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Krunal.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP normalisation functions - keeping these functions here in case I need it later\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    \n",
    "    lemmatize_verbs(words)\n",
    "    remove_stopwords(words)\n",
    "    \n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive accuracy = 100.0% via 137 samples\n",
      "Negative accuracy = 9.090909090909092% via 143 samples\n"
     ]
    }
   ],
   "source": [
    "#Textblob metric sentiment analysis evaluation\n",
    "from textblob import TextBlob\n",
    "\n",
    "neg_threshold = 0.514\n",
    "pos_threshold = 0.01\n",
    "\n",
    "\n",
    "pos_count = 0\n",
    "pos_correct = 0\n",
    "\n",
    "for line in postings.content:\n",
    "    analysis = TextBlob(line)\n",
    "\n",
    "    if analysis.sentiment.polarity >= pos_threshold:\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            pos_correct += 1\n",
    "        pos_count +=1\n",
    "\n",
    "\n",
    "neg_count = 0\n",
    "neg_correct = 0\n",
    "\n",
    "for line in postings.content:\n",
    "    analysis = TextBlob(line)\n",
    "    if analysis.sentiment.polarity < neg_threshold:\n",
    "        if analysis.sentiment.polarity <= 0:\n",
    "            neg_correct += 1\n",
    "        neg_count +=1\n",
    "\n",
    "print(\"Positive accuracy = {}% via {} samples\".format(pos_correct/pos_count*100.0, pos_count))\n",
    "print(\"Negative accuracy = {}% via {} samples\".format(neg_correct/neg_count*100.0, neg_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive accuracy = 92.66666666666666% via 150 samples\n",
      "Negative accuracy = 7.333333333333333% via 150 samples\n"
     ]
    }
   ],
   "source": [
    "#Vader metric evaluation\n",
    "neg_threshold = 0.514\n",
    "pos_threshold = 0.26\n",
    "pos_count = 0\n",
    "pos_correct = 0\n",
    "\n",
    "for line in postings.content:\n",
    "    polarity = sentiment_analyzer.polarity_scores(line)\n",
    "    if not polarity['neg'] > pos_threshold:\n",
    "        if polarity['pos']-polarity['neg'] > 0:\n",
    "            pos_correct += 1\n",
    "        pos_count +=1\n",
    "\n",
    "\n",
    "neg_count = 0\n",
    "neg_correct = 0\n",
    "\n",
    "for line in postings.content:\n",
    "    polarity = sentiment_analyzer.polarity_scores(line)\n",
    "    if not polarity['pos'] > neg_threshold:\n",
    "        if polarity['pos']-polarity['neg'] <= 0:\n",
    "            neg_correct += 1\n",
    "        neg_count +=1\n",
    "\n",
    "print(\"Positive accuracy = {}% via {} samples\".format(pos_correct/pos_count*100.0, pos_count))\n",
    "print(\"Negative accuracy = {}% via {} samples\".format(neg_correct/neg_count*100.0, neg_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 243.4,
   "position": {
    "height": "40px",
    "left": "1.425px",
    "right": "20px",
    "top": "571px",
    "width": "308px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
